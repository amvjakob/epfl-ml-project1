{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from dataprocessing import *\n",
    "from classifiers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids, features = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y=None):\n",
    "    \"\"\"\n",
    "    Splits the data matrix X into partitions based on the integer feature 'PRI_jet_num'\n",
    "    \n",
    "    :param X: examples\n",
    "    :param y: labels\n",
    "    :return indices of split for every subset, X_split, y_split\n",
    "    \"\"\"\n",
    "    # features\n",
    "    undef_feature_for = {\n",
    "        'DER_deltaeta_jet_jet'   : [0, 1],\n",
    "        'DER_mass_jet_jet'       : [0, 1],\n",
    "        'DER_prodeta_jet_jet'    : [0, 1],\n",
    "        'DER_lep_eta_centrality' : [0, 1],\n",
    "        'PRI_jet_num'            : [0, 1, 2, 3],\n",
    "        'PRI_jet_leading_pt'     : [0],\n",
    "        'PRI_jet_leading_eta'    : [0],\n",
    "        'PRI_jet_leading_phi'    : [0],\n",
    "        'PRI_jet_subleading_pt'  : [0, 1],\n",
    "        'PRI_jet_subleading_eta' : [0, 1],\n",
    "        'PRI_jet_subleading_phi' : [0, 1],\n",
    "        'PRI_jet_all_pt'         : [0]\n",
    "    }\n",
    "\n",
    "    # the feature based on which we split tX\n",
    "    jet_num_feature = \"PRI_jet_num\"\n",
    "    jet_levels = 4\n",
    "\n",
    "    # build valid features for every subset of tX\n",
    "    features_split = []\n",
    "    for jet in range(jet_levels):\n",
    "        valid_features = [ f for f in features if not ((f in undef_feature_for) and (jet in undef_feature_for[f])) ]\n",
    "        features_split.append(valid_features)\n",
    "        \n",
    "    # split data based on jet level (vertical split)\n",
    "    split_indices = [\n",
    "        X[:,features.index(jet_num_feature)] == i for i in range(jet_levels)\n",
    "    ]\n",
    "    X_split = [\n",
    "        X[X[:,features.index(jet_num_feature)] == i,:] for i in range(jet_levels)\n",
    "    ]\n",
    "    if y is None:\n",
    "        y_split = None\n",
    "    else:\n",
    "        y_split = [\n",
    "            y[X[:,features.index(jet_num_feature)] == i] for i in range(jet_levels)\n",
    "        ]\n",
    "\n",
    "    # only keep relevant features (horizontal split)\n",
    "    for i, X_ in enumerate(X_split):\n",
    "        indices = [ features.index(feature) for feature in features_split[i] ]\n",
    "        indices_bool = [ e in indices for e in range(len(features)) ]\n",
    "        X_split[i] = X_[:,indices_bool]\n",
    "        \n",
    "    return split_indices, X_split, y_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly_no_interaction(X, degree):\n",
    "    \"\"\"\n",
    "    Build a polynomial expansion of X without interaction terms\n",
    "    \n",
    "    :param X: data\n",
    "    :param degree: degree of expansion\n",
    "    :return the expanded data\n",
    "    \"\"\"\n",
    "    result = X.copy()\n",
    "    for d in range(2, degree+1):\n",
    "        # faster than np.power()\n",
    "        power = X.copy()\n",
    "        for i in range(d - 1):\n",
    "            power = power * X\n",
    "            \n",
    "        result = np.hstack((result, power))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def build_X(X, d_int, d_sq):\n",
    "    \"\"\"\n",
    "    Expands X with integer and/or half-powers\n",
    "    \n",
    "    :param X: examples\n",
    "    :param d_int: degree of integer powers\n",
    "    :param d_sq: ceil of degree of half-powers (expansion will be up to d_sq - 0.5)\n",
    "    \n",
    "    \"\"\"\n",
    "    X_ = remove_NaN_features(X, 0.2)\n",
    "    X_, mean_, std_ = standardize(X_)\n",
    "    \n",
    "    ints = []\n",
    "    sqrts = []\n",
    "    \n",
    "    # build integer powers\n",
    "    if d_int > 0:\n",
    "        ints = build_poly_no_interaction(X_, d_int)\n",
    "      \n",
    "    # build half-powers (0.5, 1.5, 2.5, etc.)\n",
    "    if d_sq > 0:\n",
    "        sqrts = np.sqrt(np.abs(X_))\n",
    "        if d_sq > 1:\n",
    "            width = sqrts.shape[1]\n",
    "            int_power = np.abs(build_poly_no_interaction(X_, d_sq - 1))\n",
    "            \n",
    "            half_power = sqrts.copy()\n",
    "            for i in range(d_sq - 1):\n",
    "                half_power = np.hstack((half_power, sqrts * int_power[:,(width*i):(width*(i+1))]))\n",
    "                \n",
    "            sqrts = np.hstack((sqrts, half_power))\n",
    "    else:\n",
    "        return ints\n",
    "\n",
    "    # concat\n",
    "    X_ = np.hstack((ints, sqrts))\n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_split, X_split, y_split = split_data(tX, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different models\n",
    "# LeastSquaresL2 is by far the best\n",
    "\n",
    "X_split_poly = [ build_X(X, 11, 3) for X in X_split ]\n",
    "lambda_ = 1e-8\n",
    "k = 5\n",
    "models_try = [\n",
    "    LeastSquaresL2(lambda_),\n",
    "    LeastSquaresL1(lambda_, verbose=True, max_evaluations=500),\n",
    "    LogisticRegression(),\n",
    "    LogisticRegressionL2(lambda_),\n",
    "    LogisticRegressionL1(lambda_, verbose=True, max_evaluations=500),\n",
    "]\n",
    "total_accs = []\n",
    "\n",
    "for model in models_try:\n",
    "    \n",
    "    accuracies = []\n",
    "    # iterate over 4 sub datasets\n",
    "    for i in range(len(X_split_poly)):\n",
    "        acc = np.mean(cross_validate_kfold(y_split[i], X_split_poly[i], model, 5))\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "    # compute mean (weighted)\n",
    "    accuracy = 0\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        accuracy += acc * len(y_split[i])\n",
    "    accuracy /= len(y) \n",
    "    \n",
    "    total_accs.append(accuracy)\n",
    "    \n",
    "plt.plot([\"LS L2\", \"LS L1\", \"LogReg\", \"LogReg L2\", \"LogReg L1\"], total_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search for rough estimate of best integer power, best half power and best lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best lambda_ or degree\n",
    "\n",
    "d_ints = range(11, 12)\n",
    "d_sqs = range(3, 4)\n",
    "#lambdas = [ math.pow(10,c) for c in np.linspace(-8,-7.5,200) ]\n",
    "lambdas = np.linspace(1.41e-08,1.47e-08,200)\n",
    "\n",
    "max_train = 0\n",
    "max_train_d_int = 0\n",
    "max_train_d_sq = 0\n",
    "max_train_d_lambda = 0\n",
    "\n",
    "max_test = 0\n",
    "max_test_d_int = 0\n",
    "max_test_d_sq = 0\n",
    "max_test_d_lambda = 0\n",
    "\n",
    "for d_int in d_ints:\n",
    "    for d_sq in d_sqs:\n",
    "        X_split_poly = [ build_X(X, d_int, d_sq) for X in X_split ]\n",
    "        X_test_split_poly = [ build_X(X, d_int, d_sq) for X in X_test_split ]\n",
    "        \n",
    "        for lambda_ in lambdas:\n",
    "            \n",
    "            models = []\n",
    "            y_pred = np.ones(tX.shape[0])\n",
    "\n",
    "            for i in range(len(X_split_poly)):\n",
    "                lse = LeastSquaresL2(lambda_)\n",
    "                lse.fit(y_split[i], X_split_poly[i])\n",
    "                models.append(lse)\n",
    "                y_pred[indices_split[i]] = lse.predict(X_split_poly[i])\n",
    "\n",
    "            acc_train = np.mean(y == y_pred)\n",
    "            if acc_train > max_train:\n",
    "                max_train = acc_train\n",
    "                max_train_d_int = d_int\n",
    "                max_train_d_sq = d_sq\n",
    "                max_train_d_lambda = lambda_\n",
    "            \n",
    "            print(f\"d_int={d_int}, d_sq={d_sq}, lambda_={lambda_} - train={acc_train}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation to optimize the hyper-parameter lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validate\n",
    "total_acc = []\n",
    "k = 10\n",
    "lambdas = np.linspace(1.41e-08,1.47e-08,50)\n",
    "d_int = 11\n",
    "d_sq = 3\n",
    "\n",
    "X_split_poly = [ build_X(X, d_int, d_sq) for X in X_split ]\n",
    "\n",
    "# run that shit\n",
    "for lambda_ in lambdas:\n",
    "    print(f\"lambda={lambda_}\", end=\" - \")\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    # iterate over 4 sub datasets\n",
    "    for i in range(len(X_split_poly)):\n",
    "        classifier = LeastSquaresL2(lambda_)\n",
    "        acc = np.mean(cross_validate_kfold(y_split[i], X_split_poly[i], classifier, k))\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "    # compute mean (weighted)\n",
    "    accuracy = 0\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        accuracy += acc * len(y_split[i])\n",
    "    accuracy /= len(y)\n",
    "        \n",
    "    print(accuracy)\n",
    "    total_acc.append(accuracy)\n",
    "    \n",
    "plt.plot(lambdas, total_acc)\n",
    "best_lambdas = [ lambdas[index] for index in np.argpartition(total_acc, -4)[-4:] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best lambda\n",
    "best_lambdas = [ lambdas[index] for index in np.argpartition(total_acc, -4)[-4:] ]\n",
    "best_lambda = best_lambdas[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train actual models\n",
    "# degrees 11,3 seem to be best\n",
    "\n",
    "best_lambda = 1.4565306122448979e-08\n",
    "best_deg_int = 11\n",
    "best_deg_sq = 3\n",
    "\n",
    "\n",
    "def model_split_data(X):\n",
    "    return build_X(X, best_deg_int, best_deg_sq)\n",
    "\n",
    "X_split_poly = [ model_split_data(X) for X in X_split ]\n",
    "lambda_ = best_lambda\n",
    "models = []\n",
    "y_pred = np.ones(tX.shape[0])\n",
    "\n",
    "for i in range(len(X_split_poly)):\n",
    "    print(f\"Building model for dataset {i}\")\n",
    "    lse = LeastSquaresL2(lambda_)\n",
    "    lse.fit(y_split[i], X_split_poly[i])\n",
    "    models.append(lse)\n",
    "    y_pred[indices_split[i]] = lse.predict(X_split_poly[i])\n",
    "    \n",
    "print(np.mean(y == y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try basic methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = remove_NaN_features(tX, 0.2)\n",
    "X = replace_NaN_by_median(X)\n",
    "X, _, _ = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lse_classifier = LeastSquares()\n",
    "accuracy_lse = cross_validate(y, X, lse_classifier, 0.8, 10)\n",
    "print(np.mean(accuracy_lse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_classifier(lambda_):\n",
    "    return ridge_regression(y, X, lambda_)\n",
    "\n",
    "lambda_ridge, _, _ = find_max_hyperparam(ridge_classifier, [10**c for c in range(-3,3)])\n",
    "print(\"Optimal lambda: %f\" % lambda_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_classifier = LeastSquaresL2(lambda_ridge)\n",
    "accuracy_ridge = cross_validate(y, X, ridge_classifier, 0.8, 10)\n",
    "print(np.mean(accuracy_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_classifier = LogisticRegression()\n",
    "accuracy_log_reg = cross_validate(y, X, log_reg_classifier, 0.8, 10)\n",
    "print(np.mean(accuracy_log_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_log_reg_classifier(lambda_):\n",
    "    return reg_logistic_regression(y, X, lambda_, np.zeros(X.shape[1]), 1000)\n",
    "\n",
    "lambda_reg_log_reg, _, _ = find_max_hyperparam(reg_log_reg_classifier, [10**c for c in range(-3,3)])\n",
    "print(\"Optimal lambda: %f\" % lambda_reg_log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_log_reg_classifier = LogisticRegressionL2(lambda_reg_log_reg)\n",
    "accuracy_log_reg = cross_validate(y, X, reg_log_reg_classifier, 0.8, 10)\n",
    "print(np.mean(accuracy_log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test, _ = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split_indices, X_test_split, _ = split_data(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions using new model\n",
    "X_test_split_poly = [ model_split_data(X) for X in X_test_split ]\n",
    "y_pred = np.ones(tX_test.shape[0])\n",
    "\n",
    "for model, X, indices in zip(models, X_test_split_poly, test_split_indices):\n",
    "    y_pred[indices] = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/predictions.csv'\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
