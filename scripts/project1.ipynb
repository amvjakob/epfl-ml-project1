{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from helpers import *\n",
    "from classifiers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt', 'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality', 'DER_lep_eta_centrality', 'PRI_tau_pt', 'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi', 'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt']\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids, features = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y=None):\n",
    "    # features\n",
    "    undef_feature_for = {\n",
    "        'DER_deltaeta_jet_jet'   : [0, 1],\n",
    "        'DER_mass_jet_jet'       : [0, 1],\n",
    "        'DER_prodeta_jet_jet'    : [0, 1],\n",
    "        'DER_lep_eta_centrality' : [0, 1],\n",
    "        'PRI_jet_num'            : [0, 1, 2, 3],\n",
    "        'PRI_jet_leading_pt'     : [0],\n",
    "        'PRI_jet_leading_eta'    : [0],\n",
    "        'PRI_jet_leading_phi'    : [0],\n",
    "        'PRI_jet_subleading_pt'  : [0, 1],\n",
    "        'PRI_jet_subleading_eta' : [0, 1],\n",
    "        'PRI_jet_subleading_phi' : [0, 1],\n",
    "        'PRI_jet_all_pt'         : [0]\n",
    "    }\n",
    "\n",
    "    jet_num_feature = \"PRI_jet_num\"\n",
    "    jet_levels = 4\n",
    "\n",
    "    features_split = []\n",
    "    for jet in range(jet_levels):\n",
    "        valid_features = [ f for f in features if not ((f in undef_feature_for) and (jet in undef_feature_for[f])) ]\n",
    "        features_split.append(valid_features)\n",
    "        \n",
    "    # split data based on jet level (vertical split)\n",
    "    split_indices = [\n",
    "        X[:,features.index(jet_num_feature)] == i for i in range(jet_levels)\n",
    "    ]\n",
    "    X_split = [\n",
    "        X[X[:,features.index(jet_num_feature)] == i,:] for i in range(jet_levels)\n",
    "    ]\n",
    "    if y is None:\n",
    "        y_split = None\n",
    "    else:\n",
    "        y_split = [\n",
    "            y[X[:,features.index(jet_num_feature)] == i] for i in range(jet_levels)\n",
    "        ]\n",
    "\n",
    "    # only keep relevant features (horizontal split)\n",
    "    for i, X_ in enumerate(X_split):\n",
    "        indices = [ features.index(feature) for feature in features_split[i] ]\n",
    "        indices_bool = [ e in indices for e in range(len(features)) ]\n",
    "        X_split[i] = X_[:,indices_bool]\n",
    "        \n",
    "    return split_indices, X_split, y_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly_no_interaction(X, degree):\n",
    "    result = X.copy()\n",
    "    for d in range(2, degree+1):\n",
    "        # faster than np.power()\n",
    "        power = X.copy()\n",
    "        for i in range(d - 1):\n",
    "            power = power * X\n",
    "            \n",
    "        result = np.hstack((result, power))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def build_X(X, d_int, d_sq):\n",
    "    \"\"\"\n",
    "    Expands X\n",
    "    \n",
    "    :param X: examples\n",
    "    :param d_int: degree of integer powers\n",
    "    :param d_sq: ceil of degree of half-powers (expansion will be up to d_sq - 0.5)\n",
    "    \n",
    "    \"\"\"\n",
    "    X_ = remove_NaN_features(X, 0.2)\n",
    "    X_, mean_, std_ = standardize(X_)\n",
    "    \n",
    "    ints = []\n",
    "    sqrts = []\n",
    "    \n",
    "    # build integer powers\n",
    "    if d_int > 0:\n",
    "        ints = build_poly_no_interaction(X_, d_int)\n",
    "      \n",
    "    # build half-powers (0.5, 1.5, 2.5, etc.)\n",
    "    if d_sq > 0:\n",
    "        sqrts = np.sqrt(np.abs(X_))\n",
    "        if d_sq > 1:\n",
    "            width = sqrts.shape[1]\n",
    "            int_power = np.abs(build_poly_no_interaction(X_, d_sq - 1))\n",
    "            \n",
    "            half_power = sqrts.copy()\n",
    "            for i in range(d_sq - 1):\n",
    "                half_power = np.hstack((half_power, sqrts * int_power[:,(width*i):(width*(i+1))]))\n",
    "                \n",
    "            sqrts = np.hstack((sqrts, half_power))\n",
    "    else:\n",
    "        return ints\n",
    "\n",
    "    # concat\n",
    "    X_ = np.hstack((ints, sqrts))\n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_split, X_split, y_split = split_data(tX, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - 0.8026435553507603\n",
      "2 - 0.8048917827344935\n",
      "3 - 0.8062559237434147\n",
      "4 - 0.807264054515904\n",
      "5 - "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-219-8f6ca7ef4d6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_split_poly\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLeastSquaresL2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_validate_kfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_split_poly\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\EPFL\\MA1\\CS-433 Machine Learning\\ML_course\\projects\\project1\\scripts\\helpers.py\u001b[0m in \u001b[0;36mcross_validate_kfold\u001b[1;34m(y, x, classifier, k_fold)\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[0mind_opp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind_sort\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind_opp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m         \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind_opp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# cross_validate\n",
    "total_acc = []\n",
    "k = 10\n",
    "lambda_ = 0.01\n",
    "degrees = range(1, 15)\n",
    "\n",
    "# run that shit\n",
    "for deg in degrees:\n",
    "    print(deg, end=\" - \")\n",
    "    X_split_poly = [ build_X(X, 9, deg) for X in X_split ]\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    # iterate over 4 sub datasets\n",
    "    for i in range(len(X_split_poly)):\n",
    "        classifier = LeastSquaresL2(lambda_)\n",
    "        acc = np.mean(cross_validate_kfold(y_split[i], X_split_poly[i], classifier, k))\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "    # compute mean (weighted)\n",
    "    accuracy = 0\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        accuracy += acc * len(y_split[i])\n",
    "    accuracy /= len(y)\n",
    "        \n",
    "    print(accuracy)\n",
    "    total_acc.append(accuracy)\n",
    "    \n",
    "plt.plot(degrees, total_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.01\n",
    "models_try = [\n",
    "    LeastSquares(),\n",
    "    LeastSquaresL2(lambda_),\n",
    "    LeastSquaresL1(lambda_, verbose=True, max_evaluations=500),\n",
    "    LogisticRegression(),\n",
    "    LogisticRegressionL2(lambda_),\n",
    "    LogisticRegressionL1(lambda_, verbose=True, max_evaluations=500),\n",
    "]\n",
    "\n",
    "for model in models_try:\n",
    "    \n",
    "    # iterate over 4 sub datasets\n",
    "    for i in range(len(X_split_poly)):\n",
    "        classifier = LeastSquaresL2(lambda_)\n",
    "        acc = np.mean(cross_validate_kfold(y_split[i], X_split_poly[i], classifier, k))\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "    # compute mean (weighted)\n",
    "    accuracy = 0\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        accuracy += acc * len(y_split[i])\n",
    "    accuracy /= len(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_int=5, d_sq=0, lambda_=1e-08 - train=0.809896 - test=0.8027727818273329\n",
      "d_int=5, d_sq=0, lambda_=1e-07 - train=0.809888 - test=0.8027868604352402\n",
      "d_int=5, d_sq=0, lambda_=1e-06 - train=0.80986 - test=0.8028238167809967\n",
      "d_int=5, d_sq=0, lambda_=1e-05 - train=0.809756 - test=0.8028185373030314\n",
      "d_int=5, d_sq=0, lambda_=0.0001 - train=0.808832 - test=0.8024260961076168\n",
      "d_int=5, d_sq=0, lambda_=0.001 - train=0.80642 - test=0.80036885952717\n",
      "d_int=5, d_sq=0, lambda_=0.01 - train=0.804676 - test=0.8035119087424636\n",
      "d_int=5, d_sq=0, lambda_=0.1 - train=0.800724 - test=0.7994308722753494\n",
      "d_int=5, d_sq=1, lambda_=1e-08 - train=0.81736 - test=0.8104808196565524\n",
      "d_int=5, d_sq=1, lambda_=1e-07 - train=0.817352 - test=0.8106075271277177\n",
      "d_int=5, d_sq=1, lambda_=1e-06 - train=0.817284 - test=0.810440343658819\n",
      "d_int=5, d_sq=1, lambda_=1e-05 - train=0.816772 - test=0.8098789591685174\n",
      "d_int=5, d_sq=1, lambda_=0.0001 - train=0.814832 - test=0.8085450110693054\n",
      "d_int=5, d_sq=1, lambda_=0.001 - train=0.811828 - test=0.8065722461363021\n",
      "d_int=5, d_sq=1, lambda_=0.01 - train=0.808552 - test=0.8079044344095256\n",
      "d_int=5, d_sq=1, lambda_=0.1 - train=0.802508 - test=0.801880550051211\n",
      "d_int=5, d_sq=2, lambda_=1e-08 - train=0.821736 - test=0.8143612359609882\n",
      "d_int=5, d_sq=2, lambda_=1e-07 - train=0.821804 - test=0.8144492272604085\n",
      "d_int=5, d_sq=2, lambda_=1e-06 - train=0.821776 - test=0.8144369084784896\n",
      "d_int=5, d_sq=2, lambda_=1e-05 - train=0.821588 - test=0.8138649650322576\n",
      "d_int=5, d_sq=2, lambda_=0.0001 - train=0.819556 - test=0.8121438552155963\n",
      "d_int=5, d_sq=2, lambda_=0.001 - train=0.815916 - test=0.8103523523593987\n",
      "d_int=5, d_sq=2, lambda_=0.01 - train=0.810128 - test=0.8095938673583956\n",
      "d_int=5, d_sq=2, lambda_=0.1 - train=0.80458 - test=0.8038656337661332\n",
      "d_int=5, d_sq=3, lambda_=1e-08 - train=0.826068 - test=0.7905349519039557\n",
      "d_int=5, d_sq=3, lambda_=1e-07 - train=0.825312 - test=0.8149560571450695\n",
      "d_int=5, d_sq=3, lambda_=1e-06 - train=0.823948 - test=0.8153045026907739\n",
      "d_int=5, d_sq=3, lambda_=1e-05 - train=0.822728 - test=0.8157479788398523\n",
      "d_int=5, d_sq=3, lambda_=0.0001 - train=0.821884 - test=0.8149243802772782\n",
      "d_int=5, d_sq=3, lambda_=0.001 - train=0.819896 - test=0.813481322966785\n",
      "d_int=5, d_sq=3, lambda_=0.01 - train=0.813808 - test=0.8129498555182864\n",
      "d_int=5, d_sq=3, lambda_=0.1 - train=0.804824 - test=0.804198240877942\n",
      "d_int=5, d_sq=4, lambda_=1e-08 - train=0.82754 - test=0.7947954906218873\n",
      "d_int=5, d_sq=4, lambda_=1e-07 - train=0.825924 - test=0.790237541311915\n",
      "d_int=5, d_sq=4, lambda_=1e-06 - train=0.82414 - test=0.789609283434054\n",
      "d_int=5, d_sq=4, lambda_=1e-05 - train=0.8231 - test=0.8160418697799161\n",
      "d_int=5, d_sq=4, lambda_=0.0001 - train=0.822076 - test=0.8153080223427508\n",
      "d_int=5, d_sq=4, lambda_=0.001 - train=0.820164 - test=0.813706580693301\n",
      "d_int=5, d_sq=4, lambda_=0.01 - train=0.815024 - test=0.813849126598362\n",
      "d_int=5, d_sq=4, lambda_=0.1 - train=0.806024 - test=0.805389643072093\n",
      "d_int=5, d_sq=5, lambda_=1e-08 - train=0.828116 - test=0.8204625526627927\n",
      "d_int=5, d_sq=5, lambda_=1e-07 - train=0.826904 - test=0.7945315167236263\n",
      "d_int=5, d_sq=5, lambda_=1e-06 - train=0.825264 - test=0.8027463844375068\n",
      "d_int=5, d_sq=5, lambda_=1e-05 - train=0.823836 - test=0.8213706228728104\n",
      "d_int=5, d_sq=5, lambda_=0.0001 - train=0.822224 - test=0.8204977491825608\n",
      "d_int=5, d_sq=5, lambda_=0.001 - train=0.82024 - test=0.8187643205839806\n",
      "d_int=5, d_sq=5, lambda_=0.01 - train=0.816116 - test=0.8150053322727449\n",
      "d_int=5, d_sq=5, lambda_=0.1 - train=0.806664 - test=0.8056870536641337\n",
      "d_int=5, d_sq=6, lambda_=1e-08 - train=0.829024 - test=0.821143605320306\n",
      "d_int=5, d_sq=6, lambda_=1e-07 - train=0.828132 - test=0.8204361552729666\n",
      "d_int=5, d_sq=6, lambda_=1e-06 - train=0.826096 - test=0.8183771588665313\n",
      "d_int=5, d_sq=6, lambda_=1e-05 - train=0.824648 - test=0.8166402106159742\n",
      "d_int=5, d_sq=6, lambda_=0.0001 - train=0.822644 - test=0.8138526462503388\n",
      "d_int=5, d_sq=6, lambda_=0.001 - train=0.82062 - test=0.8181554207919921\n",
      "d_int=5, d_sq=6, lambda_=0.01 - train=0.816264 - test=0.8153625769483913\n",
      "d_int=5, d_sq=6, lambda_=0.1 - train=0.807308 - test=0.8062871543261803\n",
      "d_int=5, d_sq=7, lambda_=1e-08 - train=0.80218 - test=0.7960748841154587\n",
      "d_int=5, d_sq=7, lambda_=1e-07 - train=0.715904 - test=0.6831204530496024\n",
      "d_int=5, d_sq=7, lambda_=1e-06 - train=0.826796 - test=0.8187889581478184\n",
      "d_int=5, d_sq=7, lambda_=1e-05 - train=0.824896 - test=0.8168584290385367\n",
      "d_int=5, d_sq=7, lambda_=0.0001 - train=0.823484 - test=0.8175588397819223\n",
      "d_int=5, d_sq=7, lambda_=0.001 - train=0.821164 - test=0.817630992647447\n",
      "d_int=5, d_sq=7, lambda_=0.01 - train=0.816756 - test=0.8151901140015275\n",
      "d_int=5, d_sq=7, lambda_=0.1 - train=0.807632 - test=0.8067482287351426\n",
      "d_int=6, d_sq=0, lambda_=1e-08 - train=0.811952 - test=0.802704148613785\n",
      "d_int=6, d_sq=0, lambda_=1e-07 - train=0.811956 - test=0.8027921399132054\n",
      "d_int=6, d_sq=0, lambda_=1e-06 - train=0.811924 - test=0.8024542533234313\n",
      "d_int=6, d_sq=0, lambda_=1e-05 - train=0.811728 - test=0.7987709375296971\n",
      "d_int=6, d_sq=0, lambda_=0.0001 - train=0.810312 - test=0.7943185777790293\n",
      "d_int=6, d_sq=0, lambda_=0.001 - train=0.80816 - test=0.8048951319693509\n",
      "d_int=6, d_sq=0, lambda_=0.01 - train=0.805588 - test=0.8042580749615478\n",
      "d_int=6, d_sq=0, lambda_=0.1 - train=0.801032 - test=0.7995663788764567\n",
      "d_int=6, d_sq=1, lambda_=1e-08 - train=0.819492 - test=0.8118992394032079\n",
      "d_int=6, d_sq=1, lambda_=1e-07 - train=0.819424 - test=0.8120206673964079\n",
      "d_int=6, d_sq=1, lambda_=1e-06 - train=0.818904 - test=0.8117021388925063\n",
      "d_int=6, d_sq=1, lambda_=1e-05 - train=0.817644 - test=0.8103576318373639\n",
      "d_int=6, d_sq=1, lambda_=0.0001 - train=0.815824 - test=0.8011572615699759\n",
      "d_int=6, d_sq=1, lambda_=0.001 - train=0.813192 - test=0.8102766798418972\n",
      "d_int=6, d_sq=1, lambda_=0.01 - train=0.809408 - test=0.8086558801065751\n",
      "d_int=6, d_sq=1, lambda_=0.1 - train=0.80302 - test=0.8020406942161559\n",
      "d_int=6, d_sq=2, lambda_=1e-08 - train=0.825304 - test=0.817726023250821\n",
      "d_int=6, d_sq=2, lambda_=1e-07 - train=0.825224 - test=0.8177031455129716\n",
      "d_int=6, d_sq=2, lambda_=1e-06 - train=0.824792 - test=0.8171382413706932\n",
      "d_int=6, d_sq=2, lambda_=1e-05 - train=0.823268 - test=0.8151267602659449\n",
      "d_int=6, d_sq=2, lambda_=0.0001 - train=0.821512 - test=0.8070245214153224\n",
      "d_int=6, d_sq=2, lambda_=0.001 - train=0.817368 - test=0.8145073015180259\n",
      "d_int=6, d_sq=2, lambda_=0.01 - train=0.81096 - test=0.8104667410486451\n",
      "d_int=6, d_sq=2, lambda_=0.1 - train=0.804992 - test=0.804141926446313\n",
      "d_int=6, d_sq=3, lambda_=1e-08 - train=0.82732 - test=0.8165363808826583\n",
      "d_int=6, d_sq=3, lambda_=1e-07 - train=0.826048 - test=0.8154188913800203\n",
      "d_int=6, d_sq=3, lambda_=1e-06 - train=0.824884 - test=0.8172702283198238\n",
      "d_int=6, d_sq=3, lambda_=1e-05 - train=0.823676 - test=0.8158060530974697\n",
      "d_int=6, d_sq=3, lambda_=0.0001 - train=0.822116 - test=0.8140339083271446\n",
      "d_int=6, d_sq=3, lambda_=0.001 - train=0.820392 - test=0.8180920670564095\n",
      "d_int=6, d_sq=3, lambda_=0.01 - train=0.814452 - test=0.8134390871430633\n",
      "d_int=6, d_sq=3, lambda_=0.1 - train=0.805316 - test=0.8045537257276001\n",
      "d_int=6, d_sq=4, lambda_=1e-08 - train=0.828032 - test=0.7999077851182075\n",
      "d_int=6, d_sq=4, lambda_=1e-07 - train=0.826532 - test=0.815612472238745\n",
      "d_int=6, d_sq=4, lambda_=1e-06 - train=0.825316 - test=0.817808735072276\n",
      "d_int=6, d_sq=4, lambda_=1e-05 - train=0.824224 - test=0.8163287214160264\n",
      "d_int=6, d_sq=4, lambda_=0.0001 - train=0.822476 - test=0.8136344278277764\n",
      "d_int=6, d_sq=4, lambda_=0.001 - train=0.82028 - test=0.8181466216620501\n",
      "d_int=6, d_sq=4, lambda_=0.01 - train=0.815956 - test=0.8149226204512898\n",
      "d_int=6, d_sq=4, lambda_=0.1 - train=0.807516 - test=0.8060847743375135\n",
      "d_int=6, d_sq=5, lambda_=1e-08 - train=0.829216 - test=0.8211752821880972\n",
      "d_int=6, d_sq=5, lambda_=1e-07 - train=0.828216 - test=0.8203622425814535\n",
      "d_int=6, d_sq=5, lambda_=1e-06 - train=0.826464 - test=0.8185760192032212\n",
      "d_int=6, d_sq=5, lambda_=1e-05 - train=0.824548 - test=0.8166525293978931\n",
      "d_int=6, d_sq=5, lambda_=0.0001 - train=0.82278 - test=0.8142362883158113\n",
      "d_int=6, d_sq=5, lambda_=0.001 - train=0.8207 - test=0.8178069752462876\n",
      "d_int=6, d_sq=5, lambda_=0.01 - train=0.816444 - test=0.8153819350342638\n",
      "d_int=6, d_sq=5, lambda_=0.1 - train=0.8071 - test=0.8058683157409395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_int=6, d_sq=6, lambda_=1e-08 - train=0.82908 - test=0.8209482646355928\n",
      "d_int=6, d_sq=6, lambda_=1e-07 - train=0.828472 - test=0.8210890507146653\n",
      "d_int=6, d_sq=6, lambda_=1e-06 - train=0.821916 - test=0.8123057592065297\n",
      "d_int=6, d_sq=6, lambda_=1e-05 - train=0.8249 - test=0.8169094639922004\n",
      "d_int=6, d_sq=6, lambda_=0.0001 - train=0.823468 - test=0.8187027266743865\n",
      "d_int=6, d_sq=6, lambda_=0.001 - train=0.821108 - test=0.8179442416733833\n",
      "d_int=6, d_sq=6, lambda_=0.01 - train=0.816576 - test=0.8153713760783333\n",
      "d_int=6, d_sq=6, lambda_=0.1 - train=0.807612 - test=0.8066373596978731\n",
      "d_int=6, d_sq=7, lambda_=1e-08 - train=0.830096 - test=0.8220287977924743\n",
      "d_int=6, d_sq=7, lambda_=1e-07 - train=0.829128 - test=0.8210749721067581\n",
      "d_int=6, d_sq=7, lambda_=1e-06 - train=0.827252 - test=0.8196671113160331\n",
      "d_int=6, d_sq=7, lambda_=1e-05 - train=0.825652 - test=0.822794322097431\n",
      "d_int=6, d_sq=7, lambda_=0.0001 - train=0.824204 - test=0.8167035643515569\n",
      "d_int=6, d_sq=7, lambda_=0.001 - train=0.821764 - test=0.8182029360936791\n",
      "d_int=6, d_sq=7, lambda_=0.01 - train=0.817164 - test=0.8153045026907739\n",
      "d_int=6, d_sq=7, lambda_=0.1 - train=0.809808 - test=0.8088828976590795\n",
      "d_int=7, d_sq=0, lambda_=1e-08 - train=0.814272 - test=0.7785223797070946\n",
      "d_int=7, d_sq=0, lambda_=1e-07 - train=0.814148 - test=0.7784959823172685\n",
      "d_int=7, d_sq=0, lambda_=1e-06 - train=0.813976 - test=0.7782707245907524\n",
      "d_int=7, d_sq=0, lambda_=1e-05 - train=0.813212 - test=0.777649506016845\n",
      "d_int=7, d_sq=0, lambda_=0.0001 - train=0.81206 - test=0.7766534445074071\n",
      "d_int=7, d_sq=0, lambda_=0.001 - train=0.809228 - test=0.8060143812979773\n",
      "d_int=7, d_sq=0, lambda_=0.01 - train=0.80708 - test=0.8009531217553209\n",
      "d_int=7, d_sq=0, lambda_=0.1 - train=0.802484 - test=0.8006381129033961\n",
      "d_int=7, d_sq=1, lambda_=1e-08 - train=0.819892 - test=0.8174514903966296\n",
      "d_int=7, d_sq=1, lambda_=1e-07 - train=0.819864 - test=0.8176151542135514\n",
      "d_int=7, d_sq=1, lambda_=1e-06 - train=0.819736 - test=0.7930796602831912\n",
      "d_int=7, d_sq=1, lambda_=1e-05 - train=0.819 - test=0.7837085868949278\n",
      "d_int=7, d_sq=1, lambda_=0.0001 - train=0.817236 - test=0.7821529007211767\n",
      "d_int=7, d_sq=1, lambda_=0.001 - train=0.814576 - test=0.8091415920793752\n",
      "d_int=7, d_sq=1, lambda_=0.01 - train=0.811012 - test=0.80543715837378\n",
      "d_int=7, d_sq=1, lambda_=0.1 - train=0.80448 - test=0.8029927600758837\n",
      "d_int=7, d_sq=2, lambda_=1e-08 - train=0.827736 - test=0.8201704215487172\n",
      "d_int=7, d_sq=2, lambda_=1e-07 - train=0.827008 - test=0.8192605915127112\n",
      "d_int=7, d_sq=2, lambda_=1e-06 - train=0.82558 - test=0.792801607777023\n",
      "d_int=7, d_sq=2, lambda_=1e-05 - train=0.823912 - test=0.7875854835473869\n",
      "d_int=7, d_sq=2, lambda_=0.0001 - train=0.822244 - test=0.7874024616445926\n",
      "d_int=7, d_sq=2, lambda_=0.001 - train=0.817848 - test=0.8123057592065297\n",
      "d_int=7, d_sq=2, lambda_=0.01 - train=0.812256 - test=0.8068907746402035\n",
      "d_int=7, d_sq=2, lambda_=0.1 - train=0.80622 - test=0.8049760839648176\n",
      "d_int=7, d_sq=3, lambda_=1e-08 - train=0.828448 - test=0.8206578933475058\n",
      "d_int=7, d_sq=3, lambda_=1e-07 - train=0.827344 - test=0.8198202161770244\n",
      "d_int=7, d_sq=3, lambda_=1e-06 - train=0.825708 - test=0.8182398924394356\n",
      "d_int=7, d_sq=3, lambda_=1e-05 - train=0.824336 - test=0.7965834738261081\n",
      "d_int=7, d_sq=3, lambda_=0.0001 - train=0.823236 - test=0.792377489713817\n",
      "d_int=7, d_sq=3, lambda_=0.001 - train=0.820716 - test=0.8152200310433304\n",
      "d_int=7, d_sq=3, lambda_=0.01 - train=0.814984 - test=0.8091961466850158\n",
      "d_int=7, d_sq=3, lambda_=0.1 - train=0.806508 - test=0.8051995818653451\n",
      "d_int=7, d_sq=4, lambda_=1e-08 - train=0.828816 - test=0.8209095484638479\n",
      "d_int=7, d_sq=4, lambda_=1e-07 - train=0.827728 - test=0.8201774608526708\n",
      "d_int=7, d_sq=4, lambda_=1e-06 - train=0.826296 - test=0.7945491149835104\n",
      "d_int=7, d_sq=4, lambda_=1e-05 - train=0.824484 - test=0.7938926998898349\n",
      "d_int=7, d_sq=4, lambda_=0.0001 - train=0.8233 - test=0.7900703578430165\n",
      "d_int=7, d_sq=4, lambda_=0.001 - train=0.821104 - test=0.8152341096512377\n",
      "d_int=7, d_sq=4, lambda_=0.01 - train=0.816968 - test=0.8110668417106917\n",
      "d_int=7, d_sq=4, lambda_=0.1 - train=0.807616 - test=0.804601241029287\n",
      "d_int=7, d_sq=5, lambda_=1e-08 - train=0.829916 - test=0.8220991908320106\n",
      "d_int=7, d_sq=5, lambda_=1e-07 - train=0.827732 - test=0.820195059112555\n",
      "d_int=7, d_sq=5, lambda_=1e-06 - train=0.827428 - test=0.8198378144369085\n",
      "d_int=7, d_sq=5, lambda_=1e-05 - train=0.825136 - test=0.7920378432980547\n",
      "d_int=7, d_sq=5, lambda_=0.0001 - train=0.823432 - test=0.7873655052988361\n",
      "d_int=7, d_sq=5, lambda_=0.001 - train=0.821136 - test=0.8110281255389467\n",
      "d_int=7, d_sq=5, lambda_=0.01 - train=0.817088 - test=0.8112657020473816\n",
      "d_int=7, d_sq=5, lambda_=0.1 - train=0.807832 - test=0.8031687426747244\n",
      "d_int=7, d_sq=6, lambda_=1e-08 - train=0.829976 - test=0.8190036569184039\n",
      "d_int=7, d_sq=6, lambda_=1e-07 - train=0.82908 - test=0.8185267440755458\n",
      "d_int=7, d_sq=6, lambda_=1e-06 - train=0.827768 - test=0.8200261158176679\n",
      "d_int=7, d_sq=6, lambda_=1e-05 - train=0.82574 - test=0.7928174462109187\n",
      "d_int=7, d_sq=6, lambda_=0.0001 - train=0.824328 - test=0.7880271998704768\n",
      "d_int=7, d_sq=6, lambda_=0.001 - train=0.821752 - test=0.8112885797852308\n",
      "d_int=7, d_sq=6, lambda_=0.01 - train=0.817236 - test=0.8114064881264541\n",
      "d_int=7, d_sq=6, lambda_=0.1 - train=0.809736 - test=0.805099271784006\n",
      "d_int=7, d_sq=7, lambda_=1e-08 - train=0.829984 - test=0.7940299663169306\n",
      "d_int=7, d_sq=7, lambda_=1e-07 - train=0.82952 - test=0.8186059362450241\n",
      "d_int=7, d_sq=7, lambda_=1e-06 - train=0.827236 - test=0.8167194027854525\n",
      "d_int=7, d_sq=7, lambda_=1e-05 - train=0.826072 - test=0.8178632896779167\n",
      "d_int=7, d_sq=7, lambda_=0.0001 - train=0.824484 - test=0.8103505925334102\n",
      "d_int=7, d_sq=7, lambda_=0.001 - train=0.822136 - test=0.8148891837575101\n",
      "d_int=7, d_sq=7, lambda_=0.01 - train=0.817732 - test=0.8165645380984728\n",
      "d_int=7, d_sq=7, lambda_=0.1 - train=0.81024 - test=0.8093510113719955\n",
      "d_int=8, d_sq=0, lambda_=1e-08 - train=0.820684 - test=0.7847310457941918\n",
      "d_int=8, d_sq=0, lambda_=1e-07 - train=0.82 - test=0.7842171766055772\n",
      "d_int=8, d_sq=0, lambda_=1e-06 - train=0.8187 - test=0.7826896476476406\n",
      "d_int=8, d_sq=0, lambda_=1e-05 - train=0.817336 - test=0.7813345816365678\n",
      "d_int=8, d_sq=0, lambda_=0.0001 - train=0.815608 - test=0.8046276384191131\n",
      "d_int=8, d_sq=0, lambda_=0.001 - train=0.813576 - test=0.8058243200912294\n",
      "d_int=8, d_sq=0, lambda_=0.01 - train=0.811108 - test=0.8053650055082553\n",
      "d_int=8, d_sq=0, lambda_=0.1 - train=0.805092 - test=0.8028132578250663\n",
      "d_int=8, d_sq=1, lambda_=1e-08 - train=0.823256 - test=0.7874763743361056\n",
      "d_int=8, d_sq=1, lambda_=1e-07 - train=0.822992 - test=0.7874798939880825\n",
      "d_int=8, d_sq=1, lambda_=1e-06 - train=0.822564 - test=0.7869501863655721\n",
      "d_int=8, d_sq=1, lambda_=1e-05 - train=0.821292 - test=0.7854912906211834\n",
      "d_int=8, d_sq=1, lambda_=0.0001 - train=0.81958 - test=0.8089321727867549\n",
      "d_int=8, d_sq=1, lambda_=0.001 - train=0.817252 - test=0.8105160161763205\n",
      "d_int=8, d_sq=1, lambda_=0.01 - train=0.814212 - test=0.8090307230421055\n",
      "d_int=8, d_sq=1, lambda_=0.1 - train=0.806832 - test=0.8047015511106261\n",
      "d_int=8, d_sq=2, lambda_=1e-08 - train=0.82858 - test=0.7957387573516731\n",
      "d_int=8, d_sq=2, lambda_=1e-07 - train=0.827892 - test=0.7951932112952671\n",
      "d_int=8, d_sq=2, lambda_=1e-06 - train=0.826124 - test=0.7903343317412774\n",
      "d_int=8, d_sq=2, lambda_=1e-05 - train=0.824608 - test=0.7902551395717992\n",
      "d_int=8, d_sq=2, lambda_=0.0001 - train=0.822496 - test=0.8114469641241874\n",
      "d_int=8, d_sq=2, lambda_=0.001 - train=0.819264 - test=0.8122512046008891\n",
      "d_int=8, d_sq=2, lambda_=0.01 - train=0.815052 - test=0.8100267845515435\n",
      "d_int=8, d_sq=2, lambda_=0.1 - train=0.808164 - test=0.8063522678877513\n",
      "d_int=8, d_sq=3, lambda_=1e-08 - train=0.829264 - test=0.8210679328028044\n",
      "d_int=8, d_sq=3, lambda_=1e-07 - train=0.828328 - test=0.7956982813539397\n",
      "d_int=8, d_sq=3, lambda_=1e-06 - train=0.8267 - test=0.7909379520553008\n",
      "d_int=8, d_sq=3, lambda_=1e-05 - train=0.825552 - test=0.8005606805599063\n",
      "d_int=8, d_sq=3, lambda_=0.0001 - train=0.823832 - test=0.8124465452856021\n",
      "d_int=8, d_sq=3, lambda_=0.001 - train=0.821004 - test=0.8137716942548721\n",
      "d_int=8, d_sq=3, lambda_=0.01 - train=0.816072 - test=0.8075348709519603\n",
      "d_int=8, d_sq=3, lambda_=0.1 - train=0.808476 - test=0.8067763859509571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_int=8, d_sq=4, lambda_=1e-08 - train=0.829488 - test=0.8206614129994826\n",
      "d_int=8, d_sq=4, lambda_=1e-07 - train=0.828744 - test=0.7958144298691745\n",
      "d_int=8, d_sq=4, lambda_=1e-06 - train=0.827104 - test=0.7912124849094921\n",
      "d_int=8, d_sq=4, lambda_=1e-05 - train=0.825668 - test=0.7895758467402744\n",
      "d_int=8, d_sq=4, lambda_=0.0001 - train=0.823948 - test=0.8128988205646226\n",
      "d_int=8, d_sq=4, lambda_=0.001 - train=0.821796 - test=0.8143981923067447\n",
      "d_int=8, d_sq=4, lambda_=0.01 - train=0.817184 - test=0.8094777188431608\n",
      "d_int=8, d_sq=4, lambda_=0.1 - train=0.808724 - test=0.8070861153249167\n",
      "d_int=8, d_sq=5, lambda_=1e-08 - train=0.830272 - test=0.8220551951823004\n",
      "d_int=8, d_sq=5, lambda_=1e-07 - train=0.829532 - test=0.7968685656362299\n",
      "d_int=8, d_sq=5, lambda_=1e-06 - train=0.826808 - test=0.798848369873187\n",
      "d_int=8, d_sq=5, lambda_=1e-05 - train=0.825636 - test=0.78947025718097\n",
      "d_int=8, d_sq=5, lambda_=0.0001 - train=0.824252 - test=0.8128477856109588\n",
      "d_int=8, d_sq=5, lambda_=0.001 - train=0.82214 - test=0.8143313189191853\n",
      "d_int=8, d_sq=5, lambda_=0.01 - train=0.817372 - test=0.8100127059436363\n",
      "d_int=8, d_sq=5, lambda_=0.1 - train=0.808748 - test=0.8071723467983486\n",
      "d_int=8, d_sq=6, lambda_=1e-08 - train=0.830388 - test=0.8142433276197649\n",
      "d_int=8, d_sq=6, lambda_=1e-07 - train=0.829708 - test=0.7971554172723401\n",
      "d_int=8, d_sq=6, lambda_=1e-06 - train=0.827892 - test=0.7954079100658527\n",
      "d_int=8, d_sq=6, lambda_=1e-05 - train=0.82628 - test=0.7900633185390629\n",
      "d_int=8, d_sq=6, lambda_=0.0001 - train=0.824608 - test=0.8129463358663095\n",
      "d_int=8, d_sq=6, lambda_=0.001 - train=0.822248 - test=0.8144949827361071\n",
      "d_int=8, d_sq=6, lambda_=0.01 - train=0.817848 - test=0.8106356843435321\n",
      "d_int=8, d_sq=6, lambda_=0.1 - train=0.810328 - test=0.8090465614760013\n",
      "d_int=8, d_sq=7, lambda_=1e-08 - train=0.770888 - test=0.736281276507379\n",
      "d_int=8, d_sq=7, lambda_=1e-07 - train=0.825192 - test=0.7971994129220503\n",
      "d_int=8, d_sq=7, lambda_=1e-06 - train=0.824432 - test=0.7888842351268307\n",
      "d_int=8, d_sq=7, lambda_=1e-05 - train=0.826244 - test=0.7904328819966282\n",
      "d_int=8, d_sq=7, lambda_=0.0001 - train=0.824812 - test=0.8152253105212957\n",
      "d_int=8, d_sq=7, lambda_=0.001 - train=0.822468 - test=0.8148047121100666\n",
      "d_int=8, d_sq=7, lambda_=0.01 - train=0.817876 - test=0.8104086667910277\n",
      "d_int=8, d_sq=7, lambda_=0.1 - train=0.810516 - test=0.8088776181811143\n",
      "d_int=9, d_sq=0, lambda_=1e-08 - train=0.82582 - test=0.7899788468916193\n",
      "d_int=9, d_sq=0, lambda_=1e-07 - train=0.82436 - test=0.7883228506365291\n",
      "d_int=9, d_sq=0, lambda_=1e-06 - train=0.822032 - test=0.8105916886938219\n",
      "d_int=9, d_sq=0, lambda_=1e-05 - train=0.81906 - test=0.8097399329154333\n",
      "d_int=9, d_sq=0, lambda_=0.0001 - train=0.817528 - test=0.7837138663728931\n",
      "d_int=9, d_sq=0, lambda_=0.001 - train=0.817052 - test=0.7843702814665686\n",
      "d_int=9, d_sq=0, lambda_=0.01 - train=0.814268 - test=0.8045414069456812\n",
      "d_int=9, d_sq=0, lambda_=0.1 - train=0.806744 - test=0.8003495014412975\n",
      "d_int=9, d_sq=1, lambda_=1e-08 - train=0.8266 - test=0.7905701484237239\n",
      "d_int=9, d_sq=1, lambda_=1e-07 - train=0.825292 - test=0.7896392004758569\n",
      "d_int=9, d_sq=1, lambda_=1e-06 - train=0.82396 - test=0.8130589647295675\n",
      "d_int=9, d_sq=1, lambda_=1e-05 - train=0.823056 - test=0.8150070920987332\n",
      "d_int=9, d_sq=1, lambda_=0.0001 - train=0.82188 - test=0.7899894058475498\n",
      "d_int=9, d_sq=1, lambda_=0.001 - train=0.819256 - test=0.7872335183497056\n",
      "d_int=9, d_sq=1, lambda_=0.01 - train=0.8162 - test=0.8063452285837976\n",
      "d_int=9, d_sq=1, lambda_=0.1 - train=0.808244 - test=0.8019034277890602\n",
      "d_int=9, d_sq=2, lambda_=1e-08 - train=0.829392 - test=0.7957827530013832\n",
      "d_int=9, d_sq=2, lambda_=1e-07 - train=0.82846 - test=0.7919516118246228\n",
      "d_int=9, d_sq=2, lambda_=1e-06 - train=0.8268 - test=0.815482245115603\n",
      "d_int=9, d_sq=2, lambda_=1e-05 - train=0.824892 - test=0.8164131930634699\n",
      "d_int=9, d_sq=2, lambda_=0.0001 - train=0.823244 - test=0.8131363970730574\n",
      "d_int=9, d_sq=2, lambda_=0.001 - train=0.820424 - test=0.7881099116919319\n",
      "d_int=9, d_sq=2, lambda_=0.01 - train=0.817056 - test=0.807091394802882\n",
      "d_int=9, d_sq=2, lambda_=0.1 - train=0.809528 - test=0.8035629436961274\n",
      "d_int=9, d_sq=3, lambda_=1e-08 - train=0.830604 - test=0.7967805743368096\n",
      "d_int=9, d_sq=3, lambda_=1e-07 - train=0.829316 - test=0.7932116472323216\n",
      "d_int=9, d_sq=3, lambda_=1e-06 - train=0.827996 - test=0.8168637085165019\n",
      "d_int=9, d_sq=3, lambda_=1e-05 - train=0.825944 - test=0.8179248835875109\n",
      "d_int=9, d_sq=3, lambda_=0.0001 - train=0.824244 - test=0.8157761360556668\n",
      "d_int=9, d_sq=3, lambda_=0.001 - train=0.821244 - test=0.7889423093844481\n",
      "d_int=9, d_sq=3, lambda_=0.01 - train=0.817244 - test=0.807061477761079\n",
      "d_int=9, d_sq=3, lambda_=0.1 - train=0.809636 - test=0.803765323684794\n",
      "d_int=9, d_sq=4, lambda_=1e-08 - train=0.830664 - test=0.7970656661469314\n",
      "d_int=9, d_sq=4, lambda_=1e-07 - train=0.829844 - test=0.7936551233814001\n",
      "d_int=9, d_sq=4, lambda_=1e-06 - train=0.828136 - test=0.8173247829254643\n",
      "d_int=9, d_sq=4, lambda_=1e-05 - train=0.826296 - test=0.8101200553289291\n",
      "d_int=9, d_sq=4, lambda_=0.0001 - train=0.824768 - test=0.8163744768917249\n",
      "d_int=9, d_sq=4, lambda_=0.001 - train=0.822036 - test=0.7896462397798105\n",
      "d_int=9, d_sq=4, lambda_=0.01 - train=0.817748 - test=0.8073254516593399\n",
      "d_int=9, d_sq=4, lambda_=0.1 - train=0.809896 - test=0.8039201883717738\n",
      "d_int=9, d_sq=5, lambda_=1e-08 - train=0.829196 - test=0.7964374082690704\n",
      "d_int=9, d_sq=5, lambda_=1e-07 - train=0.829752 - test=0.7937624727666929\n",
      "d_int=9, d_sq=5, lambda_=1e-06 - train=0.827508 - test=0.8161228217753829\n",
      "d_int=9, d_sq=5, lambda_=1e-05 - train=0.826472 - test=0.8150440484444899\n",
      "d_int=9, d_sq=5, lambda_=0.0001 - train=0.824804 - test=0.8164800664510293\n",
      "d_int=9, d_sq=5, lambda_=0.001 - train=0.82252 - test=0.7897518293391149\n",
      "d_int=9, d_sq=5, lambda_=0.01 - train=0.817692 - test=0.8075190325180646\n",
      "d_int=9, d_sq=5, lambda_=0.1 - train=0.810056 - test=0.8042017605299188\n",
      "d_int=9, d_sq=6, lambda_=1e-08 - train=0.831156 - test=0.7975425789897895\n",
      "d_int=9, d_sq=6, lambda_=1e-07 - train=0.829992 - test=0.793945494669487\n",
      "d_int=9, d_sq=6, lambda_=1e-06 - train=0.82864 - test=0.8175254030881427\n",
      "d_int=9, d_sq=6, lambda_=1e-05 - train=0.82682 - test=0.8101517321967204\n",
      "d_int=9, d_sq=6, lambda_=0.0001 - train=0.825032 - test=0.8164325511493423\n",
      "d_int=9, d_sq=6, lambda_=0.001 - train=0.822372 - test=0.7897201524713237\n",
      "d_int=9, d_sq=6, lambda_=0.01 - train=0.818084 - test=0.8074679975644008\n",
      "d_int=9, d_sq=6, lambda_=0.1 - train=0.810628 - test=0.8048388175377219\n",
      "d_int=9, d_sq=7, lambda_=1e-08 - train=0.816884 - test=0.7662106370922043\n",
      "d_int=9, d_sq=7, lambda_=1e-07 - train=0.830268 - test=0.7940053287530929\n",
      "d_int=9, d_sq=7, lambda_=1e-06 - train=0.828608 - test=0.7912740788190864\n",
      "d_int=9, d_sq=7, lambda_=1e-05 - train=0.82682 - test=0.8148557470637303\n",
      "d_int=9, d_sq=7, lambda_=0.0001 - train=0.824892 - test=0.8166965250476033\n",
      "d_int=9, d_sq=7, lambda_=0.001 - train=0.820008 - test=0.787027618709062\n",
      "d_int=9, d_sq=7, lambda_=0.01 - train=0.818336 - test=0.8019104670930138\n",
      "d_int=9, d_sq=7, lambda_=0.1 - train=0.80996 - test=0.8089444915686737\n",
      "d_int=10, d_sq=0, lambda_=1e-08 - train=0.814744 - test=0.7758896800284388\n",
      "d_int=10, d_sq=0, lambda_=1e-07 - train=0.814224 - test=0.8004691696085091\n",
      "d_int=10, d_sq=0, lambda_=1e-06 - train=0.815084 - test=0.8042387168756754\n",
      "d_int=10, d_sq=0, lambda_=1e-05 - train=0.814584 - test=0.7787089212618656\n",
      "d_int=10, d_sq=0, lambda_=0.0001 - train=0.811964 - test=0.7737268538886874\n",
      "d_int=10, d_sq=0, lambda_=0.001 - train=0.808896 - test=0.7967277795571573\n",
      "d_int=10, d_sq=0, lambda_=0.01 - train=0.80944 - test=0.7979719765309606\n",
      "d_int=10, d_sq=0, lambda_=0.1 - train=0.806688 - test=0.7976904043728156\n",
      "d_int=10, d_sq=1, lambda_=1e-08 - train=0.827632 - test=0.791450061417927\n",
      "d_int=10, d_sq=1, lambda_=1e-07 - train=0.825704 - test=0.8148610265416956\n",
      "d_int=10, d_sq=1, lambda_=1e-06 - train=0.82456 - test=0.8169182631221424\n",
      "d_int=10, d_sq=1, lambda_=1e-05 - train=0.823488 - test=0.790652860245179\n",
      "d_int=10, d_sq=1, lambda_=0.0001 - train=0.82218 - test=0.7861723432786966\n",
      "d_int=10, d_sq=1, lambda_=0.001 - train=0.819884 - test=0.8094935572770564\n",
      "d_int=10, d_sq=1, lambda_=0.01 - train=0.814304 - test=0.8055392282811076\n",
      "d_int=10, d_sq=1, lambda_=0.1 - train=0.80844 - test=0.7987075837941144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_int=10, d_sq=2, lambda_=1e-08 - train=0.829988 - test=0.7929071973363274\n",
      "d_int=10, d_sq=2, lambda_=1e-07 - train=0.82922 - test=0.8174409314406992\n",
      "d_int=10, d_sq=2, lambda_=1e-06 - train=0.827208 - test=0.8188927878811343\n",
      "d_int=10, d_sq=2, lambda_=1e-05 - train=0.825244 - test=0.7917333934020604\n",
      "d_int=10, d_sq=2, lambda_=0.0001 - train=0.82326 - test=0.7952759231167222\n",
      "d_int=10, d_sq=2, lambda_=0.001 - train=0.820824 - test=0.8101869287164886\n",
      "d_int=10, d_sq=2, lambda_=0.01 - train=0.814744 - test=0.8065968837001397\n",
      "d_int=10, d_sq=2, lambda_=0.1 - train=0.809496 - test=0.7992355315906363\n",
      "d_int=10, d_sq=3, lambda_=1e-08 - train=0.830872 - test=0.794482241595951\n",
      "d_int=10, d_sq=3, lambda_=1e-07 - train=0.830308 - test=0.8190564516980561\n",
      "d_int=10, d_sq=3, lambda_=1e-06 - train=0.828488 - test=0.8207388453429725\n",
      "d_int=10, d_sq=3, lambda_=1e-05 - train=0.826524 - test=0.7932222061882521\n",
      "d_int=10, d_sq=3, lambda_=0.0001 - train=0.824112 - test=0.7958777836047571\n",
      "d_int=10, d_sq=3, lambda_=0.001 - train=0.821888 - test=0.8109119770237119\n",
      "d_int=10, d_sq=3, lambda_=0.01 - train=0.813624 - test=0.8071617878424181\n",
      "d_int=10, d_sq=3, lambda_=0.1 - train=0.809672 - test=0.7999535405939061\n",
      "d_int=10, d_sq=4, lambda_=1e-08 - train=0.8311 - test=0.7945297568976379\n",
      "d_int=10, d_sq=4, lambda_=1e-07 - train=0.830136 - test=0.8191356438675343\n",
      "d_int=10, d_sq=4, lambda_=1e-06 - train=0.70994 - test=0.7256976830131037\n",
      "d_int=10, d_sq=4, lambda_=1e-05 - train=0.826724 - test=0.7936357652955276\n",
      "d_int=10, d_sq=4, lambda_=0.0001 - train=0.824872 - test=0.796574674696166\n",
      "d_int=10, d_sq=4, lambda_=0.001 - train=0.822412 - test=0.8113924095185469\n",
      "d_int=10, d_sq=4, lambda_=0.01 - train=0.816428 - test=0.8097540115233406\n",
      "d_int=10, d_sq=4, lambda_=0.1 - train=0.8101 - test=0.8015637813732978\n",
      "d_int=10, d_sq=5, lambda_=1e-08 - train=0.830992 - test=0.7944171280343799\n",
      "d_int=10, d_sq=5, lambda_=1e-07 - train=0.830376 - test=0.8193327443782359\n",
      "d_int=10, d_sq=5, lambda_=1e-06 - train=0.828732 - test=0.8210239371530943\n",
      "d_int=10, d_sq=5, lambda_=1e-05 - train=0.826856 - test=0.7937307958989015\n",
      "d_int=10, d_sq=5, lambda_=0.0001 - train=0.712084 - test=0.6784322766164882\n",
      "d_int=10, d_sq=5, lambda_=0.001 - train=0.819816 - test=0.8075859059056241\n",
      "d_int=10, d_sq=5, lambda_=0.01 - train=0.818268 - test=0.8113677719547091\n",
      "d_int=10, d_sq=5, lambda_=0.1 - train=0.809828 - test=0.8023803406319183\n",
      "d_int=10, d_sq=6, lambda_=1e-08 - train=0.829332 - test=0.8010868685304398\n",
      "d_int=10, d_sq=6, lambda_=1e-07 - train=0.83054 - test=0.819237713774862\n",
      "d_int=10, d_sq=6, lambda_=1e-06 - train=0.828788 - test=0.8207054086491927\n",
      "d_int=10, d_sq=6, lambda_=1e-05 - train=0.826872 - test=0.7935776910379102\n",
      "d_int=10, d_sq=6, lambda_=0.0001 - train=0.82518 - test=0.7918231445274692\n",
      "d_int=10, d_sq=6, lambda_=0.001 - train=0.822904 - test=0.8120699425240833\n",
      "d_int=10, d_sq=6, lambda_=0.01 - train=0.818132 - test=0.8166402106159742\n",
      "d_int=10, d_sq=6, lambda_=0.1 - train=0.810796 - test=0.803513668568452\n",
      "d_int=10, d_sq=7, lambda_=1e-08 - train=0.831172 - test=0.802123406037611\n",
      "d_int=10, d_sq=7, lambda_=1e-07 - train=0.830028 - test=0.8178720888078587\n",
      "d_int=10, d_sq=7, lambda_=1e-06 - train=0.826232 - test=0.8126049296245588\n",
      "d_int=10, d_sq=7, lambda_=1e-05 - train=0.81604 - test=0.7838528926259771\n",
      "d_int=10, d_sq=7, lambda_=0.0001 - train=0.824964 - test=0.7918002667896198\n",
      "d_int=10, d_sq=7, lambda_=0.001 - train=0.815156 - test=0.781209633991391\n",
      "d_int=10, d_sq=7, lambda_=0.01 - train=0.81858 - test=0.8114645623840715\n",
      "d_int=10, d_sq=7, lambda_=0.1 - train=0.810776 - test=0.8036298170836867\n",
      "d_int=11, d_sq=0, lambda_=1e-08 - train=0.828084 - test=0.816738760871325\n",
      "d_int=11, d_sq=0, lambda_=1e-07 - train=0.826848 - test=0.818627054156885\n",
      "d_int=11, d_sq=0, lambda_=1e-06 - train=0.824888 - test=0.7913092753388545\n",
      "d_int=11, d_sq=0, lambda_=1e-05 - train=0.823408 - test=0.7866580552514967\n",
      "d_int=11, d_sq=0, lambda_=0.0001 - train=0.82198 - test=0.8108398241581872\n",
      "d_int=11, d_sq=0, lambda_=0.001 - train=0.819216 - test=0.8118746018393701\n",
      "d_int=11, d_sq=0, lambda_=0.01 - train=0.816692 - test=0.7846993689264006\n",
      "d_int=11, d_sq=0, lambda_=0.1 - train=0.807096 - test=0.7942165078717016\n",
      "d_int=11, d_sq=1, lambda_=1e-08 - train=0.82584 - test=0.8146480875970984\n",
      "d_int=11, d_sq=1, lambda_=1e-07 - train=0.82294 - test=0.8153660966003682\n",
      "d_int=11, d_sq=1, lambda_=1e-06 - train=0.784364 - test=0.7448375504630103\n",
      "d_int=11, d_sq=1, lambda_=1e-05 - train=0.819188 - test=0.7818326123912868\n",
      "d_int=11, d_sq=1, lambda_=0.0001 - train=0.822396 - test=0.8116863004586107\n",
      "d_int=11, d_sq=1, lambda_=0.001 - train=0.82054 - test=0.8133088600199212\n",
      "d_int=11, d_sq=1, lambda_=0.01 - train=0.817152 - test=0.7855880810505457\n",
      "d_int=11, d_sq=1, lambda_=0.1 - train=0.808552 - test=0.7967277795571573\n",
      "d_int=11, d_sq=2, lambda_=1e-08 - train=0.830092 - test=0.8179970364530356\n",
      "d_int=11, d_sq=2, lambda_=1e-07 - train=0.829712 - test=0.8208849109000103\n",
      "d_int=11, d_sq=2, lambda_=1e-06 - train=0.827712 - test=0.7942622633474002\n",
      "d_int=11, d_sq=2, lambda_=1e-05 - train=0.825484 - test=0.7885428288850799\n",
      "d_int=11, d_sq=2, lambda_=0.0001 - train=0.82332 - test=0.8119819512246629\n",
      "d_int=11, d_sq=2, lambda_=0.001 - train=0.82098 - test=0.813697781563359\n",
      "d_int=11, d_sq=2, lambda_=0.01 - train=0.817916 - test=0.7861283476289864\n",
      "d_int=11, d_sq=2, lambda_=0.1 - train=0.81002 - test=0.7989029244788275\n",
      "d_int=11, d_sq=3, lambda_=1e-08 - train=0.831184 - test=0.8274772190525801\n",
      "d_int=11, d_sq=3, lambda_=1e-07 - train=0.83014 - test=0.8225285883731817\n",
      "d_int=11, d_sq=3, lambda_=1e-06 - train=0.829164 - test=0.7963529366216269\n",
      "d_int=11, d_sq=3, lambda_=1e-05 - train=0.826668 - test=0.7901425107085411\n",
      "d_int=11, d_sq=3, lambda_=0.0001 - train=0.824308 - test=0.8128319471770631\n",
      "d_int=11, d_sq=3, lambda_=0.001 - train=0.820892 - test=0.813333497583759\n",
      "d_int=11, d_sq=3, lambda_=0.01 - train=0.818156 - test=0.7863184088357343\n",
      "d_int=11, d_sq=3, lambda_=0.1 - train=0.810432 - test=0.7989574790844681\n",
      "d_int=11, d_sq=4, lambda_=1e-08 - train=0.831416 - test=0.8193485828121315\n",
      "d_int=11, d_sq=4, lambda_=1e-07 - train=0.830484 - test=0.8142415677937765\n",
      "d_int=11, d_sq=4, lambda_=1e-06 - train=0.82922 - test=0.7961787138487746\n",
      "d_int=11, d_sq=4, lambda_=1e-05 - train=0.826896 - test=0.7985386404992274\n",
      "d_int=11, d_sq=4, lambda_=0.0001 - train=0.825176 - test=0.8135464365283561\n",
      "d_int=11, d_sq=4, lambda_=0.001 - train=0.822584 - test=0.8147079216807042\n",
      "d_int=11, d_sq=4, lambda_=0.01 - train=0.818268 - test=0.7864028804831779\n",
      "d_int=11, d_sq=4, lambda_=0.1 - train=0.810432 - test=0.7976534480270591\n",
      "d_int=11, d_sq=5, lambda_=1e-08 - train=0.831364 - test=0.8273540312333917\n",
      "d_int=11, d_sq=5, lambda_=1e-07 - train=0.830416 - test=0.8223015708206772\n",
      "d_int=11, d_sq=5, lambda_=1e-06 - train=0.829484 - test=0.7963722947074994\n",
      "d_int=11, d_sq=5, lambda_=1e-05 - train=0.827088 - test=0.7906123842474456\n",
      "d_int=11, d_sq=5, lambda_=0.0001 - train=0.824976 - test=0.8133686941035271\n",
      "d_int=11, d_sq=5, lambda_=0.001 - train=0.822944 - test=0.8150880440942\n",
      "d_int=11, d_sq=5, lambda_=0.01 - train=0.818532 - test=0.7865999809938793\n",
      "d_int=11, d_sq=5, lambda_=0.1 - train=0.810568 - test=0.7958584255188846\n",
      "d_int=11, d_sq=6, lambda_=1e-08 - train=0.831148 - test=0.8142626857056374\n",
      "d_int=11, d_sq=6, lambda_=1e-07 - train=0.829132 - test=0.8210996096705958\n",
      "d_int=11, d_sq=6, lambda_=1e-06 - train=0.826848 - test=0.8120611433941411\n",
      "d_int=11, d_sq=6, lambda_=1e-05 - train=0.82724 - test=0.7907126943287848\n",
      "d_int=11, d_sq=6, lambda_=0.0001 - train=0.825344 - test=0.8137013012153358\n",
      "d_int=11, d_sq=6, lambda_=0.001 - train=0.822936 - test=0.8069998838514848\n",
      "d_int=11, d_sq=6, lambda_=0.01 - train=0.817416 - test=0.7852009193330963\n",
      "d_int=11, d_sq=6, lambda_=0.1 - train=0.81072 - test=0.79478141201398\n",
      "d_int=11, d_sq=7, lambda_=1e-08 - train=0.83068 - test=0.7965922729560501\n",
      "d_int=11, d_sq=7, lambda_=1e-07 - train=0.822948 - test=0.7900404408012136\n",
      "d_int=11, d_sq=7, lambda_=1e-06 - train=0.819324 - test=0.7643241036326328\n",
      "d_int=11, d_sq=7, lambda_=1e-05 - train=0.781624 - test=0.74692470408526\n",
      "d_int=11, d_sq=7, lambda_=0.0001 - train=0.822212 - test=0.7858397361668878\n",
      "d_int=11, d_sq=7, lambda_=0.001 - train=0.822668 - test=0.8069066130740993\n",
      "d_int=11, d_sq=7, lambda_=0.01 - train=0.815788 - test=0.7839936787050497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_int=11, d_sq=7, lambda_=0.1 - train=0.810932 - test=0.795131617385673\n",
      "d_int=12, d_sq=0, lambda_=1e-08 - train=0.774512 - test=0.7394243257226726\n",
      "d_int=12, d_sq=0, lambda_=1e-07 - train=0.744772 - test=0.7242493462246453\n",
      "d_int=12, d_sq=0, lambda_=1e-06 - train=0.81668 - test=0.7805778564615531\n",
      "d_int=12, d_sq=0, lambda_=1e-05 - train=0.822004 - test=0.8099071163843319\n",
      "d_int=12, d_sq=0, lambda_=0.0001 - train=0.821472 - test=0.8132384669803849\n",
      "d_int=12, d_sq=0, lambda_=0.001 - train=0.818964 - test=0.7866263783837054\n",
      "d_int=12, d_sq=0, lambda_=0.01 - train=0.816488 - test=0.8059651061703019\n",
      "d_int=12, d_sq=0, lambda_=0.1 - train=0.807112 - test=0.8051291888258089\n",
      "d_int=12, d_sq=1, lambda_=1e-08 - train=0.827164 - test=0.7988131733534188\n",
      "d_int=12, d_sq=1, lambda_=1e-07 - train=0.82486 - test=0.8218528151936336\n",
      "d_int=12, d_sq=1, lambda_=1e-06 - train=0.825692 - test=0.8146269696852375\n",
      "d_int=12, d_sq=1, lambda_=1e-05 - train=0.8248 - test=0.8134496460989937\n",
      "d_int=12, d_sq=1, lambda_=0.0001 - train=0.823316 - test=0.8151848345235623\n",
      "d_int=12, d_sq=1, lambda_=0.001 - train=0.821124 - test=0.7886959337460712\n",
      "d_int=12, d_sq=1, lambda_=0.01 - train=0.817604 - test=0.8075348709519603\n",
      "d_int=12, d_sq=1, lambda_=0.1 - train=0.80844 - test=0.8068080628187485\n",
      "d_int=12, d_sq=2, lambda_=1e-08 - train=0.829572 - test=0.8259197730528406\n",
      "d_int=12, d_sq=2, lambda_=1e-07 - train=0.828512 - test=0.8248181219840982\n",
      "d_int=12, d_sq=2, lambda_=1e-06 - train=0.8282 - test=0.7916295636687445\n",
      "d_int=12, d_sq=2, lambda_=1e-05 - train=0.825792 - test=0.8139177598119098\n",
      "d_int=12, d_sq=2, lambda_=0.0001 - train=0.823868 - test=0.8156388696285711\n",
      "d_int=12, d_sq=2, lambda_=0.001 - train=0.821592 - test=0.7891534885030568\n",
      "d_int=12, d_sq=2, lambda_=0.01 - train=0.818388 - test=0.8081402510919721\n",
      "d_int=12, d_sq=2, lambda_=0.1 - train=0.8101 - test=0.8036650136034549\n",
      "d_int=12, d_sq=3, lambda_=1e-08 - train=0.830264 - test=0.81364146713173\n",
      "d_int=12, d_sq=3, lambda_=1e-07 - train=0.829364 - test=0.8212720726174596\n",
      "d_int=12, d_sq=3, lambda_=1e-06 - train=0.82908 - test=0.7929124768142926\n",
      "d_int=12, d_sq=3, lambda_=1e-05 - train=0.82684 - test=0.815105642354084\n",
      "d_int=12, d_sq=3, lambda_=0.0001 - train=0.824276 - test=0.8158236513573538\n",
      "d_int=12, d_sq=3, lambda_=0.001 - train=0.822188 - test=0.7895318510905641\n",
      "d_int=12, d_sq=3, lambda_=0.01 - train=0.818372 - test=0.8081948056976126\n",
      "d_int=12, d_sq=3, lambda_=0.1 - train=0.81048 - test=0.8087121945382041\n",
      "d_int=12, d_sq=4, lambda_=1e-08 - train=0.830568 - test=0.8187907179738068\n",
      "d_int=12, d_sq=4, lambda_=1e-07 - train=0.829504 - test=0.8128425061329936\n",
      "d_int=12, d_sq=4, lambda_=1e-06 - train=0.82932 - test=0.7961769540227862\n",
      "d_int=12, d_sq=4, lambda_=1e-05 - train=0.827272 - test=0.8154400092918812\n",
      "d_int=12, d_sq=4, lambda_=0.0001 - train=0.81364 - test=0.7963705348815109\n",
      "d_int=12, d_sq=4, lambda_=0.001 - train=0.822816 - test=0.7899102136780715\n",
      "d_int=12, d_sq=4, lambda_=0.01 - train=0.81864 - test=0.8082634389111605\n",
      "d_int=12, d_sq=4, lambda_=0.1 - train=0.81062 - test=0.8087069150602388\n",
      "d_int=12, d_sq=5, lambda_=1e-08 - train=0.830012 - test=0.818120224272224\n",
      "d_int=12, d_sq=5, lambda_=1e-07 - train=0.828756 - test=0.7872739943474389\n",
      "d_int=12, d_sq=5, lambda_=1e-06 - train=0.827876 - test=0.7864715136967256\n",
      "d_int=12, d_sq=5, lambda_=1e-05 - train=0.826668 - test=0.8151584371337363\n",
      "d_int=12, d_sq=5, lambda_=0.0001 - train=0.825236 - test=0.8167581189571975\n",
      "d_int=12, d_sq=5, lambda_=0.001 - train=0.82288 - test=0.7902903360915673\n",
      "d_int=12, d_sq=5, lambda_=0.01 - train=0.81878 - test=0.8084165437721518\n",
      "d_int=12, d_sq=5, lambda_=0.1 - train=0.809248 - test=0.8073324909632935\n",
      "d_int=12, d_sq=6, lambda_=1e-08 - train=0.723532 - test=0.6921800372379179\n",
      "d_int=12, d_sq=6, lambda_=1e-07 - train=0.810796 - test=0.7846412946687832\n",
      "d_int=12, d_sq=6, lambda_=1e-06 - train=0.829456 - test=0.7882806148128073\n",
      "d_int=12, d_sq=6, lambda_=1e-05 - train=0.8274 - test=0.7908693188417529\n",
      "d_int=12, d_sq=6, lambda_=0.0001 - train=0.825192 - test=0.8168073940848729\n",
      "d_int=12, d_sq=6, lambda_=0.001 - train=0.82312 - test=0.7903202531333702\n",
      "d_int=12, d_sq=6, lambda_=0.01 - train=0.80932 - test=0.7861758629306734\n",
      "d_int=12, d_sq=6, lambda_=0.1 - train=0.81058 - test=0.8007507417666541\n",
      "d_int=12, d_sq=7, lambda_=1e-08 - train=0.81288 - test=0.7790098515058831\n",
      "d_int=12, d_sq=7, lambda_=1e-07 - train=0.81338 - test=0.778064824950109\n",
      "d_int=12, d_sq=7, lambda_=1e-06 - train=0.818512 - test=0.7783675150201148\n",
      "d_int=12, d_sq=7, lambda_=1e-05 - train=0.826832 - test=0.8102925182757929\n",
      "d_int=12, d_sq=7, lambda_=0.0001 - train=0.825288 - test=0.8135165194865531\n",
      "d_int=12, d_sq=7, lambda_=0.001 - train=0.822984 - test=0.7902516199198223\n",
      "d_int=12, d_sq=7, lambda_=0.01 - train=0.81866 - test=0.8023275458522662\n",
      "d_int=12, d_sq=7, lambda_=0.1 - train=0.811032 - test=0.80105343183666\n"
     ]
    }
   ],
   "source": [
    "d_ints = range(5, 13)\n",
    "d_sqs = range(0, 8)\n",
    "lambdas = [ 10**c for c in range(-8,0) ]\n",
    "\n",
    "max_train = 0\n",
    "max_train_d_int = 0\n",
    "max_train_d_sq = 0\n",
    "max_train_d_lambda = 0\n",
    "\n",
    "max_test = 0\n",
    "max_test_d_int = 0\n",
    "max_test_d_sq = 0\n",
    "max_test_d_lambda = 0\n",
    "\n",
    "\n",
    "for d_int in d_ints:\n",
    "    for d_sq in d_sqs:\n",
    "        X_split_poly = [ build_X(X, d_int, d_sq) for X in X_split ]\n",
    "        X_test_split_poly = [ build_X(X, d_int, d_sq) for X in X_test_split ]\n",
    "        \n",
    "        for lambda_ in lambdas:\n",
    "            \n",
    "            models = []\n",
    "            y_pred = np.ones(tX.shape[0])\n",
    "\n",
    "            for i in range(len(X_split_poly)):\n",
    "                lse = LeastSquaresL2(lambda_)\n",
    "                lse.fit(y_split[i], X_split_poly[i])\n",
    "                models.append(lse)\n",
    "                y_pred[indices_split[i]] = lse.predict(X_split_poly[i])\n",
    "\n",
    "            acc_train = np.mean(y == y_pred)\n",
    "            if acc_train > max_train:\n",
    "                max_train = acc_train\n",
    "                max_train_d_int = d_int\n",
    "                max_train_d_sq = d_sq\n",
    "                max_train_d_lambda = lambda_\n",
    "            \n",
    "            y_pred = np.ones(tX_test.shape[0])\n",
    "\n",
    "            for model, X, indices in zip(models, X_test_split_poly, test_split_indices):\n",
    "                y_pred[indices] = model.predict(X)\n",
    "\n",
    "            acc_test = np.mean(y_test == y_pred)\n",
    "            if acc_test > max_test:\n",
    "                max_test = acc_test\n",
    "                max_test_d_int = d_int\n",
    "                max_test_d_sq = d_sq\n",
    "                max_test_d_lambda = lambda_\n",
    "            \n",
    "            print(f\"d_int={d_int}, d_sq={d_sq}, lambda_={lambda_} - train={acc_train} - test={acc_test}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_int=11, d_sq=3, lambda_=1e-09 - train=0.83164 - test=0.8009337636694484\n",
      "d_int=11, d_sq=3, lambda_=1.0092715146305698e-09 - train=0.831636 - test=0.800949602103344\n",
      "d_int=11, d_sq=3, lambda_=1.0186289902446886e-09 - train=0.831636 - test=0.8009056064536338\n",
      "d_int=11, d_sq=3, lambda_=1.0280732238308647e-09 - train=0.831624 - test=0.8003442219633322\n",
      "d_int=11, d_sq=3, lambda_=1.0376050197669096e-09 - train=0.831624 - test=0.8004867678683931\n",
      "d_int=11, d_sq=3, lambda_=1.0472251898884355e-09 - train=0.831624 - test=0.8007595408965962\n",
      "d_int=11, d_sq=3, lambda_=1.0569345535579873e-09 - train=0.83162 - test=0.8007665802005498\n",
      "d_int=11, d_sq=3, lambda_=1.0667339377348592e-09 - train=0.831608 - test=0.8004832482164164\n",
      "d_int=11, d_sq=3, lambda_=1.0766241770454932e-09 - train=0.831628 - test=0.8009179252355527\n",
      "d_int=11, d_sq=3, lambda_=1.0866061138545958e-09 - train=0.831628 - test=0.800715545246886\n",
      "d_int=11, d_sq=3, lambda_=1.0966805983368696e-09 - train=0.831616 - test=0.8003547809192627\n",
      "d_int=11, d_sq=3, lambda_=1.1068484885494119e-09 - train=0.831616 - test=0.8009619208852629\n",
      "d_int=11, d_sq=3, lambda_=1.1171106505048218e-09 - train=0.831628 - test=0.8008827287157846\n",
      "d_int=11, d_sq=3, lambda_=1.1274679582449472e-09 - train=0.831628 - test=0.8007278640288048\n",
      "d_int=11, d_sq=3, lambda_=1.137921293915314e-09 - train=0.831624 - test=0.8011906982637557\n",
      "d_int=11, d_sq=3, lambda_=1.1484715478402913e-09 - train=0.831624 - test=0.8011537419179992\n",
      "d_int=11, d_sq=3, lambda_=1.1591196185988857e-09 - train=0.831632 - test=0.8006539513372918\n",
      "d_int=11, d_sq=3, lambda_=1.1698664131013058e-09 - train=0.831624 - test=0.8009847986231121\n",
      "d_int=11, d_sq=3, lambda_=1.1807128466661915e-09 - train=0.831636 - test=0.8008968073236918\n",
      "d_int=11, d_sq=3, lambda_=1.1916598430985587e-09 - train=0.831628 - test=0.8012276546095122\n",
      "d_int=11, d_sq=3, lambda_=1.2027083347685096e-09 - train=0.831628 - test=0.8011291043541615\n",
      "d_int=11, d_sq=3, lambda_=1.213859262690629e-09 - train=0.831632 - test=0.8009619208852629\n",
      "d_int=11, d_sq=3, lambda_=1.2251135766041177e-09 - train=0.83162 - test=0.801248772521373\n",
      "d_int=11, d_sq=3, lambda_=1.2364722350537176e-09 - train=0.831612 - test=0.8010235147948571\n",
      "d_int=11, d_sq=3, lambda_=1.2479362054713115e-09 - train=0.831608 - test=0.8015391438094601\n",
      "d_int=11, d_sq=3, lambda_=1.2595064642583565e-09 - train=0.831596 - test=0.8013702005145731\n",
      "d_int=11, d_sq=3, lambda_=1.2711839968690302e-09 - train=0.831596 - test=0.8012382135654427\n",
      "d_int=11, d_sq=3, lambda_=1.2829697978941476e-09 - train=0.831576 - test=0.8010551916626484\n",
      "d_int=11, d_sq=3, lambda_=1.2948648711459022e-09 - train=0.831588 - test=0.8259831267884231\n",
      "d_int=11, d_sq=3, lambda_=1.3068702297433476e-09 - train=0.8316 - test=0.8261907862550552\n",
      "d_int=11, d_sq=3, lambda_=1.3189868961986692e-09 - train=0.831584 - test=0.8261960657330203\n",
      "d_int=11, d_sq=3, lambda_=1.3312159025043104e-09 - train=0.831584 - test=0.8260306420901101\n",
      "d_int=11, d_sq=3, lambda_=1.3435582902208263e-09 - train=0.83158 - test=0.826099275303658\n",
      "d_int=11, d_sq=3, lambda_=1.356015110565632e-09 - train=0.831576 - test=0.8261503102573218\n",
      "d_int=11, d_sq=3, lambda_=1.3685874245025205e-09 - train=0.831596 - test=0.8263685286798841\n",
      "d_int=11, d_sq=3, lambda_=1.3812763028320094e-09 - train=0.831576 - test=0.8258986551409797\n",
      "d_int=11, d_sq=3, lambda_=1.3940828262825757e-09 - train=0.83156 - test=0.8262154238188928\n",
      "d_int=11, d_sq=3, lambda_=1.4070080856026864e-09 - train=0.831572 - test=0.8264741182391885\n",
      "d_int=11, d_sq=3, lambda_=1.4200531816536817e-09 - train=0.831556 - test=0.8261467906053449\n",
      "d_int=11, d_sq=3, lambda_=1.433219225503577e-09 - train=0.831572 - test=0.826004244700284\n",
      "d_int=11, d_sq=3, lambda_=1.4465073385216472e-09 - train=0.831536 - test=0.8257666681918492\n",
      "d_int=11, d_sq=3, lambda_=1.4599186524739771e-09 - train=0.831552 - test=0.8262013452109855\n",
      "d_int=11, d_sq=3, lambda_=1.4734543096198374e-09 - train=0.831544 - test=0.826004244700284\n",
      "d_int=11, d_sq=3, lambda_=1.4871154628089537e-09 - train=0.831544 - test=0.8262576596426146\n",
      "d_int=11, d_sq=3, lambda_=1.5009032755797335e-09 - train=0.83154 - test=0.8263579697239537\n",
      "d_int=11, d_sq=3, lambda_=1.5148189222583473e-09 - train=0.831524 - test=0.8261010351296464\n",
      "d_int=11, d_sq=3, lambda_=1.5288635880587294e-09 - train=0.831536 - test=0.8263192535522088\n",
      "d_int=11, d_sq=3, lambda_=1.5430384691835677e-09 - train=0.831532 - test=0.826116873563542\n",
      "d_int=11, d_sq=3, lambda_=1.5573447729261352e-09 - train=0.83154 - test=0.8260095241782492\n",
      "d_int=11, d_sq=3, lambda_=1.571783717773161e-09 - train=0.831548 - test=0.8260200831341797\n",
      "d_int=11, d_sq=3, lambda_=1.5863565335085928e-09 - train=0.831528 - test=0.8262154238188928\n",
      "d_int=11, d_sq=3, lambda_=1.6010644613183177e-09 - train=0.831528 - test=0.8261379914754029\n",
      "d_int=11, d_sq=3, lambda_=1.615908753895916e-09 - train=0.831528 - test=0.8262558998166262\n",
      "d_int=11, d_sq=3, lambda_=1.6308906755493343e-09 - train=0.831536 - test=0.8264142841555827\n",
      "d_int=11, d_sq=3, lambda_=1.6460115023085497e-09 - train=0.831536 - test=0.8264442011973856\n",
      "d_int=11, d_sq=3, lambda_=1.6612725220342897e-09 - train=0.831536 - test=0.8263192535522088\n",
      "d_int=11, d_sq=3, lambda_=1.676675034527701e-09 - train=0.83154 - test=0.8264617994572697\n",
      "d_int=11, d_sq=3, lambda_=1.6922203516410356e-09 - train=0.831536 - test=0.8263931662437218\n",
      "d_int=11, d_sq=3, lambda_=1.7079097973894303e-09 - train=0.831536 - test=0.8264829173691305\n",
      "d_int=11, d_sq=3, lambda_=1.7237447080636198e-09 - train=0.83154 - test=0.8263738081578493\n",
      "d_int=11, d_sq=3, lambda_=1.7397264323437989e-09 - train=0.831548 - test=0.826511074584945\n",
      "d_int=11, d_sq=3, lambda_=1.7558563314144705e-09 - train=0.831544 - test=0.8266078650143074\n",
      "d_int=11, d_sq=3, lambda_=1.7721357790803584e-09 - train=0.831548 - test=0.8263949260697102\n",
      "d_int=11, d_sq=3, lambda_=1.7885661618834582e-09 - train=0.831544 - test=0.8265445112787247\n",
      "d_int=11, d_sq=3, lambda_=1.80514887922111e-09 - train=0.831544 - test=0.8267222537035538\n",
      "d_int=11, d_sq=3, lambda_=1.8218853434651652e-09 - train=0.831548 - test=0.8263861269397682\n",
      "d_int=11, d_sq=3, lambda_=1.8387769800823307e-09 - train=0.831552 - test=0.8264617994572697\n",
      "d_int=11, d_sq=3, lambda_=1.8558252277555189e-09 - train=0.831568 - test=0.8264477208493625\n",
      "d_int=11, d_sq=3, lambda_=1.8730315385064348e-09 - train=0.831556 - test=0.8268120048289626\n",
      "d_int=11, d_sq=3, lambda_=1.8903973778192234e-09 - train=0.831556 - test=0.826567389016574\n",
      "d_int=11, d_sq=3, lambda_=1.907924224765265e-09 - train=0.831564 - test=0.8268243236108813\n",
      "d_int=11, d_sq=3, lambda_=1.9256135721291947e-09 - train=0.831576 - test=0.8268260834368698\n",
      "d_int=11, d_sq=3, lambda_=1.9434669265360222e-09 - train=0.831576 - test=0.8266201837962263\n",
      "d_int=11, d_sq=3, lambda_=1.9614858085794296e-09 - train=0.831572 - test=0.826502275455003\n",
      "d_int=11, d_sq=3, lambda_=1.9796717529513368e-09 - train=0.831568 - test=0.8264758780651769\n",
      "d_int=11, d_sq=3, lambda_=1.998026308572551e-09 - train=0.831564 - test=0.8268401620447771\n",
      "d_int=11, d_sq=3, lambda_=2.0165510387247444e-09 - train=0.83156 - test=0.8267416117894263\n",
      "d_int=11, d_sq=3, lambda_=2.0352475211835797e-09 - train=0.831556 - test=0.8268225637848929\n",
      "d_int=11, d_sq=3, lambda_=2.0541173483530643e-09 - train=0.831552 - test=0.8267028956176813\n",
      "d_int=11, d_sq=3, lambda_=2.073162127401227e-09 - train=0.831552 - test=0.8268348825668118\n",
      "d_int=11, d_sq=3, lambda_=2.0923834803969792e-09 - train=0.831548 - test=0.8266377820561103\n",
      "d_int=11, d_sq=3, lambda_=2.111783044448242e-09 - train=0.831532 - test=0.8266676990979132\n",
      "d_int=11, d_sq=3, lambda_=2.1313624718414418e-09 - train=0.83152 - test=0.8268243236108813\n",
      "d_int=11, d_sq=3, lambda_=2.151123430182167e-09 - train=0.831516 - test=0.8267292930075074\n",
      "d_int=11, d_sq=3, lambda_=2.1710676025372625e-09 - train=0.831528 - test=0.8267152143996002\n",
      "d_int=11, d_sq=3, lambda_=2.191196687578152e-09 - train=0.831516 - test=0.8267891270911132\n",
      "d_int=11, d_sq=3, lambda_=2.211512399725489e-09 - train=0.831508 - test=0.8268190441329162\n",
      "d_int=11, d_sq=3, lambda_=2.2320164692952302e-09 - train=0.83152 - test=0.8268313629148349\n",
      "d_int=11, d_sq=3, lambda_=2.2527106426459826e-09 - train=0.831516 - test=0.8268120048289626\n",
      "d_int=11, d_sq=3, lambda_=2.273596682327715e-09 - train=0.831504 - test=0.8268859175204756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_int=11, d_sq=3, lambda_=2.294676367231941e-09 - train=0.83152 - test=0.8267310528334958\n",
      "d_int=11, d_sq=3, lambda_=2.3159514927431545e-09 - train=0.831516 - test=0.8268137646549509\n",
      "d_int=11, d_sq=3, lambda_=2.3374238708918124e-09 - train=0.831512 - test=0.8267856074391364\n",
      "d_int=11, d_sq=3, lambda_=2.3590953305086387e-09 - train=0.831508 - test=0.8268577603046611\n",
      "d_int=11, d_sq=3, lambda_=2.3809677173803584e-09 - train=0.831516 - test=0.8267873672651248\n",
      "d_int=11, d_sq=3, lambda_=2.4030428944069646e-09 - train=0.831512 - test=0.8268032056990204\n",
      "d_int=11, d_sq=3, lambda_=2.4253227417603556e-09 - train=0.8315 - test=0.826783847613148\n",
      "d_int=11, d_sq=3, lambda_=2.4478091570444403e-09 - train=0.831508 - test=0.8268243236108813\n",
      "d_int=11, d_sq=3, lambda_=2.4705040554568205e-09 - train=0.831508 - test=0.8267908869171017\n",
      "d_int=11, d_sq=3, lambda_=2.4934093699518803e-09 - train=0.831512 - test=0.8267785681351828\n",
      "d_int=11, d_sq=3, lambda_=2.516527051405389e-09 - train=0.831508 - test=0.8268137646549509\n",
      "d_int=11, d_sq=3, lambda_=2.5398590687807293e-09 - train=0.831516 - test=0.8268560004786727\n",
      "d_int=11, d_sq=3, lambda_=2.563407409296515e-09 - train=0.831508 - test=0.8268384022187886\n",
      "d_int=11, d_sq=3, lambda_=2.5871740785959187e-09 - train=0.831508 - test=0.826887677346464\n",
      "d_int=11, d_sq=3, lambda_=2.6111611009174623e-09 - train=0.831504 - test=0.8268384022187886\n",
      "d_int=11, d_sq=3, lambda_=2.635370519267393e-09 - train=0.831496 - test=0.8269158345622785\n",
      "d_int=11, d_sq=3, lambda_=2.659804395593764e-09 - train=0.831496 - test=0.8269686293419307\n",
      "d_int=11, d_sq=3, lambda_=2.6844648109619653e-09 - train=0.8315 - test=0.8269563105600118\n",
      "d_int=11, d_sq=3, lambda_=2.7093538657320487e-09 - train=0.831512 - test=0.8268911969984408\n",
      "d_int=11, d_sq=3, lambda_=2.7344736797375855e-09 - train=0.8315 - test=0.8268366423928002\n",
      "d_int=11, d_sq=3, lambda_=2.7598263924661805e-09 - train=0.831508 - test=0.8268208039589046\n",
      "d_int=11, d_sq=3, lambda_=2.785414163241763e-09 - train=0.831488 - test=0.8270882975091423\n",
      "d_int=11, d_sq=3, lambda_=2.811239171408467e-09 - train=0.831504 - test=0.826878878216522\n",
      "d_int=11, d_sq=3, lambda_=2.8373036165162115e-09 - train=0.831472 - test=0.8269615900379771\n",
      "d_int=11, d_sq=3, lambda_=2.86360971850811e-09 - train=0.831468 - test=0.827000306209722\n",
      "d_int=11, d_sq=3, lambda_=2.8901597179095114e-09 - train=0.831476 - test=0.8268665594346031\n",
      "d_int=11, d_sq=3, lambda_=2.916955876018793e-09 - train=0.831468 - test=0.8269985463837336\n",
      "d_int=11, d_sq=3, lambda_=2.94400047510004e-09 - train=0.831456 - test=0.8270865376831539\n",
      "d_int=11, d_sq=3, lambda_=2.971295818577334e-09 - train=0.831452 - test=0.8269140747362901\n",
      "d_int=11, d_sq=3, lambda_=2.9988442312310246e-09 - train=0.831452 - test=0.8269862276018147\n",
      "d_int=11, d_sq=3, lambda_=3.0266480593956955e-09 - train=0.831448 - test=0.8270319830775132\n",
      "d_int=11, d_sq=3, lambda_=3.0547096711599683e-09 - train=0.831448 - test=0.8269175943882668\n",
      "d_int=11, d_sq=3, lambda_=3.0830314565682706e-09 - train=0.831444 - test=0.8271023761170495\n",
      "d_int=11, d_sq=3, lambda_=3.111615827824363e-09 - train=0.831448 - test=0.8269862276018147\n",
      "d_int=11, d_sq=3, lambda_=3.140465219496749e-09 - train=0.83144 - test=0.8270055856876872\n",
      "d_int=11, d_sq=3, lambda_=3.1695820887261217e-09 - train=0.83144 - test=0.8270724590752466\n",
      "d_int=11, d_sq=3, lambda_=3.198968915434538e-09 - train=0.831432 - test=0.8269475114300698\n",
      "d_int=11, d_sq=3, lambda_=3.228628202536727e-09 - train=0.831444 - test=0.8269967865577452\n",
      "d_int=11, d_sq=3, lambda_=3.25856247615323e-09 - train=0.831432 - test=0.8270953368130959\n",
      "d_int=11, d_sq=3, lambda_=3.28877428582551e-09 - train=0.831428 - test=0.8269844677758263\n",
      "d_int=11, d_sq=3, lambda_=3.319266204733183e-09 - train=0.831436 - test=0.8270601402933278\n",
      "d_int=11, d_sq=3, lambda_=3.350040829913136e-09 - train=0.831432 - test=0.8271006162910611\n",
      "d_int=11, d_sq=3, lambda_=3.381100782480682e-09 - train=0.83142 - test=0.827017904469606\n",
      "d_int=11, d_sq=3, lambda_=3.4124487078528964e-09 - train=0.831412 - test=0.827065419771293\n",
      "d_int=11, d_sq=3, lambda_=3.4440872759738234e-09 - train=0.831412 - test=0.8270108651656525\n",
      "d_int=11, d_sq=3, lambda_=3.476019181541974e-09 - train=0.831416 - test=0.8270988564650728\n",
      "d_int=11, d_sq=3, lambda_=3.508247144239796e-09 - train=0.83142 - test=0.827009105339664\n",
      "d_int=11, d_sq=3, lambda_=3.5407739089652697e-09 - train=0.83142 - test=0.8270548608153626\n",
      "d_int=11, d_sq=3, lambda_=3.573602246065781e-09 - train=0.831436 - test=0.8271446119407713\n",
      "d_int=11, d_sq=3, lambda_=3.6067349515740316e-09 - train=0.831436 - test=0.8271410922887945\n",
      "d_int=11, d_sq=3, lambda_=3.6401748474461375e-09 - train=0.831436 - test=0.8270794983792003\n",
      "d_int=11, d_sq=3, lambda_=3.6739247818020666e-09 - train=0.831428 - test=0.8270689394232699\n",
      "d_int=11, d_sq=3, lambda_=3.7079876291681724e-09 - train=0.831432 - test=0.8271551708967018\n",
      "d_int=11, d_sq=3, lambda_=3.742366290721977e-09 - train=0.831412 - test=0.8270759787272235\n",
      "d_int=11, d_sq=3, lambda_=3.777063694539372e-09 - train=0.831412 - test=0.8270970966390844\n",
      "d_int=11, d_sq=3, lambda_=3.812082795843888e-09 - train=0.831404 - test=0.8271498914187365\n",
      "d_int=11, d_sq=3, lambda_=3.847426577258498e-09 - train=0.831408 - test=0.8271428521147829\n",
      "d_int=11, d_sq=3, lambda_=3.883098049059609e-09 - train=0.831392 - test=0.8271991665464119\n",
      "d_int=11, d_sq=3, lambda_=3.919100249433402e-09 - train=0.831388 - test=0.8271657298526321\n",
      "d_int=11, d_sq=3, lambda_=3.95543624473471e-09 - train=0.83138 - test=0.8271604503746669\n",
      "d_int=11, d_sq=3, lambda_=3.992109129748054e-09 - train=0.831372 - test=0.8271833281125163\n",
      "d_int=11, d_sq=3, lambda_=4.029122027951344e-09 - train=0.831372 - test=0.8271111752469916\n",
      "d_int=11, d_sq=3, lambda_=4.066478091781862e-09 - train=0.83138 - test=0.8271622102006554\n",
      "d_int=11, d_sq=3, lambda_=4.10418050290471e-09 - train=0.83138 - test=0.8271886075904814\n",
      "d_int=11, d_sq=3, lambda_=4.1422324724838895e-09 - train=0.83138 - test=0.8271569307226901\n",
      "d_int=11, d_sq=3, lambda_=4.1806372414557625e-09 - train=0.83138 - test=0.827104135943038\n",
      "d_int=11, d_sq=3, lambda_=4.219398080805024e-09 - train=0.831368 - test=0.8271586905486785\n",
      "d_int=11, d_sq=3, lambda_=4.258518291843406e-09 - train=0.83136 - test=0.8272238041102495\n",
      "d_int=11, d_sq=3, lambda_=4.298001206490799e-09 - train=0.83138 - test=0.8272238041102495\n",
      "d_int=11, d_sq=3, lambda_=4.337850187558984e-09 - train=0.83138 - test=0.8272537211520525\n",
      "d_int=11, d_sq=3, lambda_=4.378068629038175e-09 - train=0.831376 - test=0.8272167648062959\n",
      "d_int=11, d_sq=3, lambda_=4.4186599563859416e-09 - train=0.83138 - test=0.8271410922887945\n",
      "d_int=11, d_sq=3, lambda_=4.459627626819087e-09 - train=0.83138 - test=0.8271358128108293\n",
      "d_int=11, d_sq=3, lambda_=4.500975129608051e-09 - train=0.831372 - test=0.8271322931588525\n",
      "d_int=11, d_sq=3, lambda_=4.542705986374043e-09 - train=0.831376 - test=0.8271428521147829\n",
      "d_int=11, d_sq=3, lambda_=4.584823751389105e-09 - train=0.831392 - test=0.827225563936238\n",
      "d_int=11, d_sq=3, lambda_=4.627332011878693e-09 - train=0.831384 - test=0.8271569307226901\n",
      "d_int=11, d_sq=3, lambda_=4.67023438832733e-09 - train=0.831384 - test=0.8272167648062959\n",
      "d_int=11, d_sq=3, lambda_=4.713534534786917e-09 - train=0.831388 - test=0.8271428521147829\n",
      "d_int=11, d_sq=3, lambda_=4.757236139187889e-09 - train=0.831392 - test=0.8272977168017627\n",
      "d_int=11, d_sq=3, lambda_=4.801342923653445e-09 - train=0.831404 - test=0.8272097255023423\n",
      "d_int=11, d_sq=3, lambda_=4.8458586448165004e-09 - train=0.831376 - test=0.8272466818480989\n",
      "d_int=11, d_sq=3, lambda_=4.89078709413959e-09 - train=0.8314 - test=0.8271481315927481\n",
      "d_int=11, d_sq=3, lambda_=4.936132098237907e-09 - train=0.831388 - test=0.8272677997599598\n",
      "d_int=11, d_sq=3, lambda_=4.981897519205165e-09 - train=0.831372 - test=0.8272396425441453\n",
      "d_int=11, d_sq=3, lambda_=5.0280872549424746e-09 - train=0.831384 - test=0.8272132451543192\n",
      "d_int=11, d_sq=3, lambda_=5.074705239490476e-09 - train=0.831396 - test=0.8272396425441453\n",
      "d_int=11, d_sq=3, lambda_=5.1217554433642414e-09 - train=0.83138 - test=0.8272572408040293\n",
      "d_int=11, d_sq=3, lambda_=5.169241873891593e-09 - train=0.831376 - test=0.827290677497809\n",
      "d_int=11, d_sq=3, lambda_=5.217168575554354e-09 - train=0.831372 - test=0.827243162196122\n",
      "d_int=11, d_sq=3, lambda_=5.2655396303327555e-09 - train=0.831376 - test=0.827281878367867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_int=11, d_sq=3, lambda_=5.31435915805323e-09 - train=0.831388 - test=0.8273100355836814\n",
      "d_int=11, d_sq=3, lambda_=5.3636313167392454e-09 - train=0.831384 - test=0.8271762888085626\n",
      "d_int=11, d_sq=3, lambda_=5.413360302965376e-09 - train=0.831372 - test=0.827225563936238\n",
      "d_int=11, d_sq=3, lambda_=5.463550352214887e-09 - train=0.831376 - test=0.8272677997599598\n",
      "d_int=11, d_sq=3, lambda_=5.514205739240302e-09 - train=0.831348 - test=0.8272765988899018\n",
      "d_int=11, d_sq=3, lambda_=5.56533077842764e-09 - train=0.831348 - test=0.827290677497809\n",
      "d_int=11, d_sq=3, lambda_=5.616929824163815e-09 - train=0.83136 - test=0.8273029962797278\n",
      "d_int=11, d_sq=3, lambda_=5.6690072712074336e-09 - train=0.831356 - test=0.827299476627751\n",
      "d_int=11, d_sq=3, lambda_=5.72156755506324e-09 - train=0.831336 - test=0.827290677497809\n",
      "d_int=11, d_sq=3, lambda_=5.774615152359826e-09 - train=0.831352 - test=0.8273346731475192\n",
      "d_int=11, d_sq=3, lambda_=5.82815458123084e-09 - train=0.831348 - test=0.8273117954096699\n",
      "d_int=11, d_sq=3, lambda_=5.8821904016999675e-09 - train=0.831344 - test=0.827299476627751\n",
      "d_int=11, d_sq=3, lambda_=5.936727216069126e-09 - train=0.831348 - test=0.8273188347136236\n",
      "d_int=11, d_sq=3, lambda_=5.991769669310613e-09 - train=0.831348 - test=0.8273205945396119\n",
      "d_int=11, d_sq=3, lambda_=6.047322449462655e-09 - train=0.83134 - test=0.8272871578458322\n",
      "d_int=11, d_sq=3, lambda_=6.103390288028621e-09 - train=0.831332 - test=0.8273241141915887\n",
      "d_int=11, d_sq=3, lambda_=6.159977960380156e-09 - train=0.831328 - test=0.8272836381938554\n",
      "d_int=11, d_sq=3, lambda_=6.217090286163834e-09 - train=0.83134 - test=0.8273065159317047\n",
      "d_int=11, d_sq=3, lambda_=6.274732129711575e-09 - train=0.831324 - test=0.8273153150616467\n",
      "d_int=11, d_sq=3, lambda_=6.332908400455102e-09 - train=0.831316 - test=0.8273205945396119\n",
      "d_int=11, d_sq=3, lambda_=6.391624053344005e-09 - train=0.831328 - test=0.8273205945396119\n",
      "d_int=11, d_sq=3, lambda_=6.450884089267686e-09 - train=0.83132 - test=0.8273311534955423\n",
      "d_int=11, d_sq=3, lambda_=6.510693555481468e-09 - train=0.831316 - test=0.8273663500153104\n",
      "d_int=11, d_sq=3, lambda_=6.57105754603627e-09 - train=0.831324 - test=0.8273399526254844\n",
      "d_int=11, d_sq=3, lambda_=6.631981202212662e-09 - train=0.831316 - test=0.8273663500153104\n",
      "d_int=11, d_sq=3, lambda_=6.693469712958668e-09 - train=0.831316 - test=0.8273469919294381\n",
      "d_int=11, d_sq=3, lambda_=6.755528315331639e-09 - train=0.831324 - test=0.8273593107113568\n",
      "d_int=11, d_sq=3, lambda_=6.818162294944493e-09 - train=0.831312 - test=0.8273997867090902\n",
      "d_int=11, d_sq=3, lambda_=6.88137698641567e-09 - train=0.831312 - test=0.8274613806186845\n",
      "d_int=11, d_sq=3, lambda_=6.945177773823689e-09 - train=0.831332 - test=0.8273733893192641\n",
      "d_int=11, d_sq=3, lambda_=7.009570091165632e-09 - train=0.831328 - test=0.8273575508853684\n",
      "d_int=11, d_sq=3, lambda_=7.074559422819879e-09 - train=0.831324 - test=0.8273716294932757\n",
      "d_int=11, d_sq=3, lambda_=7.1401513040133885e-09 - train=0.831328 - test=0.8274349832288583\n",
      "d_int=11, d_sq=3, lambda_=7.20635132129306e-09 - train=0.831316 - test=0.8273751491452526\n",
      "d_int=11, d_sq=3, lambda_=7.273165113001454e-09 - train=0.831324 - test=0.8273751491452526\n",
      "d_int=11, d_sq=3, lambda_=7.340598369757197e-09 - train=0.831308 - test=0.8273821884492062\n",
      "d_int=11, d_sq=3, lambda_=7.4086568349395674e-09 - train=0.831316 - test=0.8274789788785685\n",
      "d_int=11, d_sq=3, lambda_=7.47734630517758e-09 - train=0.831308 - test=0.8273698696672873\n",
      "d_int=11, d_sq=3, lambda_=7.546672630843902e-09 - train=0.831308 - test=0.8273769089712409\n",
      "d_int=11, d_sq=3, lambda_=7.616641716552891e-09 - train=0.831308 - test=0.8273927474051366\n",
      "d_int=11, d_sq=3, lambda_=7.687259521663721e-09 - train=0.831304 - test=0.8274613806186845\n",
      "d_int=11, d_sq=3, lambda_=7.758532060787844e-09 - train=0.831304 - test=0.827459620792696\n",
      "d_int=11, d_sq=3, lambda_=7.830465404301183e-09 - train=0.831304 - test=0.8274314635768815\n",
      "d_int=11, d_sq=3, lambda_=7.903065678861332e-09 - train=0.83132 - test=0.8274578609667076\n",
      "d_int=11, d_sq=3, lambda_=7.976339067929282e-09 - train=0.831316 - test=0.8273909875791482\n",
      "d_int=11, d_sq=3, lambda_=8.050291812295973e-09 - train=0.831312 - test=0.82743322340287\n",
      "d_int=11, d_sq=3, lambda_=8.124930210614065e-09 - train=0.8313 - test=0.827459620792696\n",
      "d_int=11, d_sq=3, lambda_=8.20026061993413e-09 - train=0.831304 - test=0.8274649002706612\n",
      "d_int=11, d_sq=3, lambda_=8.276289456246335e-09 - train=0.831308 - test=0.8274543413147308\n",
      "d_int=11, d_sq=3, lambda_=8.353023195026788e-09 - train=0.8313 - test=0.8274578609667076\n",
      "d_int=11, d_sq=3, lambda_=8.430468371788968e-09 - train=0.831296 - test=0.8274789788785685\n",
      "d_int=11, d_sq=3, lambda_=8.508631582640565e-09 - train=0.831296 - test=0.8274807387045569\n",
      "d_int=11, d_sq=3, lambda_=8.587519484845179e-09 - train=0.83128 - test=0.8274754592265917\n",
      "d_int=11, d_sq=3, lambda_=8.667138797389225e-09 - train=0.831268 - test=0.8274684199226381\n",
      "d_int=11, d_sq=3, lambda_=8.747496301554433e-09 - train=0.831256 - test=0.8274754592265917\n",
      "d_int=11, d_sq=3, lambda_=8.82859884149515e-09 - train=0.83124 - test=0.8274525814887423\n",
      "d_int=11, d_sq=3, lambda_=8.910453324821504e-09 - train=0.831224 - test=0.8274385028808351\n",
      "d_int=11, d_sq=3, lambda_=8.993066723187632e-09 - train=0.831212 - test=0.8274613806186845\n",
      "d_int=11, d_sq=3, lambda_=9.076446072885357e-09 - train=0.831216 - test=0.8274701797486265\n",
      "d_int=11, d_sq=3, lambda_=9.16059847544369e-09 - train=0.83122 - test=0.8274877780085105\n",
      "d_int=11, d_sq=3, lambda_=9.24553109823358e-09 - train=0.831208 - test=0.8274525814887423\n",
      "d_int=11, d_sq=3, lambda_=9.331251175078241e-09 - train=0.831204 - test=0.8274912976604873\n",
      "d_int=11, d_sq=3, lambda_=9.417766006869538e-09 - train=0.831208 - test=0.8274684199226381\n",
      "d_int=11, d_sq=3, lambda_=9.505082962189511e-09 - train=0.8312 - test=0.8274402627068236\n",
      "d_int=11, d_sq=3, lambda_=9.593209477938232e-09 - train=0.831192 - test=0.827459620792696\n",
      "d_int=11, d_sq=3, lambda_=9.682153059967097e-09 - train=0.831188 - test=0.827489537834499\n",
      "d_int=11, d_sq=3, lambda_=9.771921283717996e-09 - train=0.831188 - test=0.8274649002706612\n",
      "d_int=11, d_sq=3, lambda_=9.862521794868764e-09 - train=0.831176 - test=0.8274789788785685\n",
      "d_int=11, d_sq=3, lambda_=9.953962309984244e-09 - train=0.831184 - test=0.8275176950503135\n",
      "d_int=11, d_sq=3, lambda_=1.0046250617173403e-08 - train=0.83118 - test=0.8274631404446728\n",
      "d_int=11, d_sq=3, lambda_=1.0139394576752917e-08 - train=0.831172 - test=0.8274736994006032\n",
      "d_int=11, d_sq=3, lambda_=1.0233402121916422e-08 - train=0.831164 - test=0.8274684199226381\n",
      "d_int=11, d_sq=3, lambda_=1.0328281259410274e-08 - train=0.831164 - test=0.8274736994006032\n",
      "d_int=11, d_sq=3, lambda_=1.0424040070215558e-08 - train=0.831164 - test=0.8275088959203714\n",
      "d_int=11, d_sq=3, lambda_=1.0520686710236229e-08 - train=0.831128 - test=0.8274807387045569\n",
      "d_int=11, d_sq=3, lambda_=1.0618229410993846e-08 - train=0.83114 - test=0.8274684199226381\n",
      "d_int=11, d_sq=3, lambda_=1.0716676480328645e-08 - train=0.831144 - test=0.8274719395746148\n",
      "d_int=11, d_sq=3, lambda_=1.0816036303107095e-08 - train=0.831124 - test=0.8274842583565337\n",
      "d_int=11, d_sq=3, lambda_=1.0916317341936148e-08 - train=0.831124 - test=0.8274789788785685\n",
      "d_int=11, d_sq=3, lambda_=1.1017528137883874e-08 - train=0.83112 - test=0.8274948173124641\n",
      "d_int=11, d_sq=3, lambda_=1.1119677311206978e-08 - train=0.83112 - test=0.8275000967904294\n",
      "d_int=11, d_sq=3, lambda_=1.1222773562085071e-08 - train=0.831096 - test=0.8274772190525801\n",
      "d_int=11, d_sq=3, lambda_=1.1326825671361538e-08 - train=0.831108 - test=0.8274807387045569\n",
      "d_int=11, d_sq=3, lambda_=1.1431842501291502e-08 - train=0.831104 - test=0.8274561011407192\n",
      "d_int=11, d_sq=3, lambda_=1.153783299629662e-08 - train=0.8311 - test=0.8274525814887423\n",
      "d_int=11, d_sq=3, lambda_=1.1644806183726855e-08 - train=0.831112 - test=0.8274719395746148\n",
      "d_int=11, d_sq=3, lambda_=1.1752771174629452e-08 - train=0.831104 - test=0.8274719395746148\n",
      "d_int=11, d_sq=3, lambda_=1.1861737164524792e-08 - train=0.831096 - test=0.8274772190525801\n",
      "d_int=11, d_sq=3, lambda_=1.1971713434189657e-08 - train=0.831088 - test=0.827489537834499\n",
      "d_int=11, d_sq=3, lambda_=1.208270935044776e-08 - train=0.831104 - test=0.8274719395746148\n",
      "d_int=11, d_sq=3, lambda_=1.2194734366967383e-08 - train=0.831096 - test=0.8274719395746148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_int=11, d_sq=3, lambda_=1.2307798025066658e-08 - train=0.8311 - test=0.8274860181825221\n",
      "d_int=11, d_sq=3, lambda_=1.2421909954526187e-08 - train=0.831096 - test=0.8274684199226381\n",
      "d_int=11, d_sq=3, lambda_=1.2537079874409196e-08 - train=0.831088 - test=0.8274877780085105\n",
      "d_int=11, d_sq=3, lambda_=1.265331759388943e-08 - train=0.831084 - test=0.8274701797486265\n",
      "d_int=11, d_sq=3, lambda_=1.2770633013086447e-08 - train=0.83108 - test=0.8274754592265917\n",
      "d_int=11, d_sq=3, lambda_=1.2889036123908915e-08 - train=0.83106 - test=0.8274824985305453\n",
      "d_int=11, d_sq=3, lambda_=1.3008537010905705e-08 - train=0.831072 - test=0.8275018566164177\n",
      "d_int=11, d_sq=3, lambda_=1.3129145852124654e-08 - train=0.83106 - test=0.8275018566164177\n",
      "d_int=11, d_sq=3, lambda_=1.3250872919979538e-08 - train=0.831068 - test=0.8274912976604873\n",
      "d_int=11, d_sq=3, lambda_=1.3373728582124977e-08 - train=0.831076 - test=0.8275212147022902\n",
      "d_int=11, d_sq=3, lambda_=1.3497723302339418e-08 - train=0.831076 - test=0.8275212147022902\n",
      "d_int=11, d_sq=3, lambda_=1.3622867641416468e-08 - train=0.83106 - test=0.8275176950503135\n",
      "d_int=11, d_sq=3, lambda_=1.3749172258064204e-08 - train=0.831048 - test=0.8275124155723482\n",
      "d_int=11, d_sq=3, lambda_=1.3876647909813071e-08 - train=0.831052 - test=0.8275352933101975\n",
      "d_int=11, d_sq=3, lambda_=1.4005305453932198e-08 - train=0.831036 - test=0.8275564112220584\n",
      "d_int=11, d_sq=3, lambda_=1.4135155848353957e-08 - train=0.831032 - test=0.8275317736582207\n",
      "d_int=11, d_sq=3, lambda_=1.4266210152607355e-08 - train=0.831036 - test=0.8275247343542671\n",
      "d_int=11, d_sq=3, lambda_=1.4398479528760066e-08 - train=0.831036 - test=0.82755465139607\n",
      "d_int=11, d_sq=3, lambda_=1.4531975242368955e-08 - train=0.83104 - test=0.82755465139607\n",
      "d_int=11, d_sq=3, lambda_=1.4666708663439685e-08 - train=0.83104 - test=0.8275528915700816\n",
      "d_int=11, d_sq=3, lambda_=1.48026912673951e-08 - train=0.831044 - test=0.8275440924401395\n",
      "d_int=11, d_sq=3, lambda_=1.4939934636052564e-08 - train=0.83104 - test=0.8275212147022902\n",
      "d_int=11, d_sq=3, lambda_=1.507845045861051e-08 - train=0.831016 - test=0.8275176950503135\n",
      "d_int=11, d_sq=3, lambda_=1.5218250532643872e-08 - train=0.831028 - test=0.8275053762683946\n",
      "d_int=11, d_sq=3, lambda_=1.5359346765108953e-08 - train=0.831032 - test=0.8275564112220584\n",
      "d_int=11, d_sq=3, lambda_=1.550175117335769e-08 - train=0.83104 - test=0.8275088959203714\n",
      "d_int=11, d_sq=3, lambda_=1.5645475886160957e-08 - train=0.831048 - test=0.8275036164424062\n",
      "d_int=11, d_sq=3, lambda_=1.579053314474176e-08 - train=0.83104 - test=0.8275141753983366\n",
      "d_int=11, d_sq=3, lambda_=1.5936935303817763e-08 - train=0.831032 - test=0.8275106557463598\n",
      "d_int=11, d_sq=3, lambda_=1.608469483265355e-08 - train=0.831044 - test=0.8275282540062439\n",
      "d_int=11, d_sq=3, lambda_=1.6233824316122783e-08 - train=0.831052 - test=0.8275423326141511\n",
      "d_int=11, d_sq=3, lambda_=1.6384336455779847e-08 - train=0.831052 - test=0.8275176950503135\n",
      "d_int=11, d_sq=3, lambda_=1.6536244070941787e-08 - train=0.83104 - test=0.8275018566164177\n",
      "d_int=11, d_sq=3, lambda_=1.668956009978023e-08 - train=0.831028 - test=0.8275036164424062\n",
      "d_int=11, d_sq=3, lambda_=1.6844297600423153e-08 - train=0.831028 - test=0.8275247343542671\n",
      "d_int=11, d_sq=3, lambda_=1.700046975206718e-08 - train=0.831024 - test=0.8275176950503135\n",
      "d_int=11, d_sq=3, lambda_=1.7158089856100067e-08 - train=0.831036 - test=0.8275388129621744\n",
      "d_int=11, d_sq=3, lambda_=1.731717133723353e-08 - train=0.831024 - test=0.827515935224325\n",
      "d_int=11, d_sq=3, lambda_=1.747772774464681e-08 - train=0.831004 - test=0.8275423326141511\n",
      "d_int=11, d_sq=3, lambda_=1.763977275314045e-08 - train=0.830992 - test=0.8275388129621744\n",
      "d_int=11, d_sq=3, lambda_=1.780332016430112e-08 - train=0.830992 - test=0.8275282540062439\n",
      "d_int=11, d_sq=3, lambda_=1.7968383907677193e-08 - train=0.831008 - test=0.8275370531361859\n",
      "d_int=11, d_sq=3, lambda_=1.8134978041964953e-08 - train=0.831012 - test=0.8275528915700816\n",
      "d_int=11, d_sq=3, lambda_=1.830311675620613e-08 - train=0.831012 - test=0.8275581710480467\n",
      "d_int=11, d_sq=3, lambda_=1.847281437099636e-08 - train=0.831024 - test=0.8275352933101975\n",
      "d_int=11, d_sq=3, lambda_=1.8644085339704852e-08 - train=0.831028 - test=0.8275493719181047\n",
      "d_int=11, d_sq=3, lambda_=1.8816944249705555e-08 - train=0.831016 - test=0.827545852266128\n",
      "d_int=11, d_sq=3, lambda_=1.8991405823619356e-08 - train=0.830976 - test=0.827545852266128\n",
      "d_int=11, d_sq=3, lambda_=1.916748492056813e-08 - train=0.830988 - test=0.827545852266128\n",
      "d_int=11, d_sq=3, lambda_=1.934519653744044e-08 - train=0.830996 - test=0.8275282540062439\n",
      "d_int=11, d_sq=3, lambda_=1.952455581016861e-08 - train=0.83098 - test=0.8275194548763019\n",
      "d_int=11, d_sq=3, lambda_=1.9705578015018002e-08 - train=0.830968 - test=0.8275440924401395\n",
      "d_int=11, d_sq=3, lambda_=1.988827856988812e-08 - train=0.830948 - test=0.8275370531361859\n",
      "d_int=11, d_sq=3, lambda_=2.0072673035625682e-08 - train=0.830948 - test=0.8275370531361859\n",
      "d_int=11, d_sq=3, lambda_=2.0258777117350173e-08 - train=0.830952 - test=0.8275388129621744\n",
      "d_int=11, d_sq=3, lambda_=2.0446606665791178e-08 - train=0.830936 - test=0.8275176950503135\n",
      "d_int=11, d_sq=3, lambda_=2.0636177678638566e-08 - train=0.830968 - test=0.8275476120921164\n",
      "d_int=11, d_sq=3, lambda_=2.0827506301905142e-08 - train=0.830964 - test=0.8275194548763019\n",
      "d_int=11, d_sq=3, lambda_=2.1020608831301584e-08 - train=0.830976 - test=0.8275124155723482\n",
      "d_int=11, d_sq=3, lambda_=2.1215501713624523e-08 - train=0.830992 - test=0.8275088959203714\n",
      "d_int=11, d_sq=3, lambda_=2.1412201548157317e-08 - train=0.830976 - test=0.8275176950503135\n",
      "d_int=11, d_sq=3, lambda_=2.1610725088083766e-08 - train=0.831012 - test=0.8275229745282786\n",
      "d_int=11, d_sq=3, lambda_=2.1811089241915198e-08 - train=0.831016 - test=0.827507136094383\n",
      "d_int=11, d_sq=3, lambda_=2.2013311074930324e-08 - train=0.831012 - test=0.8275300138322322\n",
      "d_int=11, d_sq=3, lambda_=2.2217407810628825e-08 - train=0.831 - test=0.8275053762683946\n",
      "d_int=11, d_sq=3, lambda_=2.242339683219845e-08 - train=0.830992 - test=0.8275176950503135\n",
      "d_int=11, d_sq=3, lambda_=2.2631295683995295e-08 - train=0.830996 - test=0.827507136094383\n",
      "d_int=11, d_sq=3, lambda_=2.284112207303821e-08 - train=0.830992 - test=0.8275124155723482\n",
      "d_int=11, d_sq=3, lambda_=2.3052893870517062e-08 - train=0.830968 - test=0.8275124155723482\n",
      "d_int=11, d_sq=3, lambda_=2.326662911331458e-08 - train=0.830964 - test=0.8275124155723482\n",
      "d_int=11, d_sq=3, lambda_=2.3482346005542764e-08 - train=0.830952 - test=0.8275088959203714\n",
      "d_int=11, d_sq=3, lambda_=2.3700062920093305e-08 - train=0.830948 - test=0.8275036164424062\n",
      "d_int=11, d_sq=3, lambda_=2.3919798400202372e-08 - train=0.830944 - test=0.827515935224325\n",
      "d_int=11, d_sq=3, lambda_=2.414157116103018e-08 - train=0.83094 - test=0.8275264941802555\n",
      "d_int=11, d_sq=3, lambda_=2.436540009125466e-08 - train=0.830924 - test=0.8275053762683946\n",
      "d_int=11, d_sq=3, lambda_=2.4591304254680413e-08 - train=0.830944 - test=0.827507136094383\n",
      "d_int=11, d_sq=3, lambda_=2.4819302891862526e-08 - train=0.83094 - test=0.8275212147022902\n",
      "d_int=11, d_sq=3, lambda_=2.5049415421745025e-08 - train=0.830936 - test=0.8275053762683946\n",
      "d_int=11, d_sq=3, lambda_=2.5281661443315004e-08 - train=0.830928 - test=0.8225849028048107\n",
      "d_int=11, d_sq=3, lambda_=2.5516060737271865e-08 - train=0.830936 - test=0.8225813831528338\n",
      "d_int=11, d_sq=3, lambda_=2.5752633267711987e-08 - train=0.830928 - test=0.8226200993245788\n",
      "d_int=11, d_sq=3, lambda_=2.599139918382933e-08 - train=0.830916 - test=0.8226077805426599\n",
      "d_int=11, d_sq=3, lambda_=2.6232378821631237e-08 - train=0.830904 - test=0.822625378802544\n",
      "d_int=11, d_sq=3, lambda_=2.647559270567064e-08 - train=0.830908 - test=0.8226394574104513\n",
      "d_int=11, d_sq=3, lambda_=2.6721061550794325e-08 - train=0.8309 - test=0.8226130600206252\n",
      "d_int=11, d_sq=3, lambda_=2.6968806263906925e-08 - train=0.830912 - test=0.8226007412387063\n",
      "d_int=11, d_sq=3, lambda_=2.7218847945751796e-08 - train=0.8309 - test=0.8226113001946368\n",
      "d_int=11, d_sq=3, lambda_=2.7471207892708145e-08 - train=0.830912 - test=0.8226324181064977\n",
      "d_int=11, d_sq=3, lambda_=2.772590759860481e-08 - train=0.830892 - test=0.8226288984545208\n",
      "d_int=11, d_sq=3, lambda_=2.798296875655116e-08 - train=0.830868 - test=0.822625378802544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_int=11, d_sq=3, lambda_=2.8242413260784357e-08 - train=0.83086 - test=0.8225989814127179\n",
      "d_int=11, d_sq=3, lambda_=2.8504263208534316e-08 - train=0.83086 - test=0.822595461760741\n",
      "d_int=11, d_sq=3, lambda_=2.8768540901905912e-08 - train=0.830852 - test=0.8226077805426599\n",
      "d_int=11, d_sq=3, lambda_=2.903526884977814e-08 - train=0.830844 - test=0.8226077805426599\n",
      "d_int=11, d_sq=3, lambda_=2.9304469769721444e-08 - train=0.83084 - test=0.8226165796726019\n",
      "d_int=11, d_sq=3, lambda_=2.9576166589932567e-08 - train=0.830824 - test=0.8226306582805092\n",
      "d_int=11, d_sq=3, lambda_=2.9850382451187295e-08 - train=0.830816 - test=0.8226200993245788\n",
      "d_int=11, d_sq=3, lambda_=3.012714070881164e-08 - train=0.83084 - test=0.8226218591505672\n",
      "d_int=11, d_sq=3, lambda_=3.0406464934670685e-08 - train=0.830848 - test=0.8226095403686483\n",
      "d_int=11, d_sq=3, lambda_=3.068837891917639e-08 - train=0.830828 - test=0.8226095403686483\n",
      "d_int=11, d_sq=3, lambda_=3.0972906673314065e-08 - train=0.830808 - test=0.8226113001946368\n",
      "d_int=11, d_sq=3, lambda_=3.126007243068704e-08 - train=0.830816 - test=0.8226042608906832\n",
      "d_int=11, d_sq=3, lambda_=3.154990064958089e-08 - train=0.8308 - test=0.8225743438488802\n",
      "d_int=11, d_sq=3, lambda_=3.184241601504656e-08 - train=0.830796 - test=0.8225849028048107\n",
      "d_int=11, d_sq=3, lambda_=3.2137643441002756e-08 - train=0.830788 - test=0.8225673045449265\n",
      "d_int=11, d_sq=3, lambda_=3.2435608072358115e-08 - train=0.830788 - test=0.8225532259370193\n",
      "d_int=11, d_sq=3, lambda_=3.2736335287152474e-08 - train=0.830788 - test=0.8225637848929498\n",
      "d_int=11, d_sq=3, lambda_=3.3039850698718545e-08 - train=0.830796 - test=0.8225743438488802\n",
      "d_int=11, d_sq=3, lambda_=3.3346180157863626e-08 - train=0.830788 - test=0.8225796233268454\n",
      "d_int=11, d_sq=3, lambda_=3.3655349755070945e-08 - train=0.830792 - test=0.8225919421087643\n",
      "d_int=11, d_sq=3, lambda_=3.39673858227221e-08 - train=0.830796 - test=0.8225849028048107\n",
      "d_int=11, d_sq=3, lambda_=3.428231493733974e-08 - train=0.830796 - test=0.822595461760741\n",
      "d_int=11, d_sq=3, lambda_=3.460016392185109e-08 - train=0.830784 - test=0.8225989814127179\n",
      "d_int=11, d_sq=3, lambda_=3.4920959847872717e-08 - train=0.830772 - test=0.8225919421087643\n",
      "d_int=11, d_sq=3, lambda_=3.524473003801588e-08 - train=0.83078 - test=0.8226095403686483\n",
      "d_int=11, d_sq=3, lambda_=3.557150206821382e-08 - train=0.830776 - test=0.8225989814127179\n",
      "d_int=11, d_sq=3, lambda_=3.5901303770070686e-08 - train=0.830792 - test=0.8226025010646947\n",
      "d_int=11, d_sq=3, lambda_=3.6234163233231496e-08 - train=0.830784 - test=0.8226077805426599\n",
      "d_int=11, d_sq=3, lambda_=3.657010880777486e-08 - train=0.830784 - test=0.8226060207166715\n",
      "d_int=11, d_sq=3, lambda_=3.6909169106627744e-08 - train=0.830784 - test=0.8226095403686483\n",
      "d_int=11, d_sq=3, lambda_=3.7251373008002096e-08 - train=0.830764 - test=0.8226288984545208\n",
      "d_int=11, d_sq=3, lambda_=3.7596749657854675e-08 - train=0.830764 - test=0.8226218591505672\n",
      "d_int=11, d_sq=3, lambda_=3.794532847236942e-08 - train=0.83076 - test=0.8226183394985904\n",
      "d_int=11, d_sq=3, lambda_=3.829713914046277e-08 - train=0.83076 - test=0.8226113001946368\n",
      "d_int=11, d_sq=3, lambda_=3.865221162631262e-08 - train=0.830744 - test=0.8226271386285324\n",
      "d_int=11, d_sq=3, lambda_=3.901057617190993e-08 - train=0.830736 - test=0.8226412172364397\n",
      "d_int=11, d_sq=3, lambda_=3.937226329963475e-08 - train=0.830736 - test=0.8226148198466136\n",
      "d_int=11, d_sq=3, lambda_=3.9737303814856046e-08 - train=0.830724 - test=0.8226464967144049\n",
      "d_int=11, d_sq=3, lambda_=4.010572880855496e-08 - train=0.830696 - test=0.822625378802544\n",
      "d_int=11, d_sq=3, lambda_=4.0477569659973224e-08 - train=0.8307 - test=0.8226640949742889\n",
      "d_int=11, d_sq=3, lambda_=4.0852858039285656e-08 - train=0.830696 - test=0.8226535360183586\n",
      "d_int=11, d_sq=3, lambda_=4.123162591029748e-08 - train=0.830688 - test=0.8226464967144049\n",
      "d_int=11, d_sq=3, lambda_=4.161390553316707e-08 - train=0.83066 - test=0.8226570556703353\n",
      "d_int=11, d_sq=3, lambda_=4.1999729467153065e-08 - train=0.830664 - test=0.8226464967144049\n",
      "d_int=11, d_sq=3, lambda_=4.238913057338774e-08 - train=0.830648 - test=0.8226271386285324\n",
      "d_int=11, d_sq=3, lambda_=4.2782142017676126e-08 - train=0.83068 - test=0.8226658548002773\n",
      "d_int=11, d_sq=3, lambda_=4.317879727332022e-08 - train=0.830664 - test=0.8226676146262658\n",
      "d_int=11, d_sq=3, lambda_=4.35791301239703e-08 - train=0.830656 - test=0.8226869727121382\n",
      "d_int=11, d_sq=3, lambda_=4.398317466650228e-08 - train=0.83066 - test=0.8226852128861498\n",
      "d_int=11, d_sq=3, lambda_=4.439096531392166e-08 - train=0.830656 - test=0.8226781735821962\n",
      "d_int=11, d_sq=3, lambda_=4.4802536798294895e-08 - train=0.830656 - test=0.8226746539302194\n",
      "d_int=11, d_sq=3, lambda_=4.521792417370702e-08 - train=0.830664 - test=0.8226605753223122\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-315-4902b3688c39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_split_poly\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mlse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLeastSquaresL2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[0mlse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_split_poly\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                 \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_split_poly\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\EPFL\\MA1\\CS-433 Machine Learning\\ML_course\\projects\\project1\\scripts\\classifiers.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, y, X)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;31m# find weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d_ints = range(11, 12)\n",
    "d_sqs = range(3, 4)\n",
    "lambdas = [ math.pow(10,c) for c in np.linspace(-9,-7,500) ]\n",
    "\n",
    "max_train = 0\n",
    "max_train_d_int = 0\n",
    "max_train_d_sq = 0\n",
    "max_train_d_lambda = 0\n",
    "\n",
    "max_test = 0\n",
    "max_test_d_int = 0\n",
    "max_test_d_sq = 0\n",
    "max_test_d_lambda = 0\n",
    "\n",
    "\n",
    "for d_int in d_ints:\n",
    "    for d_sq in d_sqs:\n",
    "        X_split_poly = [ build_X(X, d_int, d_sq) for X in X_split ]\n",
    "        X_test_split_poly = [ build_X(X, d_int, d_sq) for X in X_test_split ]\n",
    "        \n",
    "        for lambda_ in lambdas:\n",
    "            \n",
    "            models = []\n",
    "            y_pred = np.ones(tX.shape[0])\n",
    "\n",
    "            for i in range(len(X_split_poly)):\n",
    "                lse = LeastSquaresL2(lambda_)\n",
    "                lse.fit(y_split[i], X_split_poly[i])\n",
    "                models.append(lse)\n",
    "                y_pred[indices_split[i]] = lse.predict(X_split_poly[i])\n",
    "\n",
    "            acc_train = np.mean(y == y_pred)\n",
    "            if acc_train > max_train:\n",
    "                max_train = acc_train\n",
    "                max_train_d_int = d_int\n",
    "                max_train_d_sq = d_sq\n",
    "                max_train_d_lambda = lambda_\n",
    "            \n",
    "            y_pred = np.ones(tX_test.shape[0])\n",
    "\n",
    "            for model, X, indices in zip(models, X_test_split_poly, test_split_indices):\n",
    "                y_pred[indices] = model.predict(X)\n",
    "\n",
    "            acc_test = np.mean(y_test == y_pred)\n",
    "            if acc_test > max_test:\n",
    "                max_test = acc_test\n",
    "                max_test_d_int = d_int\n",
    "                max_test_d_sq = d_sq\n",
    "                max_test_d_lambda = lambda_\n",
    "            \n",
    "            print(f\"d_int={d_int}, d_sq={d_sq}, lambda_={lambda_} - train={acc_train} - test={acc_test}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5264179671752334e-08"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_test_d_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.831416"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_test_d_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model for dataset 0\n",
      "Building model for dataset 1\n",
      "Building model for dataset 2\n",
      "Building model for dataset 3\n",
      "0.831012\n"
     ]
    }
   ],
   "source": [
    "# train actual models\n",
    "# degree 9 seems to be best\n",
    "\n",
    "def model_split_data(X):\n",
    "    return build_X(X, 11, 3)\n",
    "\n",
    "X_split_poly = [ model_split_data(X) for X in X_split ]\n",
    "lambda_ = 1.830311675620613e-08\n",
    "models = []\n",
    "y_pred = np.ones(tX.shape[0])\n",
    "\n",
    "for i in range(len(X_split_poly)):\n",
    "    print(f\"Building model for dataset {i}\")\n",
    "    lse = LeastSquaresL2(lambda_)\n",
    "    lse.fit(y_split[i], X_split_poly[i])\n",
    "    models.append(lse)\n",
    "    y_pred[indices_split[i]] = lse.predict(X_split_poly[i])\n",
    "    \n",
    "print(np.mean(y == y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "Evaluating model\n",
      "Evaluating model\n",
      "Evaluating model\n",
      "0.8275581710480467\n"
     ]
    }
   ],
   "source": [
    "X_test_split_poly = [ model_split_data(X) for X in X_test_split ]\n",
    "y_pred = np.ones(tX_test.shape[0])\n",
    "\n",
    "for model, X, indices in zip(models, X_test_split_poly, test_split_indices):\n",
    "    print(\"Evaluating model\")\n",
    "    y_pred[indices] = model.predict(X)\n",
    "    \n",
    "print(np.mean(y_test == y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ...,  1, -1, -1])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_new: 76658898567251466163194953728.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 38329449283176485824627736576.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 19164724641363614885782487040.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 9582362320569493429625552896.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 4791181160228590807691558912.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 2395590580086217725163077632.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 1197795290029069748362280960.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 598897645007515248352034816.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 299448822500247879980941312.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 149724411248369067892932608.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 74862205623307097897697280.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 37431102811214830924464128.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 18715551405388056450039808.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 9357775702584346571440128.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 4678887851237333532672000.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 2339443925591246352941056.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 1169721962781913238208512.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 584860981384101649973248.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 292430490688623340421120.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 146215245342597927927808.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 73107622670442084433920.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 36553811334792602451968.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 18276905667182079246336.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 9138452833483928633344.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 4569226416688408821760.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 2284613208317426663424.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 1142306604145324326912.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 571153302065967857664.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 285576651029636710400.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 142788325513144713216.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 71394162755735568384.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 35697081377449381888.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 17848540688515493888.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 8924270344153145344.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 4462135172024272384.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 2231067585985986560.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 1115533792979918336.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 557766896483421696.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 278883448238442208.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 139441724117586736.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 69720862057976232.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 34860431028579564.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 17430215514085542.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 8715107756940701.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 4357553878419383.500 - f: 69254.414 - Backtracking...\n",
      "f_new: 2178776939184305.750 - f: 69254.414 - Backtracking...\n",
      "f_new: 1089388469579598.500 - f: 69254.414 - Backtracking...\n",
      "f_new: 544694234783711.938 - f: 69254.414 - Backtracking...\n",
      "f_new: 272347117389066.719 - f: 69254.414 - Backtracking...\n",
      "f_new: 136173558693473.750 - f: 69254.414 - Backtracking...\n",
      "f_new: 68086779346642.148 - f: 69254.414 - Backtracking...\n",
      "f_new: 34043389673833.574 - f: 69254.414 - Backtracking...\n",
      "f_new: 17021694837892.352 - f: 69254.414 - Backtracking...\n",
      "f_new: 8510847420357.464 - f: 69254.414 - Backtracking...\n",
      "f_new: 4255423712067.646 - f: 69254.414 - Backtracking...\n",
      "f_new: 2127711858493.365 - f: 69254.414 - Backtracking...\n",
      "f_new: 1063855932419.652 - f: 69254.414 - Backtracking...\n",
      "f_new: 531927970307.091 - f: 69254.414 - Backtracking...\n",
      "f_new: 265963990478.546 - f: 69254.414 - Backtracking...\n",
      "f_new: 132982002214.515 - f: 69254.414 - Backtracking...\n",
      "f_new: 66491010303.369 - f: 69254.414 - Backtracking...\n",
      "f_new: 33245517324.573 - f: 69254.414 - Backtracking...\n",
      "f_new: 16622774634.730 - f: 69254.414 - Backtracking...\n",
      "f_new: 8311407338.168 - f: 69254.414 - Backtracking...\n",
      "f_new: 4155727005.679 - f: 69254.414 - Backtracking...\n",
      "f_new: 2077889258.295 - f: 69254.414 - Backtracking...\n",
      "f_new: 1038972215.206 - f: 69254.414 - Backtracking...\n",
      "f_new: 519515086.671 - f: 69254.414 - Backtracking...\n",
      "f_new: 259787523.507 - f: 69254.414 - Backtracking...\n",
      "f_new: 129924456.921 - f: 69254.414 - Backtracking...\n",
      "f_new: 64993486.571 - f: 69254.414 - Backtracking...\n",
      "f_new: 32528485.172 - f: 69254.414 - Backtracking...\n",
      "f_new: 16296420.965 - f: 69254.414 - Backtracking...\n",
      "f_new: 8180791.802 - f: 69254.414 - Backtracking...\n",
      "f_new: 4123352.532 - f: 69254.414 - Backtracking...\n",
      "f_new: 2094974.372 - f: 69254.414 - Backtracking...\n",
      "f_new: 1081082.186 - f: 69254.414 - Backtracking...\n",
      "f_new: 574383.365 - f: 69254.414 - Backtracking...\n",
      "f_new: 321231.732 - f: 69254.414 - Backtracking...\n",
      "f_new: 194809.026 - f: 69254.414 - Backtracking...\n",
      "f_new: 131714.538 - f: 69254.414 - Backtracking...\n",
      "f_new: 100257.696 - f: 69254.414 - Backtracking...\n",
      "f_new: 84596.086 - f: 69254.414 - Backtracking...\n",
      "f_new: 76811.424 - f: 69254.414 - Backtracking...\n",
      "f_new: 72951.059 - f: 69254.414 - Backtracking...\n",
      "f_new: 71043.769 - f: 69254.414 - Backtracking...\n",
      "f_new: 70107.038 - f: 69254.414 - Backtracking...\n",
      "f_new: 69651.210 - f: 69254.414 - Backtracking...\n",
      "f_new: 69432.345 - f: 69254.414 - Backtracking...\n",
      "f_new: 69329.220 - f: 69254.414 - Backtracking...\n",
      "f_new: 69282.072 - f: 69254.414 - Backtracking...\n",
      "f_new: 69261.326 - f: 69254.414 - Backtracking...\n",
      "f_new: 69252.593 - f: 69254.414 - Backtracking...\n",
      "f_new: 69249.404 - f: 69254.414 - Backtracking...\n",
      "f_new: 69248.856 - f: 69254.414 - Backtracking...\n",
      "f_new: 69249.478 - f: 69254.414 - Backtracking...\n",
      "f_new: 69250.558 - f: 69254.414 - Backtracking...\n",
      "f_new: 69251.556 - f: 69254.414 - Backtracking...\n",
      "f_new: 69252.226 - f: 69254.414 - Backtracking...\n",
      "f_new: 69252.610 - f: 69254.414 - Backtracking...\n",
      "f_new: 69252.822 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.227 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.431 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.564 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.640 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.680 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.700 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.711 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.716 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.718 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.720 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.720 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "121 - loss: 69253.721\n",
      "122 - loss: 69253.721\n",
      "123 - loss: 69253.158\n",
      "124 - loss: 69253.102\n",
      "125 - loss: 69253.065\n",
      "126 - loss: 69253.050\n",
      "127 - loss: 69253.043\n",
      "128 - loss: 69253.040\n",
      "129 - loss: 69253.039\n",
      "130 - loss: 69253.039\n",
      "131 - loss: 69253.038\n",
      "132 - loss: 69253.038\n",
      "133 - loss: 69253.038\n",
      "134 - loss: 69253.038\n",
      "135 - loss: 69253.031\n",
      "136 - loss: 69252.548\n",
      "f_new: 69624.582 - f: 69252.548 - Backtracking...\n",
      "138 - loss: 69252.548\n",
      "f_new: 69549.467 - f: 69252.548 - Backtracking...\n",
      "140 - loss: 69252.548\n",
      "f_new: 69541.870 - f: 69252.548 - Backtracking...\n",
      "142 - loss: 69252.548\n",
      "f_new: 69534.511 - f: 69252.548 - Backtracking...\n",
      "144 - loss: 69252.548\n",
      "f_new: 69526.762 - f: 69252.548 - Backtracking...\n",
      "146 - loss: 69252.548\n",
      "f_new: 69518.668 - f: 69252.548 - Backtracking...\n",
      "148 - loss: 69252.547\n",
      "f_new: 69510.288 - f: 69252.547 - Backtracking...\n",
      "150 - loss: 69252.547\n",
      "f_new: 69501.689 - f: 69252.547 - Backtracking...\n",
      "152 - loss: 69252.547\n",
      "f_new: 69492.941 - f: 69252.547 - Backtracking...\n",
      "154 - loss: 69252.547\n",
      "f_new: 69484.115 - f: 69252.547 - Backtracking...\n",
      "156 - loss: 69252.547\n",
      "f_new: 69475.281 - f: 69252.547 - Backtracking...\n",
      "158 - loss: 69252.547\n",
      "f_new: 69466.505 - f: 69252.547 - Backtracking...\n",
      "160 - loss: 69252.547\n",
      "f_new: 69457.845 - f: 69252.547 - Backtracking...\n",
      "162 - loss: 69252.547\n",
      "f_new: 69449.354 - f: 69252.547 - Backtracking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164 - loss: 69252.547\n",
      "f_new: 69441.077 - f: 69252.547 - Backtracking...\n",
      "166 - loss: 69252.547\n",
      "f_new: 69433.048 - f: 69252.547 - Backtracking...\n",
      "168 - loss: 69252.547\n",
      "f_new: 69425.296 - f: 69252.547 - Backtracking...\n",
      "170 - loss: 69252.547\n",
      "f_new: 69417.839 - f: 69252.547 - Backtracking...\n",
      "172 - loss: 69252.547\n",
      "f_new: 69410.690 - f: 69252.547 - Backtracking...\n",
      "174 - loss: 69252.547\n",
      "f_new: 69403.855 - f: 69252.547 - Backtracking...\n",
      "176 - loss: 69252.547\n",
      "f_new: 69397.336 - f: 69252.547 - Backtracking...\n",
      "178 - loss: 69252.546\n",
      "f_new: 69391.131 - f: 69252.546 - Backtracking...\n",
      "180 - loss: 69252.546\n",
      "f_new: 69385.233 - f: 69252.546 - Backtracking...\n",
      "182 - loss: 69252.546\n",
      "f_new: 69379.633 - f: 69252.546 - Backtracking...\n",
      "184 - loss: 69252.546\n",
      "f_new: 69374.322 - f: 69252.546 - Backtracking...\n",
      "186 - loss: 69252.546\n",
      "f_new: 69369.287 - f: 69252.546 - Backtracking...\n",
      "188 - loss: 69252.546\n",
      "f_new: 69364.516 - f: 69252.546 - Backtracking...\n",
      "190 - loss: 69252.546\n",
      "f_new: 69359.995 - f: 69252.546 - Backtracking...\n",
      "192 - loss: 69252.546\n",
      "f_new: 69355.713 - f: 69252.546 - Backtracking...\n",
      "194 - loss: 69252.546\n",
      "f_new: 69351.656 - f: 69252.546 - Backtracking...\n",
      "196 - loss: 69252.546\n",
      "f_new: 69347.812 - f: 69252.546 - Backtracking...\n",
      "198 - loss: 69252.546\n",
      "f_new: 69344.167 - f: 69252.546 - Backtracking...\n",
      "200 - loss: 69252.546\n",
      "f_new: 69340.710 - f: 69252.546 - Backtracking...\n",
      "202 - loss: 69252.546\n",
      "f_new: 69337.431 - f: 69252.546 - Backtracking...\n",
      "204 - loss: 69252.546\n",
      "f_new: 69334.317 - f: 69252.546 - Backtracking...\n",
      "206 - loss: 69252.546\n",
      "f_new: 69331.360 - f: 69252.546 - Backtracking...\n",
      "208 - loss: 69252.546\n",
      "f_new: 69328.549 - f: 69252.546 - Backtracking...\n",
      "210 - loss: 69252.546\n",
      "f_new: 69325.876 - f: 69252.546 - Backtracking...\n",
      "212 - loss: 69252.546\n",
      "f_new: 69323.332 - f: 69252.546 - Backtracking...\n",
      "214 - loss: 69252.546\n",
      "f_new: 69320.909 - f: 69252.546 - Backtracking...\n",
      "216 - loss: 69252.546\n",
      "f_new: 69318.600 - f: 69252.546 - Backtracking...\n",
      "218 - loss: 69252.546\n",
      "f_new: 69316.398 - f: 69252.546 - Backtracking...\n",
      "220 - loss: 69252.546\n",
      "f_new: 69314.296 - f: 69252.546 - Backtracking...\n",
      "222 - loss: 69252.546\n",
      "f_new: 69312.289 - f: 69252.546 - Backtracking...\n",
      "224 - loss: 69252.546\n",
      "f_new: 69310.372 - f: 69252.546 - Backtracking...\n",
      "226 - loss: 69252.546\n",
      "f_new: 69308.537 - f: 69252.546 - Backtracking...\n",
      "228 - loss: 69252.546\n",
      "f_new: 69306.782 - f: 69252.546 - Backtracking...\n",
      "230 - loss: 69252.546\n",
      "f_new: 69305.101 - f: 69252.546 - Backtracking...\n",
      "232 - loss: 69252.546\n",
      "f_new: 69303.491 - f: 69252.546 - Backtracking...\n",
      "234 - loss: 69252.546\n",
      "f_new: 69301.946 - f: 69252.546 - Backtracking...\n",
      "236 - loss: 69252.546\n",
      "f_new: 69300.464 - f: 69252.546 - Backtracking...\n",
      "238 - loss: 69252.546\n",
      "f_new: 69299.041 - f: 69252.546 - Backtracking...\n",
      "240 - loss: 69252.545\n",
      "f_new: 69297.674 - f: 69252.545 - Backtracking...\n",
      "242 - loss: 69252.545\n",
      "f_new: 69296.359 - f: 69252.545 - Backtracking...\n",
      "244 - loss: 69252.545\n",
      "f_new: 69295.095 - f: 69252.545 - Backtracking...\n",
      "246 - loss: 69252.545\n",
      "f_new: 69293.878 - f: 69252.545 - Backtracking...\n",
      "248 - loss: 69252.545\n",
      "f_new: 69292.706 - f: 69252.545 - Backtracking...\n",
      "250 - loss: 69252.545\n",
      "f_new: 69291.577 - f: 69252.545 - Backtracking...\n",
      "252 - loss: 69252.545\n",
      "f_new: 69290.488 - f: 69252.545 - Backtracking...\n",
      "254 - loss: 69252.545\n",
      "f_new: 69289.438 - f: 69252.545 - Backtracking...\n",
      "256 - loss: 69252.545\n",
      "f_new: 69288.424 - f: 69252.545 - Backtracking...\n",
      "258 - loss: 69252.545\n",
      "f_new: 69287.444 - f: 69252.545 - Backtracking...\n",
      "260 - loss: 69252.545\n",
      "f_new: 69286.498 - f: 69252.545 - Backtracking...\n",
      "262 - loss: 69252.545\n",
      "f_new: 69285.584 - f: 69252.545 - Backtracking...\n",
      "264 - loss: 69252.545\n",
      "f_new: 69284.699 - f: 69252.545 - Backtracking...\n",
      "266 - loss: 69252.545\n",
      "f_new: 69283.843 - f: 69252.545 - Backtracking...\n",
      "268 - loss: 69252.545\n",
      "f_new: 69283.014 - f: 69252.545 - Backtracking...\n",
      "270 - loss: 69252.545\n",
      "f_new: 69282.211 - f: 69252.545 - Backtracking...\n",
      "272 - loss: 69252.545\n",
      "f_new: 69281.433 - f: 69252.545 - Backtracking...\n",
      "274 - loss: 69252.545\n",
      "f_new: 69280.679 - f: 69252.545 - Backtracking...\n",
      "276 - loss: 69252.545\n",
      "f_new: 69279.948 - f: 69252.545 - Backtracking...\n",
      "278 - loss: 69252.545\n",
      "f_new: 69279.238 - f: 69252.545 - Backtracking...\n",
      "280 - loss: 69252.545\n",
      "f_new: 69278.549 - f: 69252.545 - Backtracking...\n",
      "282 - loss: 69252.545\n",
      "f_new: 69277.879 - f: 69252.545 - Backtracking...\n",
      "284 - loss: 69252.545\n",
      "f_new: 69277.229 - f: 69252.545 - Backtracking...\n",
      "286 - loss: 69252.545\n",
      "f_new: 69276.597 - f: 69252.545 - Backtracking...\n",
      "288 - loss: 69252.545\n",
      "f_new: 69275.983 - f: 69252.545 - Backtracking...\n",
      "290 - loss: 69252.545\n",
      "f_new: 69275.385 - f: 69252.545 - Backtracking...\n",
      "292 - loss: 69252.545\n",
      "f_new: 69274.803 - f: 69252.545 - Backtracking...\n",
      "294 - loss: 69252.545\n",
      "f_new: 69274.237 - f: 69252.545 - Backtracking...\n",
      "296 - loss: 69252.545\n",
      "f_new: 69273.685 - f: 69252.545 - Backtracking...\n",
      "298 - loss: 69252.545\n",
      "f_new: 69273.148 - f: 69252.545 - Backtracking...\n",
      "300 - loss: 69252.545\n",
      "f_new: 69272.625 - f: 69252.545 - Backtracking...\n",
      "302 - loss: 69252.545\n",
      "f_new: 69272.114 - f: 69252.545 - Backtracking...\n",
      "304 - loss: 69252.545\n",
      "f_new: 69271.616 - f: 69252.545 - Backtracking...\n",
      "306 - loss: 69252.545\n",
      "f_new: 69271.131 - f: 69252.545 - Backtracking...\n",
      "308 - loss: 69252.545\n",
      "f_new: 69270.657 - f: 69252.545 - Backtracking...\n",
      "310 - loss: 69252.545\n",
      "f_new: 69270.194 - f: 69252.545 - Backtracking...\n",
      "312 - loss: 69252.545\n",
      "f_new: 69269.742 - f: 69252.545 - Backtracking...\n",
      "314 - loss: 69252.545\n",
      "f_new: 69269.300 - f: 69252.545 - Backtracking...\n",
      "316 - loss: 69252.545\n",
      "f_new: 69268.869 - f: 69252.545 - Backtracking...\n",
      "318 - loss: 69252.545\n",
      "f_new: 69268.447 - f: 69252.545 - Backtracking...\n",
      "320 - loss: 69252.545\n",
      "f_new: 69268.034 - f: 69252.545 - Backtracking...\n",
      "322 - loss: 69252.545\n",
      "f_new: 69267.631 - f: 69252.545 - Backtracking...\n",
      "324 - loss: 69252.545\n",
      "f_new: 69267.236 - f: 69252.545 - Backtracking...\n",
      "326 - loss: 69252.545\n",
      "f_new: 69266.849 - f: 69252.545 - Backtracking...\n",
      "328 - loss: 69252.545\n",
      "f_new: 69266.470 - f: 69252.545 - Backtracking...\n",
      "330 - loss: 69252.545\n",
      "f_new: 69266.100 - f: 69252.545 - Backtracking...\n",
      "332 - loss: 69252.545\n",
      "f_new: 69265.736 - f: 69252.545 - Backtracking...\n",
      "334 - loss: 69252.545\n",
      "f_new: 69265.380 - f: 69252.545 - Backtracking...\n",
      "336 - loss: 69252.545\n",
      "f_new: 69265.031 - f: 69252.545 - Backtracking...\n",
      "338 - loss: 69252.545\n",
      "f_new: 69264.688 - f: 69252.545 - Backtracking...\n",
      "340 - loss: 69252.545\n",
      "f_new: 69264.352 - f: 69252.545 - Backtracking...\n",
      "342 - loss: 69252.545\n",
      "f_new: 69264.022 - f: 69252.545 - Backtracking...\n",
      "344 - loss: 69252.545\n",
      "f_new: 69263.698 - f: 69252.545 - Backtracking...\n",
      "346 - loss: 69252.545\n",
      "f_new: 69263.380 - f: 69252.545 - Backtracking...\n",
      "348 - loss: 69252.545\n",
      "f_new: 69263.067 - f: 69252.545 - Backtracking...\n",
      "350 - loss: 69252.545\n",
      "f_new: 69262.759 - f: 69252.545 - Backtracking...\n",
      "352 - loss: 69252.545\n",
      "f_new: 69262.457 - f: 69252.545 - Backtracking...\n",
      "354 - loss: 69252.545\n",
      "f_new: 69262.160 - f: 69252.545 - Backtracking...\n",
      "356 - loss: 69252.545\n",
      "f_new: 69261.867 - f: 69252.545 - Backtracking...\n",
      "358 - loss: 69252.545\n",
      "f_new: 69261.579 - f: 69252.545 - Backtracking...\n",
      "360 - loss: 69252.545\n",
      "f_new: 69261.296 - f: 69252.545 - Backtracking...\n",
      "362 - loss: 69252.545\n",
      "f_new: 69261.016 - f: 69252.545 - Backtracking...\n",
      "364 - loss: 69252.545\n",
      "f_new: 69260.741 - f: 69252.545 - Backtracking...\n",
      "366 - loss: 69252.545\n",
      "f_new: 69260.469 - f: 69252.545 - Backtracking...\n",
      "368 - loss: 69252.545\n",
      "f_new: 69260.201 - f: 69252.545 - Backtracking...\n",
      "370 - loss: 69252.545\n",
      "f_new: 69259.937 - f: 69252.545 - Backtracking...\n",
      "372 - loss: 69252.545\n",
      "f_new: 69259.675 - f: 69252.545 - Backtracking...\n",
      "374 - loss: 69252.545\n",
      "f_new: 69259.417 - f: 69252.545 - Backtracking...\n",
      "376 - loss: 69252.545\n",
      "f_new: 69259.162 - f: 69252.545 - Backtracking...\n",
      "378 - loss: 69252.545\n",
      "f_new: 69258.909 - f: 69252.545 - Backtracking...\n",
      "380 - loss: 69252.544\n",
      "f_new: 69258.660 - f: 69252.544 - Backtracking...\n",
      "382 - loss: 69252.544\n",
      "f_new: 69258.412 - f: 69252.544 - Backtracking...\n",
      "384 - loss: 69252.544\n",
      "f_new: 69258.167 - f: 69252.544 - Backtracking...\n",
      "386 - loss: 69252.544\n",
      "f_new: 69257.924 - f: 69252.544 - Backtracking...\n",
      "388 - loss: 69252.544\n",
      "f_new: 69257.682 - f: 69252.544 - Backtracking...\n",
      "390 - loss: 69252.544\n",
      "f_new: 69257.442 - f: 69252.544 - Backtracking...\n",
      "392 - loss: 69252.544\n",
      "f_new: 69257.204 - f: 69252.544 - Backtracking...\n",
      "394 - loss: 69252.544\n",
      "f_new: 69256.967 - f: 69252.544 - Backtracking...\n",
      "396 - loss: 69252.544\n",
      "f_new: 69256.731 - f: 69252.544 - Backtracking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398 - loss: 69252.544\n",
      "f_new: 69256.496 - f: 69252.544 - Backtracking...\n",
      "400 - loss: 69252.544\n",
      "Reached maximum number of function evaluations 400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3692912834165724"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing regularized logistic regression \n",
    "# seems to be absolute shit for this \n",
    "classifier = LogisticRegressionL2(0.01, verbose=True, max_evaluations=400)\n",
    "classifier.fit(y_split[0], X_split_poly[0])\n",
    "np.mean(y_split[0] == classifier.predict(X_split_poly[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.543932\n"
     ]
    }
   ],
   "source": [
    "kernel = LeastSquaresKernel(Kernel.kernel_poly, verbose=True, max_evals=300)\n",
    "p = 4\n",
    "lambda_ = 0\n",
    "incr = 40\n",
    "incr_pred = 30\n",
    "pred = kernel.predict(y[::incr], X__[::incr], X__[::incr_pred], p, lambda_=lambda_)\n",
    "print(np.mean(y[::incr_pred] == pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = replace_NaN_by_median(X)\n",
    "X, _, _ = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub = X[::30]\n",
    "y_sub = y[::30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_new: 351332120652.464 - f: 5776.689 - Backtracking...\n",
      "f_new: 49908376999.952 - f: 5776.689 - Backtracking...\n",
      "f_new: 7089719742.161 - f: 5776.689 - Backtracking...\n",
      "f_new: 1007128546.313 - f: 5776.689 - Backtracking...\n",
      "f_new: 143067923.558 - f: 5776.689 - Backtracking...\n",
      "f_new: 20324057.388 - f: 5776.689 - Backtracking...\n",
      "f_new: 2887715.309 - f: 5776.689 - Backtracking...\n",
      "f_new: 410801.817 - f: 5776.689 - Backtracking...\n",
      "f_new: 58949.841 - f: 5776.689 - Backtracking...\n",
      "f_new: 9667.717 - f: 5776.689 - Backtracking...\n",
      "11 - loss: 5549.281\n",
      "12 - loss: 5405.800\n",
      "13 - loss: 5352.943\n",
      "14 - loss: 5260.879\n",
      "15 - loss: 5057.649\n",
      "f_new: 5188.842 - f: 5057.649 - Backtracking...\n",
      "17 - loss: 5042.088\n",
      "18 - loss: 5030.287\n",
      "19 - loss: 5022.815\n",
      "20 - loss: 4723.083\n",
      "f_new: 5530.003 - f: 4723.083 - Backtracking...\n",
      "22 - loss: 4714.058\n",
      "23 - loss: 4702.917\n",
      "24 - loss: 4700.799\n",
      "25 - loss: 4698.556\n",
      "26 - loss: 4630.235\n",
      "27 - loss: 4608.311\n",
      "f_new: 4631.497 - f: 4608.311 - Backtracking...\n",
      "29 - loss: 4598.083\n",
      "30 - loss: 4596.268\n",
      "31 - loss: 4594.125\n",
      "32 - loss: 4582.093\n",
      "33 - loss: 4575.996\n",
      "34 - loss: 4550.599\n",
      "f_new: 4573.412 - f: 4550.599 - Backtracking...\n",
      "36 - loss: 4549.247\n",
      "37 - loss: 4549.045\n",
      "38 - loss: 4548.844\n",
      "39 - loss: 4529.424\n",
      "f_new: 4534.597 - f: 4529.424 - Backtracking...\n",
      "41 - loss: 4526.565\n",
      "f_new: 4531.257 - f: 4526.565 - Backtracking...\n",
      "43 - loss: 4525.883\n",
      "44 - loss: 4525.601\n",
      "45 - loss: 4525.368\n",
      "46 - loss: 4523.787\n",
      "47 - loss: 4522.931\n",
      "48 - loss: 4522.724\n",
      "49 - loss: 4522.637\n",
      "50 - loss: 4522.573\n",
      "51 - loss: 4521.830\n",
      "52 - loss: 4503.879\n",
      "f_new: 4512.120 - f: 4503.879 - Backtracking...\n",
      "54 - loss: 4503.280\n",
      "f_new: 4504.387 - f: 4503.280 - Backtracking...\n",
      "56 - loss: 4503.135\n",
      "57 - loss: 4503.078\n",
      "58 - loss: 4503.030\n",
      "59 - loss: 4502.749\n",
      "60 - loss: 4502.654\n",
      "61 - loss: 4502.642\n",
      "62 - loss: 4502.635\n",
      "63 - loss: 4502.627\n",
      "64 - loss: 4499.973\n",
      "65 - loss: 4499.873\n",
      "f_new: 4501.201 - f: 4499.873 - Backtracking...\n",
      "67 - loss: 4499.465\n",
      "68 - loss: 4499.452\n",
      "69 - loss: 4499.420\n",
      "70 - loss: 4499.313\n",
      "71 - loss: 4499.241\n",
      "72 - loss: 4499.151\n",
      "73 - loss: 4499.114\n",
      "74 - loss: 4499.100\n",
      "75 - loss: 4499.098\n",
      "76 - loss: 4498.991\n",
      "77 - loss: 4498.931\n",
      "78 - loss: 4498.902\n",
      "f_new: 4498.948 - f: 4498.902 - Backtracking...\n",
      "80 - loss: 4498.897\n",
      "81 - loss: 4498.896\n",
      "82 - loss: 4498.895\n",
      "83 - loss: 4498.882\n",
      "84 - loss: 4498.868\n",
      "85 - loss: 4498.805\n",
      "86 - loss: 4498.798\n",
      "87 - loss: 4498.741\n",
      "88 - loss: 4498.741\n",
      "89 - loss: 4498.740\n",
      "90 - loss: 4498.728\n",
      "91 - loss: 4498.716\n",
      "92 - loss: 4498.636\n",
      "93 - loss: 4498.474\n",
      "94 - loss: 4498.428\n",
      "95 - loss: 4498.427\n",
      "96 - loss: 4498.427\n",
      "97 - loss: 4498.417\n",
      "98 - loss: 4498.407\n",
      "99 - loss: 4497.942\n",
      "f_new: 4498.956 - f: 4497.942 - Backtracking...\n",
      "101 - loss: 4497.912\n",
      "102 - loss: 4497.882\n",
      "103 - loss: 4497.881\n",
      "104 - loss: 4497.880\n",
      "105 - loss: 4497.852\n",
      "106 - loss: 4497.820\n",
      "107 - loss: 4497.806\n",
      "108 - loss: 4497.796\n",
      "109 - loss: 4497.795\n",
      "110 - loss: 4497.794\n",
      "111 - loss: 4497.793\n",
      "112 - loss: 4497.354\n",
      "113 - loss: 4496.800\n",
      "114 - loss: 4496.751\n",
      "f_new: 4496.845 - f: 4496.751 - Backtracking...\n",
      "116 - loss: 4496.748\n",
      "117 - loss: 4496.747\n",
      "118 - loss: 4496.746\n",
      "119 - loss: 4496.737\n",
      "120 - loss: 4496.728\n",
      "121 - loss: 4496.620\n",
      "122 - loss: 4496.497\n",
      "123 - loss: 4496.491\n",
      "124 - loss: 4496.490\n",
      "125 - loss: 4496.489\n",
      "126 - loss: 4496.475\n",
      "127 - loss: 4496.460\n",
      "128 - loss: 4496.432\n",
      "129 - loss: 4496.381\n",
      "130 - loss: 4496.363\n",
      "131 - loss: 4496.360\n",
      "132 - loss: 4496.360\n",
      "133 - loss: 4496.359\n",
      "134 - loss: 4496.353\n",
      "135 - loss: 4496.343\n",
      "136 - loss: 4496.325\n",
      "137 - loss: 4496.039\n",
      "138 - loss: 4496.038\n",
      "139 - loss: 4496.037\n",
      "140 - loss: 4496.036\n",
      "141 - loss: 4496.026\n",
      "142 - loss: 4496.015\n",
      "143 - loss: 4495.933\n",
      "144 - loss: 4493.641\n",
      "f_new: 4493.794 - f: 4493.641 - Backtracking...\n",
      "146 - loss: 4493.636\n",
      "147 - loss: 4493.634\n",
      "148 - loss: 4493.633\n",
      "149 - loss: 4493.621\n",
      "150 - loss: 4493.613\n",
      "151 - loss: 4493.612\n",
      "152 - loss: 4493.611\n",
      "153 - loss: 4493.611\n",
      "154 - loss: 4468.050\n",
      "f_new: 17878.460 - f: 4468.050 - Backtracking...\n",
      "f_new: 4468.051 - f: 4468.050 - Backtracking...\n",
      "157 - loss: 4468.048\n",
      "158 - loss: 4468.046\n",
      "159 - loss: 4468.046\n",
      "160 - loss: 4468.045\n",
      "161 - loss: 4468.032\n",
      "162 - loss: 4468.018\n",
      "163 - loss: 4468.005\n",
      "164 - loss: 4468.003\n",
      "165 - loss: 4468.002\n",
      "166 - loss: 4468.002\n",
      "167 - loss: 4468.000\n",
      "168 - loss: 4467.988\n",
      "169 - loss: 4467.972\n",
      "170 - loss: 4467.958\n",
      "171 - loss: 4467.956\n",
      "172 - loss: 4467.955\n",
      "173 - loss: 4467.954\n",
      "174 - loss: 4467.954\n",
      "175 - loss: 4467.945\n",
      "176 - loss: 4467.935\n",
      "177 - loss: 4467.921\n",
      "178 - loss: 4467.908\n",
      "179 - loss: 4467.908\n",
      "180 - loss: 4467.907\n",
      "181 - loss: 4467.907\n",
      "182 - loss: 4467.899\n",
      "183 - loss: 4467.892\n",
      "184 - loss: 4467.873\n",
      "185 - loss: 4467.829\n",
      "186 - loss: 4467.828\n",
      "187 - loss: 4467.828\n",
      "188 - loss: 4467.828\n",
      "189 - loss: 4467.822\n",
      "190 - loss: 4467.817\n",
      "191 - loss: 4467.605\n",
      "192 - loss: 4467.590\n",
      "193 - loss: 4467.517\n",
      "194 - loss: 4467.516\n",
      "195 - loss: 4467.516\n",
      "196 - loss: 4467.484\n",
      "197 - loss: 4467.446\n",
      "198 - loss: 4467.440\n",
      "199 - loss: 4467.434\n",
      "200 - loss: 4467.433\n",
      "201 - loss: 4467.433\n",
      "202 - loss: 4467.433\n",
      "203 - loss: 4467.224\n",
      "204 - loss: 4461.692\n",
      "f_new: 4465.338 - f: 4461.692 - Backtracking...\n",
      "206 - loss: 4461.682\n",
      "207 - loss: 4461.676\n",
      "208 - loss: 4461.674\n",
      "209 - loss: 4461.673\n",
      "210 - loss: 4461.665\n",
      "211 - loss: 4461.660\n",
      "212 - loss: 4461.659\n",
      "213 - loss: 4461.659\n",
      "214 - loss: 4461.659\n",
      "215 - loss: 4461.646\n",
      "216 - loss: 4461.562\n",
      "217 - loss: 4461.462\n",
      "218 - loss: 4461.461\n",
      "219 - loss: 4461.461\n",
      "220 - loss: 4461.460\n",
      "221 - loss: 4461.445\n",
      "222 - loss: 4461.427\n",
      "223 - loss: 4461.421\n",
      "224 - loss: 4461.416\n",
      "225 - loss: 4461.412\n",
      "226 - loss: 4461.412\n",
      "227 - loss: 4461.411\n",
      "228 - loss: 4461.411\n",
      "229 - loss: 4460.963\n",
      "230 - loss: 4460.522\n",
      "f_new: 4460.592 - f: 4460.522 - Backtracking...\n",
      "232 - loss: 4460.519\n",
      "233 - loss: 4460.517\n",
      "234 - loss: 4460.516\n",
      "235 - loss: 4460.513\n",
      "236 - loss: 4460.506\n",
      "237 - loss: 4460.502\n",
      "238 - loss: 4460.501\n",
      "239 - loss: 4460.500\n",
      "240 - loss: 4460.500\n",
      "241 - loss: 4460.446\n",
      "242 - loss: 4460.396\n",
      "243 - loss: 4460.393\n",
      "244 - loss: 4460.392\n",
      "245 - loss: 4460.392\n",
      "246 - loss: 4460.389\n",
      "247 - loss: 4460.365\n",
      "248 - loss: 4460.297\n",
      "249 - loss: 4460.287\n",
      "250 - loss: 4460.282\n",
      "251 - loss: 4460.280\n",
      "252 - loss: 4460.280\n",
      "253 - loss: 4460.279\n",
      "254 - loss: 4460.246\n",
      "255 - loss: 4453.774\n",
      "f_new: 4465.565 - f: 4453.774 - Backtracking...\n",
      "257 - loss: 4453.764\n",
      "258 - loss: 4453.761\n",
      "259 - loss: 4453.759\n",
      "260 - loss: 4453.759\n",
      "261 - loss: 4453.758\n",
      "262 - loss: 4453.748\n",
      "263 - loss: 4453.736\n",
      "264 - loss: 4453.730\n",
      "265 - loss: 4453.725\n",
      "266 - loss: 4453.721\n",
      "267 - loss: 4453.720\n",
      "268 - loss: 4453.720\n",
      "269 - loss: 4453.720\n",
      "270 - loss: 4453.154\n",
      "271 - loss: 4452.624\n",
      "f_new: 4452.717 - f: 4452.624 - Backtracking...\n",
      "273 - loss: 4452.598\n",
      "274 - loss: 4452.582\n",
      "275 - loss: 4452.577\n",
      "276 - loss: 4452.574\n",
      "277 - loss: 4452.553\n",
      "278 - loss: 4452.546\n",
      "279 - loss: 4452.541\n",
      "280 - loss: 4452.537\n",
      "281 - loss: 4452.536\n",
      "282 - loss: 4452.536\n",
      "283 - loss: 4452.509\n",
      "284 - loss: 4452.487\n",
      "285 - loss: 4452.478\n",
      "286 - loss: 4452.476\n",
      "287 - loss: 4452.476\n",
      "288 - loss: 4452.476\n",
      "289 - loss: 4452.458\n",
      "290 - loss: 4452.438\n",
      "291 - loss: 4452.434\n",
      "292 - loss: 4452.430\n",
      "293 - loss: 4452.428\n",
      "294 - loss: 4452.428\n",
      "295 - loss: 4452.428\n",
      "296 - loss: 4452.420\n",
      "297 - loss: 4452.183\n",
      "298 - loss: 4451.893\n",
      "f_new: 4451.894 - f: 4451.893 - Backtracking...\n",
      "300 - loss: 4451.891\n",
      "301 - loss: 4451.891\n",
      "302 - loss: 4451.891\n",
      "303 - loss: 4451.886\n",
      "304 - loss: 4451.882\n",
      "305 - loss: 4451.871\n",
      "306 - loss: 4451.870\n",
      "307 - loss: 4451.870\n",
      "308 - loss: 4451.869\n",
      "309 - loss: 4451.818\n",
      "310 - loss: 4451.767\n",
      "311 - loss: 4451.711\n",
      "312 - loss: 4451.709\n",
      "313 - loss: 4451.709\n",
      "314 - loss: 4451.709\n",
      "315 - loss: 4451.705\n",
      "316 - loss: 4451.701\n",
      "317 - loss: 4451.073\n",
      "f_new: 4452.226 - f: 4451.073 - Backtracking...\n",
      "319 - loss: 4451.067\n",
      "320 - loss: 4451.061\n",
      "321 - loss: 4451.060\n",
      "322 - loss: 4451.060\n",
      "323 - loss: 4451.017\n",
      "324 - loss: 4450.972\n",
      "325 - loss: 4450.967\n",
      "326 - loss: 4450.963\n",
      "327 - loss: 4450.963\n",
      "328 - loss: 4450.963\n",
      "329 - loss: 4450.962\n",
      "330 - loss: 4450.785\n",
      "331 - loss: 4450.594\n",
      "332 - loss: 4450.583\n",
      "f_new: 4450.584 - f: 4450.583 - Backtracking...\n",
      "334 - loss: 4450.582\n",
      "335 - loss: 4450.581\n",
      "336 - loss: 4450.581\n",
      "337 - loss: 4450.580\n",
      "338 - loss: 4450.575\n",
      "339 - loss: 4450.569\n",
      "340 - loss: 4450.555\n",
      "341 - loss: 4450.551\n",
      "342 - loss: 4450.551\n",
      "343 - loss: 4450.550\n",
      "344 - loss: 4450.550\n",
      "345 - loss: 4450.521\n",
      "346 - loss: 4450.491\n",
      "347 - loss: 4450.487\n",
      "348 - loss: 4450.483\n",
      "349 - loss: 4450.483\n",
      "350 - loss: 4450.482\n",
      "351 - loss: 4450.482\n",
      "352 - loss: 4447.174\n",
      "f_new: 4494.868 - f: 4447.174 - Backtracking...\n",
      "354 - loss: 4446.870\n",
      "355 - loss: 4446.776\n",
      "356 - loss: 4446.682\n",
      "357 - loss: 4446.678\n",
      "358 - loss: 4446.626\n",
      "359 - loss: 4446.595\n",
      "360 - loss: 4446.566\n",
      "f_new: 4446.600 - f: 4446.566 - Backtracking...\n",
      "362 - loss: 4446.564\n",
      "363 - loss: 4446.562\n",
      "364 - loss: 4446.562\n",
      "365 - loss: 4446.561\n",
      "366 - loss: 4446.545\n",
      "367 - loss: 4446.533\n",
      "368 - loss: 4446.529\n",
      "f_new: 4446.530 - f: 4446.529 - Backtracking...\n",
      "370 - loss: 4446.528\n",
      "371 - loss: 4446.528\n",
      "372 - loss: 4446.527\n",
      "373 - loss: 4446.523\n",
      "374 - loss: 4446.519\n",
      "375 - loss: 4446.508\n",
      "376 - loss: 4446.491\n",
      "377 - loss: 4446.491\n",
      "378 - loss: 4446.491\n",
      "379 - loss: 4446.490\n",
      "380 - loss: 4446.487\n",
      "381 - loss: 4446.483\n",
      "382 - loss: 4446.426\n",
      "383 - loss: 4446.209\n",
      "384 - loss: 4446.208\n",
      "385 - loss: 4446.207\n",
      "386 - loss: 4446.207\n",
      "387 - loss: 4446.203\n",
      "388 - loss: 4446.199\n",
      "389 - loss: 4446.186\n",
      "390 - loss: 4446.165\n",
      "391 - loss: 4446.162\n",
      "392 - loss: 4446.162\n",
      "393 - loss: 4446.162\n",
      "394 - loss: 4446.161\n",
      "395 - loss: 4446.144\n",
      "396 - loss: 4446.124\n",
      "397 - loss: 4446.119\n",
      "398 - loss: 4446.116\n",
      "399 - loss: 4446.114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 - loss: 4446.114\n",
      "401 - loss: 4446.114\n",
      "402 - loss: 4446.112\n",
      "403 - loss: 4439.172\n",
      "f_new: 6378.863 - f: 4439.172 - Backtracking...\n",
      "405 - loss: 4439.165\n",
      "406 - loss: 4439.154\n",
      "407 - loss: 4439.154\n",
      "408 - loss: 4439.154\n",
      "409 - loss: 4439.151\n",
      "410 - loss: 4439.148\n",
      "411 - loss: 4439.077\n",
      "f_new: 4439.109 - f: 4439.077 - Backtracking...\n",
      "413 - loss: 4439.072\n",
      "414 - loss: 4439.067\n",
      "415 - loss: 4439.067\n",
      "416 - loss: 4439.066\n",
      "417 - loss: 4438.991\n",
      "418 - loss: 4438.892\n",
      "419 - loss: 4438.888\n",
      "f_new: 4438.888 - f: 4438.888 - Backtracking...\n",
      "421 - loss: 4438.887\n",
      "422 - loss: 4438.887\n",
      "423 - loss: 4438.887\n",
      "424 - loss: 4438.884\n",
      "425 - loss: 4438.881\n",
      "426 - loss: 4438.876\n",
      "427 - loss: 4438.798\n",
      "428 - loss: 4438.691\n",
      "429 - loss: 4438.690\n",
      "430 - loss: 4438.689\n",
      "431 - loss: 4438.689\n",
      "432 - loss: 4438.688\n",
      "433 - loss: 4438.685\n",
      "434 - loss: 4438.682\n",
      "435 - loss: 4438.675\n",
      "436 - loss: 4438.674\n",
      "437 - loss: 4438.674\n",
      "438 - loss: 4438.674\n",
      "439 - loss: 4438.644\n",
      "440 - loss: 4438.614\n",
      "441 - loss: 4438.574\n",
      "442 - loss: 4438.573\n",
      "443 - loss: 4438.572\n",
      "444 - loss: 4438.572\n",
      "445 - loss: 4438.572\n",
      "446 - loss: 4438.569\n",
      "447 - loss: 4438.566\n",
      "448 - loss: 4438.536\n",
      "449 - loss: 4438.519\n",
      "450 - loss: 4438.518\n",
      "451 - loss: 4438.518\n",
      "452 - loss: 4438.518\n",
      "453 - loss: 4438.508\n",
      "454 - loss: 4438.498\n",
      "455 - loss: 4438.494\n",
      "456 - loss: 4438.491\n",
      "457 - loss: 4438.487\n",
      "458 - loss: 4438.487\n",
      "459 - loss: 4438.487\n",
      "460 - loss: 4438.487\n",
      "461 - loss: 4437.533\n",
      "f_new: 4442.972 - f: 4437.533 - Backtracking...\n",
      "463 - loss: 4436.354\n",
      "464 - loss: 4435.380\n",
      "f_new: 4437.712 - f: 4435.380 - Backtracking...\n",
      "466 - loss: 4435.229\n",
      "467 - loss: 4435.229\n",
      "468 - loss: 4435.228\n",
      "469 - loss: 4435.225\n",
      "470 - loss: 4435.222\n",
      "471 - loss: 4435.211\n",
      "472 - loss: 4435.210\n",
      "473 - loss: 4435.210\n",
      "474 - loss: 4435.209\n",
      "475 - loss: 4435.198\n",
      "476 - loss: 4435.186\n",
      "477 - loss: 4435.183\n",
      "478 - loss: 4435.181\n",
      "479 - loss: 4435.180\n",
      "480 - loss: 4435.180\n",
      "481 - loss: 4435.180\n",
      "482 - loss: 4434.978\n",
      "483 - loss: 4434.638\n",
      "484 - loss: 4434.624\n",
      "f_new: 4434.651 - f: 4434.624 - Backtracking...\n",
      "486 - loss: 4434.622\n",
      "487 - loss: 4434.621\n",
      "488 - loss: 4434.620\n",
      "489 - loss: 4434.614\n",
      "490 - loss: 4434.612\n",
      "491 - loss: 4434.611\n",
      "492 - loss: 4434.611\n",
      "493 - loss: 4434.611\n",
      "494 - loss: 4434.592\n",
      "495 - loss: 4434.573\n",
      "496 - loss: 4434.544\n",
      "497 - loss: 4434.543\n",
      "498 - loss: 4434.543\n",
      "499 - loss: 4434.543\n",
      "500 - loss: 4434.539\n",
      "501 - loss: 4434.534\n",
      "502 - loss: 4434.528\n",
      "503 - loss: 4434.518\n",
      "504 - loss: 4434.513\n",
      "505 - loss: 4434.513\n",
      "506 - loss: 4434.513\n",
      "507 - loss: 4434.513\n",
      "508 - loss: 4434.509\n",
      "509 - loss: 4434.506\n",
      "510 - loss: 4434.490\n",
      "511 - loss: 4434.389\n",
      "512 - loss: 4434.388\n",
      "513 - loss: 4434.388\n",
      "514 - loss: 4434.388\n",
      "515 - loss: 4434.385\n",
      "516 - loss: 4434.383\n",
      "517 - loss: 4432.661\n",
      "f_new: 4435.044 - f: 4432.661 - Backtracking...\n",
      "519 - loss: 4432.661\n",
      "520 - loss: 4432.661\n",
      "521 - loss: 4432.661\n",
      "522 - loss: 4432.635\n",
      "523 - loss: 4432.609\n",
      "524 - loss: 4432.606\n",
      "525 - loss: 4432.604\n",
      "526 - loss: 4432.603\n",
      "527 - loss: 4432.603\n",
      "528 - loss: 4432.603\n",
      "529 - loss: 4432.582\n",
      "530 - loss: 4432.445\n",
      "531 - loss: 4431.995\n",
      "f_new: 4432.004 - f: 4431.995 - Backtracking...\n",
      "533 - loss: 4431.993\n",
      "534 - loss: 4431.993\n",
      "535 - loss: 4431.993\n",
      "536 - loss: 4431.989\n",
      "537 - loss: 4431.986\n",
      "538 - loss: 4431.977\n",
      "539 - loss: 4431.977\n",
      "540 - loss: 4431.976\n",
      "541 - loss: 4431.976\n",
      "542 - loss: 4431.966\n",
      "543 - loss: 4431.955\n",
      "544 - loss: 4431.952\n",
      "545 - loss: 4431.950\n",
      "546 - loss: 4431.950\n",
      "547 - loss: 4431.950\n",
      "548 - loss: 4431.949\n",
      "549 - loss: 4431.870\n",
      "550 - loss: 4431.790\n",
      "551 - loss: 4431.775\n",
      "552 - loss: 4431.773\n",
      "553 - loss: 4431.772\n",
      "554 - loss: 4431.772\n",
      "555 - loss: 4431.772\n",
      "556 - loss: 4431.769\n",
      "557 - loss: 4431.767\n",
      "558 - loss: 4431.746\n",
      "559 - loss: 4431.696\n",
      "560 - loss: 4431.688\n",
      "561 - loss: 4431.687\n",
      "562 - loss: 4431.686\n",
      "563 - loss: 4431.686\n",
      "564 - loss: 4431.684\n",
      "565 - loss: 4431.681\n",
      "566 - loss: 4431.554\n",
      "567 - loss: 4431.105\n",
      "f_new: 4431.108 - f: 4431.105 - Backtracking...\n",
      "569 - loss: 4431.103\n",
      "570 - loss: 4431.103\n",
      "571 - loss: 4431.102\n",
      "572 - loss: 4431.091\n",
      "573 - loss: 4431.082\n",
      "574 - loss: 4431.079\n",
      "575 - loss: 4431.077\n",
      "576 - loss: 4431.077\n",
      "577 - loss: 4431.077\n",
      "578 - loss: 4431.076\n",
      "579 - loss: 4431.065\n",
      "580 - loss: 4431.054\n",
      "581 - loss: 4431.052\n",
      "582 - loss: 4431.049\n",
      "583 - loss: 4431.047\n",
      "584 - loss: 4431.047\n",
      "585 - loss: 4431.046\n",
      "586 - loss: 4431.046\n",
      "587 - loss: 4429.409\n",
      "588 - loss: 4428.748\n",
      "f_new: 4435.013 - f: 4428.748 - Backtracking...\n",
      "590 - loss: 4428.595\n",
      "591 - loss: 4428.578\n",
      "592 - loss: 4428.559\n",
      "593 - loss: 4427.957\n",
      "594 - loss: 4427.932\n",
      "f_new: 4428.104 - f: 4427.932 - Backtracking...\n",
      "596 - loss: 4427.866\n",
      "597 - loss: 4427.826\n",
      "598 - loss: 4427.813\n",
      "599 - loss: 4427.805\n",
      "600 - loss: 4427.750\n",
      "601 - loss: 4427.734\n",
      "602 - loss: 4427.729\n",
      "603 - loss: 4427.726\n",
      "604 - loss: 4427.725\n",
      "605 - loss: 4427.722\n",
      "606 - loss: 4427.592\n",
      "607 - loss: 4427.571\n",
      "608 - loss: 4427.564\n",
      "609 - loss: 4427.560\n",
      "610 - loss: 4427.559\n",
      "611 - loss: 4427.555\n",
      "612 - loss: 4427.550\n",
      "613 - loss: 4427.546\n",
      "614 - loss: 4427.542\n",
      "615 - loss: 4427.541\n",
      "616 - loss: 4427.540\n",
      "617 - loss: 4427.540\n",
      "618 - loss: 4427.540\n",
      "619 - loss: 4427.534\n",
      "620 - loss: 4427.527\n",
      "621 - loss: 4427.525\n",
      "622 - loss: 4427.522\n",
      "623 - loss: 4427.522\n",
      "624 - loss: 4427.522\n",
      "625 - loss: 4427.522\n",
      "626 - loss: 4427.505\n",
      "627 - loss: 4427.488\n",
      "628 - loss: 4427.480\n",
      "629 - loss: 4427.478\n",
      "630 - loss: 4427.476\n",
      "631 - loss: 4427.475\n",
      "632 - loss: 4427.475\n",
      "633 - loss: 4427.475\n",
      "634 - loss: 4425.864\n",
      "f_new: 4425.902 - f: 4425.864 - Backtracking...\n",
      "636 - loss: 4425.048\n",
      "f_new: 4431.087 - f: 4425.048 - Backtracking...\n",
      "638 - loss: 4424.960\n",
      "639 - loss: 4424.934\n",
      "640 - loss: 4424.909\n",
      "641 - loss: 4424.625\n",
      "642 - loss: 4424.465\n",
      "643 - loss: 4424.461\n",
      "644 - loss: 4424.348\n",
      "645 - loss: 4424.340\n",
      "646 - loss: 4424.338\n",
      "647 - loss: 4424.272\n",
      "648 - loss: 4424.254\n",
      "649 - loss: 4424.245\n",
      "f_new: 4424.256 - f: 4424.245 - Backtracking...\n",
      "651 - loss: 4424.240\n",
      "652 - loss: 4424.240\n",
      "653 - loss: 4424.239\n",
      "654 - loss: 4424.235\n",
      "655 - loss: 4424.230\n",
      "656 - loss: 4424.221\n",
      "f_new: 4424.222 - f: 4424.221 - Backtracking...\n",
      "658 - loss: 4424.220\n",
      "659 - loss: 4424.219\n",
      "660 - loss: 4424.219\n",
      "661 - loss: 4424.217\n",
      "662 - loss: 4424.201\n",
      "663 - loss: 4424.171\n",
      "f_new: 4424.175 - f: 4424.171 - Backtracking...\n",
      "665 - loss: 4424.170\n",
      "666 - loss: 4424.169\n",
      "667 - loss: 4424.169\n",
      "668 - loss: 4424.168\n",
      "669 - loss: 4424.166\n",
      "670 - loss: 4424.164\n",
      "671 - loss: 4424.135\n",
      "672 - loss: 4424.078\n",
      "673 - loss: 4424.078\n",
      "674 - loss: 4424.077\n",
      "675 - loss: 4424.077\n",
      "676 - loss: 4424.075\n",
      "677 - loss: 4424.073\n",
      "678 - loss: 4424.062\n",
      "679 - loss: 4424.050\n",
      "680 - loss: 4424.049\n",
      "681 - loss: 4424.049\n",
      "682 - loss: 4424.049\n",
      "683 - loss: 4424.035\n",
      "684 - loss: 4424.018\n",
      "685 - loss: 4424.016\n",
      "686 - loss: 4424.014\n",
      "687 - loss: 4424.012\n",
      "688 - loss: 4424.012\n",
      "689 - loss: 4424.012\n",
      "690 - loss: 4424.011\n",
      "691 - loss: 4423.206\n",
      "692 - loss: 4422.442\n",
      "f_new: 4422.453 - f: 4422.442 - Backtracking...\n",
      "694 - loss: 4422.425\n",
      "695 - loss: 4422.425\n",
      "696 - loss: 4422.424\n",
      "697 - loss: 4422.421\n",
      "698 - loss: 4422.419\n",
      "699 - loss: 4422.409\n",
      "700 - loss: 4422.408\n",
      "701 - loss: 4422.408\n",
      "702 - loss: 4422.408\n",
      "703 - loss: 4422.314\n",
      "704 - loss: 4422.227\n",
      "f_new: 4422.263 - f: 4422.227 - Backtracking...\n",
      "706 - loss: 4422.227\n",
      "707 - loss: 4422.227\n",
      "708 - loss: 4422.227\n",
      "709 - loss: 4422.224\n",
      "710 - loss: 4422.221\n",
      "711 - loss: 4422.216\n",
      "712 - loss: 4422.212\n",
      "713 - loss: 4422.211\n",
      "714 - loss: 4422.211\n",
      "715 - loss: 4422.211\n",
      "716 - loss: 4422.208\n",
      "717 - loss: 4422.205\n",
      "718 - loss: 4422.200\n",
      "719 - loss: 4422.192\n",
      "720 - loss: 4422.191\n",
      "721 - loss: 4422.191\n",
      "722 - loss: 4422.190\n",
      "723 - loss: 4422.190\n",
      "724 - loss: 4422.188\n",
      "725 - loss: 4422.185\n",
      "726 - loss: 4422.177\n",
      "727 - loss: 4422.161\n",
      "728 - loss: 4422.161\n",
      "729 - loss: 4422.161\n",
      "730 - loss: 4422.161\n",
      "731 - loss: 4422.159\n",
      "732 - loss: 4422.157\n",
      "733 - loss: 4422.108\n",
      "734 - loss: 4422.046\n",
      "735 - loss: 4422.043\n",
      "736 - loss: 4422.043\n",
      "737 - loss: 4422.042\n",
      "738 - loss: 4422.040\n",
      "739 - loss: 4422.038\n",
      "740 - loss: 4422.021\n",
      "741 - loss: 4421.875\n",
      "742 - loss: 4421.875\n",
      "743 - loss: 4421.873\n",
      "744 - loss: 4421.873\n",
      "745 - loss: 4421.873\n",
      "746 - loss: 4421.861\n",
      "747 - loss: 4421.847\n",
      "748 - loss: 4421.845\n",
      "749 - loss: 4421.843\n",
      "750 - loss: 4421.842\n",
      "751 - loss: 4421.842\n",
      "752 - loss: 4421.842\n",
      "753 - loss: 4421.841\n",
      "754 - loss: 4420.840\n",
      "755 - loss: 4420.055\n",
      "f_new: 4420.469 - f: 4420.055 - Backtracking...\n",
      "757 - loss: 4419.993\n",
      "758 - loss: 4419.989\n",
      "759 - loss: 4419.983\n",
      "760 - loss: 4419.956\n",
      "761 - loss: 4419.937\n",
      "762 - loss: 4419.890\n",
      "763 - loss: 4419.880\n",
      "764 - loss: 4419.874\n",
      "765 - loss: 4419.873\n",
      "766 - loss: 4419.797\n",
      "f_new: 4419.812 - f: 4419.797 - Backtracking...\n",
      "768 - loss: 4419.791\n",
      "769 - loss: 4419.787\n",
      "f_new: 4419.788 - f: 4419.787 - Backtracking...\n",
      "771 - loss: 4419.785\n",
      "772 - loss: 4419.784\n",
      "773 - loss: 4419.784\n",
      "774 - loss: 4419.756\n",
      "775 - loss: 4419.728\n",
      "776 - loss: 4419.725\n",
      "777 - loss: 4419.724\n",
      "778 - loss: 4419.724\n",
      "779 - loss: 4419.723\n",
      "780 - loss: 4419.723\n",
      "781 - loss: 4419.713\n",
      "782 - loss: 4419.701\n",
      "783 - loss: 4419.699\n",
      "784 - loss: 4419.697\n",
      "785 - loss: 4419.697\n",
      "786 - loss: 4419.697\n",
      "787 - loss: 4419.697\n",
      "788 - loss: 4418.609\n",
      "789 - loss: 4418.283\n",
      "f_new: 4419.368 - f: 4418.283 - Backtracking...\n",
      "791 - loss: 4417.996\n",
      "792 - loss: 4417.806\n",
      "793 - loss: 4417.744\n",
      "794 - loss: 4417.683\n",
      "795 - loss: 4417.422\n",
      "f_new: 4417.425 - f: 4417.422 - Backtracking...\n",
      "797 - loss: 4417.421\n",
      "798 - loss: 4417.420\n",
      "799 - loss: 4417.420\n",
      "800 - loss: 4417.419\n",
      "801 - loss: 4417.413\n",
      "802 - loss: 4417.404\n",
      "803 - loss: 4417.398\n",
      "804 - loss: 4417.397\n",
      "805 - loss: 4417.396\n",
      "806 - loss: 4417.396\n",
      "807 - loss: 4417.393\n",
      "808 - loss: 4417.389\n",
      "809 - loss: 4417.378\n",
      "810 - loss: 4417.346\n",
      "f_new: 4417.350 - f: 4417.346 - Backtracking...\n",
      "812 - loss: 4417.345\n",
      "813 - loss: 4417.345\n",
      "814 - loss: 4417.344\n",
      "815 - loss: 4417.342\n",
      "816 - loss: 4417.340\n",
      "817 - loss: 4417.337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "818 - loss: 4417.337\n",
      "819 - loss: 4417.337\n",
      "820 - loss: 4417.337\n",
      "821 - loss: 4417.236\n",
      "822 - loss: 4417.145\n",
      "f_new: 4417.199 - f: 4417.145 - Backtracking...\n",
      "824 - loss: 4417.145\n",
      "825 - loss: 4417.145\n",
      "826 - loss: 4417.145\n",
      "827 - loss: 4417.140\n",
      "828 - loss: 4417.134\n",
      "829 - loss: 4417.132\n",
      "830 - loss: 4417.130\n",
      "831 - loss: 4417.129\n",
      "832 - loss: 4417.128\n",
      "833 - loss: 4417.128\n",
      "834 - loss: 4417.128\n",
      "835 - loss: 4417.107\n",
      "836 - loss: 4417.087\n",
      "837 - loss: 4417.085\n",
      "838 - loss: 4417.085\n",
      "839 - loss: 4417.085\n",
      "840 - loss: 4417.084\n",
      "841 - loss: 4417.083\n",
      "842 - loss: 4417.081\n",
      "843 - loss: 4417.059\n",
      "844 - loss: 4416.973\n",
      "845 - loss: 4416.972\n",
      "846 - loss: 4416.972\n",
      "847 - loss: 4416.972\n",
      "848 - loss: 4416.971\n",
      "849 - loss: 4416.969\n",
      "850 - loss: 4416.967\n",
      "851 - loss: 4416.958\n",
      "852 - loss: 4416.943\n",
      "853 - loss: 4416.941\n",
      "854 - loss: 4416.941\n",
      "855 - loss: 4416.941\n",
      "856 - loss: 4416.941\n",
      "857 - loss: 4416.908\n",
      "858 - loss: 4416.869\n",
      "859 - loss: 4416.867\n",
      "860 - loss: 4416.866\n",
      "861 - loss: 4416.865\n",
      "862 - loss: 4416.865\n",
      "863 - loss: 4416.865\n",
      "864 - loss: 4416.724\n",
      "865 - loss: 4416.566\n",
      "866 - loss: 4416.555\n",
      "f_new: 4416.557 - f: 4416.555 - Backtracking...\n",
      "868 - loss: 4416.554\n",
      "869 - loss: 4416.554\n",
      "870 - loss: 4416.554\n",
      "871 - loss: 4416.552\n",
      "872 - loss: 4416.550\n",
      "873 - loss: 4416.540\n",
      "874 - loss: 4416.536\n",
      "875 - loss: 4416.536\n",
      "876 - loss: 4416.536\n",
      "877 - loss: 4416.536\n",
      "878 - loss: 4415.496\n",
      "879 - loss: 4415.033\n",
      "f_new: 4420.744 - f: 4415.033 - Backtracking...\n",
      "881 - loss: 4414.972\n",
      "882 - loss: 4414.933\n",
      "883 - loss: 4414.909\n",
      "884 - loss: 4414.745\n",
      "885 - loss: 4414.611\n",
      "f_new: 4414.659 - f: 4414.611 - Backtracking...\n",
      "887 - loss: 4414.592\n",
      "888 - loss: 4414.574\n",
      "889 - loss: 4414.569\n",
      "890 - loss: 4414.561\n",
      "891 - loss: 4414.383\n",
      "f_new: 4414.387 - f: 4414.383 - Backtracking...\n",
      "893 - loss: 4414.379\n",
      "f_new: 4414.385 - f: 4414.379 - Backtracking...\n",
      "895 - loss: 4414.379\n",
      "896 - loss: 4414.378\n",
      "897 - loss: 4414.378\n",
      "898 - loss: 4414.377\n",
      "899 - loss: 4414.375\n",
      "900 - loss: 4414.373\n",
      "901 - loss: 4414.373\n",
      "902 - loss: 4414.373\n",
      "903 - loss: 4414.373\n",
      "904 - loss: 4414.359\n",
      "905 - loss: 4414.345\n",
      "906 - loss: 4414.328\n",
      "907 - loss: 4414.327\n",
      "908 - loss: 4414.327\n",
      "909 - loss: 4414.327\n",
      "910 - loss: 4414.326\n",
      "911 - loss: 4414.324\n",
      "912 - loss: 4414.301\n",
      "913 - loss: 4414.275\n",
      "914 - loss: 4414.275\n",
      "915 - loss: 4414.275\n",
      "916 - loss: 4414.275\n",
      "917 - loss: 4414.268\n",
      "918 - loss: 4414.262\n",
      "919 - loss: 4414.260\n",
      "920 - loss: 4414.258\n",
      "921 - loss: 4414.256\n",
      "922 - loss: 4414.256\n",
      "923 - loss: 4414.256\n",
      "924 - loss: 4414.256\n",
      "925 - loss: 4414.037\n",
      "926 - loss: 4413.702\n",
      "f_new: 4413.762 - f: 4413.702 - Backtracking...\n",
      "928 - loss: 4413.701\n",
      "929 - loss: 4413.701\n",
      "930 - loss: 4413.701\n",
      "931 - loss: 4413.699\n",
      "932 - loss: 4413.696\n",
      "933 - loss: 4413.692\n",
      "934 - loss: 4413.692\n",
      "935 - loss: 4413.692\n",
      "936 - loss: 4413.692\n",
      "937 - loss: 4413.686\n",
      "938 - loss: 4413.679\n",
      "939 - loss: 4413.677\n",
      "940 - loss: 4413.676\n",
      "941 - loss: 4413.676\n",
      "942 - loss: 4413.675\n",
      "943 - loss: 4413.675\n",
      "944 - loss: 4413.657\n",
      "945 - loss: 4413.638\n",
      "946 - loss: 4413.633\n",
      "947 - loss: 4413.631\n",
      "948 - loss: 4413.631\n",
      "949 - loss: 4413.631\n",
      "950 - loss: 4413.631\n",
      "951 - loss: 4413.627\n",
      "952 - loss: 4413.622\n",
      "953 - loss: 4413.620\n",
      "954 - loss: 4413.618\n",
      "955 - loss: 4413.616\n",
      "956 - loss: 4413.615\n",
      "957 - loss: 4413.615\n",
      "958 - loss: 4413.615\n",
      "959 - loss: 4413.558\n",
      "960 - loss: 4413.493\n",
      "961 - loss: 4413.491\n",
      "962 - loss: 4413.491\n",
      "963 - loss: 4413.490\n",
      "964 - loss: 4413.490\n",
      "965 - loss: 4413.489\n",
      "966 - loss: 4413.487\n",
      "967 - loss: 4413.483\n",
      "968 - loss: 4413.475\n",
      "969 - loss: 4413.472\n",
      "970 - loss: 4413.472\n",
      "971 - loss: 4413.472\n",
      "972 - loss: 4413.472\n",
      "973 - loss: 4413.470\n",
      "974 - loss: 4413.468\n",
      "975 - loss: 4413.460\n",
      "976 - loss: 4413.416\n",
      "977 - loss: 4413.414\n",
      "978 - loss: 4413.414\n",
      "979 - loss: 4413.414\n",
      "980 - loss: 4413.414\n",
      "981 - loss: 4413.412\n",
      "982 - loss: 4413.410\n",
      "983 - loss: 4413.399\n",
      "984 - loss: 4413.258\n",
      "985 - loss: 4413.252\n",
      "f_new: 4413.252 - f: 4413.252 - Backtracking...\n",
      "987 - loss: 4413.252\n",
      "988 - loss: 4413.252\n",
      "989 - loss: 4413.251\n",
      "990 - loss: 4413.250\n",
      "991 - loss: 4413.249\n",
      "992 - loss: 4413.247\n",
      "993 - loss: 4413.247\n",
      "994 - loss: 4413.247\n",
      "995 - loss: 4413.247\n",
      "996 - loss: 4389.214\n",
      "f_new: 22935.718 - f: 4389.214 - Backtracking...\n",
      "f_new: 4389.214 - f: 4389.214 - Backtracking...\n",
      "999 - loss: 4389.213\n",
      "1000 - loss: 4389.213\n",
      "Reached maximum number of function evaluations 1000\n",
      "0.7283417326613871\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(verbose=True, max_evaluations=1000)\n",
    "logReg.fit(y_sub, np.c_[np.ones(X_sub.shape[0]), X_sub])\n",
    "pred = logReg.predict(np.c_[np.ones(X_sub.shape[0]), X_sub])\n",
    "print(compute_accuracy(pred, y_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_new: 388952141074152048493827070704946719003374518272.000 - f: 0.693 - Backtracking...\n",
      "f_new: 42948940344232256197308449618584789201601953792.000 - f: 0.693 - Backtracking...\n",
      "f_new: 4742515291465520316589241094131375336488697856.000 - f: 0.693 - Backtracking...\n",
      "f_new: 523678840723825601365942021904012656230006784.000 - f: 0.693 - Backtracking...\n",
      "f_new: 57825755188467729788181246775209813142929408.000 - f: 0.693 - Backtracking...\n",
      "f_new: 6385245503703762415557669590722242188148736.000 - f: 0.693 - Backtracking...\n",
      "f_new: 705072679287725974358492902368409655705600.000 - f: 0.693 - Backtracking...\n",
      "f_new: 77855656887368529251737837091182763573248.000 - f: 0.693 - Backtracking...\n",
      "f_new: 8596990760565376003751822352078876966912.000 - f: 0.693 - Backtracking...\n",
      "f_new: 949298395159228346089345013828158488576.000 - f: 0.693 - Backtracking...\n",
      "f_new: 104823590969245276918563331445266317312.000 - f: 0.693 - Backtracking...\n",
      "f_new: 11574848624751544723633017999918628864.000 - f: 0.693 - Backtracking...\n",
      "f_new: 1278119929369916136911791130408910848.000 - f: 0.693 - Backtracking...\n",
      "f_new: 141132779080955290663632729679069184.000 - f: 0.693 - Backtracking...\n",
      "f_new: 15584188051064248468229727084085248.000 - f: 0.693 - Backtracking...\n",
      "f_new: 1720839898374159823323440951066624.000 - f: 0.693 - Backtracking...\n",
      "f_new: 190018879785922593601210233651200.000 - f: 0.693 - Backtracking...\n",
      "f_new: 20982297486948518276197993414656.000 - f: 0.693 - Backtracking...\n",
      "f_new: 2316910868682124542541257768960.000 - f: 0.693 - Backtracking...\n",
      "f_new: 255838331181627255559634812928.000 - f: 0.693 - Backtracking...\n",
      "f_new: 28250224290686777644342575104.000 - f: 0.693 - Backtracking...\n",
      "f_new: 3119451134582065730238283776.000 - f: 0.693 - Backtracking...\n",
      "f_new: 344456570713080468250558464.000 - f: 0.693 - Backtracking...\n",
      "f_new: 38035642806538756811653120.000 - f: 0.693 - Backtracking...\n",
      "f_new: 4199978304120254887886848.000 - f: 0.693 - Backtracking...\n",
      "f_new: 463770728019571584794624.000 - f: 0.693 - Backtracking...\n",
      "f_new: 51210571244333019693056.000 - f: 0.693 - Backtracking...\n",
      "f_new: 5654782522324772978688.000 - f: 0.693 - Backtracking...\n",
      "f_new: 624413370087690076160.000 - f: 0.693 - Backtracking...\n",
      "f_new: 68949080748020621312.000 - f: 0.693 - Backtracking...\n",
      "f_new: 7613507275363817472.000 - f: 0.693 - Backtracking...\n",
      "f_new: 840700012286702080.000 - f: 0.693 - Backtracking...\n",
      "f_new: 92831921622493920.000 - f: 0.693 - Backtracking...\n",
      "f_new: 10250702445792234.000 - f: 0.693 - Backtracking...\n",
      "f_new: 1131904831825757.250 - f: 0.693 - Backtracking...\n",
      "f_new: 124987390384784.203 - f: 0.693 - Backtracking...\n",
      "f_new: 13801379158352.516 - f: 0.693 - Backtracking...\n",
      "f_new: 1523978267617.324 - f: 0.693 - Backtracking...\n",
      "f_new: 168280990872.210 - f: 0.693 - Backtracking...\n",
      "f_new: 18581952571.600 - f: 0.693 - Backtracking...\n",
      "f_new: 2051859568.882 - f: 0.693 - Backtracking...\n",
      "f_new: 226570790.942 - f: 0.693 - Backtracking...\n",
      "f_new: 25018438.979 - f: 0.693 - Backtracking...\n",
      "f_new: 2762590.440 - f: 0.693 - Backtracking...\n",
      "f_new: 305051.297 - f: 0.693 - Backtracking...\n",
      "f_new: 33684.487 - f: 0.693 - Backtracking...\n",
      "f_new: 3719.574 - f: 0.693 - Backtracking...\n",
      "f_new: 410.801 - f: 0.693 - Backtracking...\n",
      "f_new: 45.573 - f: 0.693 - Backtracking...\n",
      "f_new: 5.430 - f: 0.693 - Backtracking...\n",
      "f_new: 1.140 - f: 0.693 - Backtracking...\n",
      "f_new: 0.727 - f: 0.693 - Backtracking...\n",
      "f_new: 0.695 - f: 0.693 - Backtracking...\n",
      "54 - loss: 0.693\n",
      "55 - loss: 0.693\n",
      "56 - loss: 0.693\n",
      "57 - loss: 0.693\n",
      "58 - loss: 0.693\n",
      "59 - loss: 0.692\n",
      "f_new: 0.692 - f: 0.692 - Backtracking...\n",
      "61 - loss: 0.692\n",
      "62 - loss: 0.692\n",
      "63 - loss: 0.692\n",
      "64 - loss: 0.691\n",
      "65 - loss: 0.691\n",
      "f_new: 0.693 - f: 0.691 - Backtracking...\n",
      "67 - loss: 0.691\n",
      "68 - loss: 0.690\n",
      "69 - loss: 0.690\n",
      "70 - loss: 0.690\n",
      "f_new: 0.691 - f: 0.690 - Backtracking...\n",
      "72 - loss: 0.689\n",
      "73 - loss: 0.689\n",
      "74 - loss: 0.689\n",
      "75 - loss: 0.689\n",
      "76 - loss: 0.689\n",
      "77 - loss: 0.686\n",
      "f_new: 0.706 - f: 0.686 - Backtracking...\n",
      "79 - loss: 0.686\n",
      "80 - loss: 0.686\n",
      "81 - loss: 0.686\n",
      "82 - loss: 0.686\n",
      "83 - loss: 0.685\n",
      "f_new: 0.698 - f: 0.685 - Backtracking...\n",
      "85 - loss: 0.685\n",
      "86 - loss: 0.685\n",
      "87 - loss: 0.685\n",
      "88 - loss: 0.685\n",
      "89 - loss: 0.684\n",
      "f_new: 0.687 - f: 0.684 - Backtracking...\n",
      "91 - loss: 0.684\n",
      "92 - loss: 0.684\n",
      "93 - loss: 0.684\n",
      "94 - loss: 0.684\n",
      "95 - loss: 0.684\n",
      "96 - loss: 0.683\n",
      "f_new: 0.686 - f: 0.683 - Backtracking...\n",
      "98 - loss: 0.683\n",
      "99 - loss: 0.683\n",
      "100 - loss: 0.683\n",
      "101 - loss: 0.683\n",
      "102 - loss: 0.683\n",
      "103 - loss: 0.683\n",
      "104 - loss: 0.681\n",
      "f_new: 0.683 - f: 0.681 - Backtracking...\n",
      "106 - loss: 0.681\n",
      "107 - loss: 0.681\n",
      "108 - loss: 0.681\n",
      "109 - loss: 0.681\n",
      "110 - loss: 0.679\n",
      "f_new: 0.748 - f: 0.679 - Backtracking...\n",
      "112 - loss: 0.679\n",
      "113 - loss: 0.679\n",
      "114 - loss: 0.679\n",
      "115 - loss: 0.679\n",
      "116 - loss: 0.679\n",
      "f_new: 0.679 - f: 0.679 - Backtracking...\n",
      "118 - loss: 0.679\n",
      "119 - loss: 0.679\n",
      "120 - loss: 0.679\n",
      "121 - loss: 0.679\n",
      "122 - loss: 0.679\n",
      "123 - loss: 0.678\n",
      "f_new: 0.684 - f: 0.678 - Backtracking...\n",
      "125 - loss: 0.678\n",
      "126 - loss: 0.678\n",
      "127 - loss: 0.678\n",
      "128 - loss: 0.678\n",
      "129 - loss: 0.678\n",
      "130 - loss: 0.678\n",
      "f_new: 0.678 - f: 0.678 - Backtracking...\n",
      "132 - loss: 0.678\n",
      "133 - loss: 0.678\n",
      "134 - loss: 0.678\n",
      "135 - loss: 0.678\n",
      "136 - loss: 0.678\n",
      "137 - loss: 0.677\n",
      "f_new: 0.678 - f: 0.677 - Backtracking...\n",
      "139 - loss: 0.677\n",
      "140 - loss: 0.677\n",
      "141 - loss: 0.677\n",
      "142 - loss: 0.677\n",
      "143 - loss: 0.677\n",
      "f_new: 0.677 - f: 0.677 - Backtracking...\n",
      "145 - loss: 0.677\n",
      "146 - loss: 0.677\n",
      "147 - loss: 0.677\n",
      "148 - loss: 0.677\n",
      "149 - loss: 0.677\n",
      "150 - loss: 0.677\n",
      "f_new: 0.677 - f: 0.677 - Backtracking...\n",
      "152 - loss: 0.677\n",
      "153 - loss: 0.676\n",
      "154 - loss: 0.676\n",
      "155 - loss: 0.676\n",
      "156 - loss: 0.676\n",
      "157 - loss: 0.676\n",
      "158 - loss: 0.676\n",
      "159 - loss: 0.676\n",
      "160 - loss: 0.676\n",
      "161 - loss: 0.676\n",
      "162 - loss: 0.676\n",
      "163 - loss: 0.675\n",
      "f_new: 0.683 - f: 0.675 - Backtracking...\n",
      "165 - loss: 0.675\n",
      "166 - loss: 0.675\n",
      "167 - loss: 0.675\n",
      "168 - loss: 0.675\n",
      "169 - loss: 0.675\n",
      "170 - loss: 0.666\n",
      "f_new: 0.886 - f: 0.666 - Backtracking...\n",
      "f_new: 0.666 - f: 0.666 - Backtracking...\n",
      "173 - loss: 0.666\n",
      "174 - loss: 0.666\n",
      "175 - loss: 0.666\n",
      "176 - loss: 0.666\n",
      "177 - loss: 0.666\n",
      "f_new: 0.667 - f: 0.666 - Backtracking...\n",
      "179 - loss: 0.666\n",
      "180 - loss: 0.666\n",
      "181 - loss: 0.666\n",
      "182 - loss: 0.666\n",
      "f_new: 0.666 - f: 0.666 - Backtracking...\n",
      "184 - loss: 0.666\n",
      "185 - loss: 0.666\n",
      "186 - loss: 0.666\n",
      "187 - loss: 0.666\n",
      "188 - loss: 0.666\n",
      "189 - loss: 0.666\n",
      "190 - loss: 0.666\n",
      "191 - loss: 0.666\n",
      "192 - loss: 0.666\n",
      "193 - loss: 0.666\n",
      "194 - loss: 0.665\n",
      "195 - loss: 0.665\n",
      "196 - loss: 0.665\n",
      "197 - loss: 0.665\n",
      "198 - loss: 0.665\n",
      "199 - loss: 0.665\n",
      "200 - loss: 0.664\n",
      "f_new: 0.721 - f: 0.664 - Backtracking...\n",
      "202 - loss: 0.664\n",
      "203 - loss: 0.664\n",
      "204 - loss: 0.664\n",
      "205 - loss: 0.664\n",
      "206 - loss: 0.664\n",
      "207 - loss: 0.664\n",
      "208 - loss: 0.664\n",
      "209 - loss: 0.664\n",
      "210 - loss: 0.664\n",
      "211 - loss: 0.664\n",
      "212 - loss: 0.664\n",
      "213 - loss: 0.664\n",
      "214 - loss: 0.664\n",
      "215 - loss: 0.664\n",
      "216 - loss: 0.664\n",
      "217 - loss: 0.664\n",
      "218 - loss: 0.663\n",
      "f_new: 0.682 - f: 0.663 - Backtracking...\n",
      "220 - loss: 0.663\n",
      "221 - loss: 0.663\n",
      "222 - loss: 0.663\n",
      "223 - loss: 0.663\n",
      "224 - loss: 0.663\n",
      "f_new: 0.663 - f: 0.663 - Backtracking...\n",
      "226 - loss: 0.663\n",
      "227 - loss: 0.663\n",
      "228 - loss: 0.663\n",
      "229 - loss: 0.663\n",
      "230 - loss: 0.662\n",
      "f_new: 0.663 - f: 0.662 - Backtracking...\n",
      "232 - loss: 0.662\n",
      "233 - loss: 0.662\n",
      "234 - loss: 0.662\n",
      "235 - loss: 0.662\n",
      "236 - loss: 0.662\n",
      "237 - loss: 0.662\n",
      "f_new: 0.663 - f: 0.662 - Backtracking...\n",
      "239 - loss: 0.662\n",
      "240 - loss: 0.662\n",
      "241 - loss: 0.662\n",
      "242 - loss: 0.662\n",
      "243 - loss: 0.662\n",
      "244 - loss: 0.662\n",
      "245 - loss: 0.662\n",
      "246 - loss: 0.662\n",
      "247 - loss: 0.662\n",
      "248 - loss: 0.662\n",
      "249 - loss: 0.662\n",
      "250 - loss: 0.662\n",
      "251 - loss: 0.662\n",
      "252 - loss: 0.662\n",
      "253 - loss: 0.662\n",
      "254 - loss: 0.662\n",
      "255 - loss: 0.662\n",
      "256 - loss: 0.662\n",
      "f_new: 0.662 - f: 0.662 - Backtracking...\n",
      "258 - loss: 0.662\n",
      "259 - loss: 0.662\n",
      "260 - loss: 0.662\n",
      "261 - loss: 0.662\n",
      "262 - loss: 0.662\n",
      "263 - loss: 0.662\n",
      "264 - loss: 0.662\n",
      "265 - loss: 0.662\n",
      "266 - loss: 0.662\n",
      "267 - loss: 0.662\n",
      "268 - loss: 0.661\n",
      "f_new: 0.674 - f: 0.661 - Backtracking...\n",
      "270 - loss: 0.661\n",
      "271 - loss: 0.661\n",
      "272 - loss: 0.661\n",
      "273 - loss: 0.661\n",
      "274 - loss: 0.661\n",
      "275 - loss: 0.661\n",
      "276 - loss: 0.661\n",
      "277 - loss: 0.661\n",
      "278 - loss: 0.661\n",
      "279 - loss: 0.661\n",
      "280 - loss: 0.661\n",
      "281 - loss: 0.661\n",
      "282 - loss: 0.661\n",
      "f_new: 0.661 - f: 0.661 - Backtracking...\n",
      "284 - loss: 0.661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 - loss: 0.661\n",
      "286 - loss: 0.661\n",
      "287 - loss: 0.661\n",
      "288 - loss: 0.661\n",
      "f_new: 0.661 - f: 0.661 - Backtracking...\n",
      "290 - loss: 0.661\n",
      "291 - loss: 0.661\n",
      "292 - loss: 0.661\n",
      "293 - loss: 0.661\n",
      "294 - loss: 0.661\n",
      "f_new: 0.661 - f: 0.661 - Backtracking...\n",
      "296 - loss: 0.661\n",
      "297 - loss: 0.661\n",
      "298 - loss: 0.661\n",
      "299 - loss: 0.661\n",
      "300 - loss: 0.660\n",
      "301 - loss: 0.660\n",
      "302 - loss: 0.660\n",
      "303 - loss: 0.660\n",
      "304 - loss: 0.660\n",
      "305 - loss: 0.660\n",
      "306 - loss: 0.660\n",
      "307 - loss: 0.660\n",
      "308 - loss: 0.660\n",
      "309 - loss: 0.660\n",
      "310 - loss: 0.660\n",
      "311 - loss: 0.660\n",
      "312 - loss: 0.660\n",
      "313 - loss: 0.660\n",
      "314 - loss: 0.660\n",
      "315 - loss: 0.660\n",
      "316 - loss: 0.660\n",
      "317 - loss: 0.660\n",
      "318 - loss: 0.660\n",
      "319 - loss: 0.660\n",
      "320 - loss: 0.660\n",
      "321 - loss: 0.660\n",
      "322 - loss: 0.660\n",
      "323 - loss: 0.660\n",
      "324 - loss: 0.660\n",
      "f_new: 0.660 - f: 0.660 - Backtracking...\n",
      "326 - loss: 0.660\n",
      "327 - loss: 0.660\n",
      "328 - loss: 0.660\n",
      "329 - loss: 0.660\n",
      "330 - loss: 0.660\n",
      "331 - loss: 0.660\n",
      "332 - loss: 0.660\n",
      "333 - loss: 0.660\n",
      "334 - loss: 0.660\n",
      "335 - loss: 0.660\n",
      "336 - loss: 0.660\n",
      "f_new: 0.661 - f: 0.660 - Backtracking...\n",
      "338 - loss: 0.660\n",
      "339 - loss: 0.660\n",
      "340 - loss: 0.660\n",
      "341 - loss: 0.660\n",
      "342 - loss: 0.660\n",
      "343 - loss: 0.660\n",
      "344 - loss: 0.660\n",
      "f_new: 0.660 - f: 0.660 - Backtracking...\n",
      "346 - loss: 0.660\n",
      "347 - loss: 0.660\n",
      "348 - loss: 0.660\n",
      "349 - loss: 0.660\n",
      "350 - loss: 0.660\n",
      "351 - loss: 0.660\n",
      "352 - loss: 0.660\n",
      "353 - loss: 0.660\n",
      "354 - loss: 0.660\n",
      "355 - loss: 0.660\n",
      "356 - loss: 0.658\n",
      "f_new: 0.806 - f: 0.658 - Backtracking...\n",
      "f_new: 0.658 - f: 0.658 - Backtracking...\n",
      "359 - loss: 0.658\n",
      "360 - loss: 0.658\n",
      "361 - loss: 0.658\n",
      "362 - loss: 0.658\n",
      "363 - loss: 0.658\n",
      "f_new: 0.658 - f: 0.658 - Backtracking...\n",
      "365 - loss: 0.658\n",
      "366 - loss: 0.658\n",
      "367 - loss: 0.658\n",
      "368 - loss: 0.658\n",
      "369 - loss: 0.658\n",
      "370 - loss: 0.658\n",
      "371 - loss: 0.658\n",
      "372 - loss: 0.658\n",
      "373 - loss: 0.658\n",
      "374 - loss: 0.658\n",
      "375 - loss: 0.658\n",
      "376 - loss: 0.658\n",
      "377 - loss: 0.658\n",
      "378 - loss: 0.658\n",
      "379 - loss: 0.658\n",
      "380 - loss: 0.658\n",
      "381 - loss: 0.658\n",
      "382 - loss: 0.656\n",
      "f_new: 1.061 - f: 0.656 - Backtracking...\n",
      "f_new: 0.656 - f: 0.656 - Backtracking...\n",
      "385 - loss: 0.656\n",
      "386 - loss: 0.656\n",
      "387 - loss: 0.656\n",
      "388 - loss: 0.656\n",
      "389 - loss: 0.656\n",
      "390 - loss: 0.656\n",
      "391 - loss: 0.656\n",
      "392 - loss: 0.656\n",
      "393 - loss: 0.656\n",
      "394 - loss: 0.656\n",
      "395 - loss: 0.656\n",
      "396 - loss: 0.656\n",
      "397 - loss: 0.656\n",
      "398 - loss: 0.656\n",
      "399 - loss: 0.656\n",
      "400 - loss: 0.656\n",
      "401 - loss: 0.656\n",
      "402 - loss: 0.656\n",
      "403 - loss: 0.656\n",
      "404 - loss: 0.656\n",
      "405 - loss: 0.656\n",
      "406 - loss: 0.656\n",
      "f_new: 0.662 - f: 0.656 - Backtracking...\n",
      "408 - loss: 0.656\n",
      "409 - loss: 0.656\n",
      "410 - loss: 0.656\n",
      "411 - loss: 0.656\n",
      "412 - loss: 0.656\n",
      "413 - loss: 0.656\n",
      "f_new: 0.656 - f: 0.656 - Backtracking...\n",
      "415 - loss: 0.656\n",
      "416 - loss: 0.656\n",
      "417 - loss: 0.656\n",
      "418 - loss: 0.656\n",
      "419 - loss: 0.656\n",
      "420 - loss: 0.656\n",
      "421 - loss: 0.656\n",
      "422 - loss: 0.656\n",
      "423 - loss: 0.656\n",
      "424 - loss: 0.656\n",
      "425 - loss: 0.656\n",
      "426 - loss: 0.656\n",
      "427 - loss: 0.656\n",
      "428 - loss: 0.656\n",
      "429 - loss: 0.656\n",
      "430 - loss: 0.656\n",
      "431 - loss: 0.654\n",
      "f_new: 1.052 - f: 0.654 - Backtracking...\n",
      "f_new: 0.654 - f: 0.654 - Backtracking...\n",
      "434 - loss: 0.654\n",
      "435 - loss: 0.654\n",
      "436 - loss: 0.654\n",
      "437 - loss: 0.654\n",
      "438 - loss: 0.654\n",
      "439 - loss: 0.654\n",
      "440 - loss: 0.654\n",
      "441 - loss: 0.654\n",
      "442 - loss: 0.654\n",
      "443 - loss: 0.654\n",
      "444 - loss: 0.654\n",
      "445 - loss: 0.654\n",
      "446 - loss: 0.654\n",
      "447 - loss: 0.654\n",
      "448 - loss: 0.654\n",
      "449 - loss: 0.654\n",
      "450 - loss: 0.654\n",
      "451 - loss: 0.654\n",
      "452 - loss: 0.654\n",
      "453 - loss: 0.654\n",
      "454 - loss: 0.654\n",
      "455 - loss: 0.654\n",
      "456 - loss: 0.654\n",
      "f_new: 0.654 - f: 0.654 - Backtracking...\n",
      "458 - loss: 0.654\n",
      "459 - loss: 0.654\n",
      "460 - loss: 0.654\n",
      "461 - loss: 0.654\n",
      "462 - loss: 0.654\n",
      "463 - loss: 0.654\n",
      "f_new: 0.654 - f: 0.654 - Backtracking...\n",
      "465 - loss: 0.654\n",
      "466 - loss: 0.654\n",
      "467 - loss: 0.654\n",
      "468 - loss: 0.654\n",
      "469 - loss: 0.653\n",
      "470 - loss: 0.653\n",
      "471 - loss: 0.653\n",
      "472 - loss: 0.653\n",
      "473 - loss: 0.653\n",
      "474 - loss: 0.653\n",
      "475 - loss: 0.653\n",
      "476 - loss: 0.653\n",
      "477 - loss: 0.653\n",
      "478 - loss: 0.653\n",
      "479 - loss: 0.653\n",
      "480 - loss: 0.653\n",
      "481 - loss: 0.653\n",
      "482 - loss: 0.653\n",
      "483 - loss: 0.653\n",
      "484 - loss: 0.653\n",
      "485 - loss: 0.653\n",
      "486 - loss: 0.653\n",
      "487 - loss: 0.653\n",
      "488 - loss: 0.653\n",
      "489 - loss: 0.653\n",
      "490 - loss: 0.653\n",
      "491 - loss: 0.653\n",
      "492 - loss: 0.653\n",
      "493 - loss: 0.653\n",
      "494 - loss: 0.653\n",
      "495 - loss: 0.653\n",
      "496 - loss: 0.653\n",
      "497 - loss: 0.653\n",
      "498 - loss: 0.653\n",
      "499 - loss: 0.653\n",
      "500 - loss: 0.653\n",
      "501 - loss: 0.653\n",
      "502 - loss: 0.653\n",
      "503 - loss: 0.653\n",
      "504 - loss: 0.653\n",
      "505 - loss: 0.653\n",
      "506 - loss: 0.653\n",
      "507 - loss: 0.653\n",
      "508 - loss: 0.653\n",
      "509 - loss: 0.653\n",
      "510 - loss: 0.653\n",
      "511 - loss: 0.653\n",
      "512 - loss: 0.653\n",
      "513 - loss: 0.653\n",
      "514 - loss: 0.653\n",
      "515 - loss: 0.653\n",
      "516 - loss: 0.653\n",
      "517 - loss: 0.653\n",
      "518 - loss: 0.653\n",
      "519 - loss: 0.653\n",
      "520 - loss: 0.653\n",
      "521 - loss: 0.653\n",
      "522 - loss: 0.653\n",
      "523 - loss: 0.653\n",
      "f_new: 0.653 - f: 0.653 - Backtracking...\n",
      "525 - loss: 0.653\n",
      "526 - loss: 0.653\n",
      "527 - loss: 0.653\n",
      "528 - loss: 0.653\n",
      "529 - loss: 0.653\n",
      "530 - loss: 0.653\n",
      "531 - loss: 0.653\n",
      "532 - loss: 0.653\n",
      "533 - loss: 0.653\n",
      "534 - loss: 0.653\n",
      "535 - loss: 0.653\n",
      "536 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "538 - loss: 0.652\n",
      "539 - loss: 0.652\n",
      "540 - loss: 0.652\n",
      "541 - loss: 0.652\n",
      "542 - loss: 0.652\n",
      "543 - loss: 0.652\n",
      "544 - loss: 0.652\n",
      "545 - loss: 0.652\n",
      "546 - loss: 0.652\n",
      "547 - loss: 0.652\n",
      "548 - loss: 0.652\n",
      "549 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "551 - loss: 0.652\n",
      "552 - loss: 0.652\n",
      "553 - loss: 0.652\n",
      "554 - loss: 0.652\n",
      "555 - loss: 0.652\n",
      "556 - loss: 0.652\n",
      "557 - loss: 0.652\n",
      "558 - loss: 0.652\n",
      "559 - loss: 0.652\n",
      "560 - loss: 0.652\n",
      "561 - loss: 0.652\n",
      "562 - loss: 0.652\n",
      "563 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "565 - loss: 0.652\n",
      "566 - loss: 0.652\n",
      "567 - loss: 0.652\n",
      "568 - loss: 0.652\n",
      "569 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "571 - loss: 0.652\n",
      "572 - loss: 0.652\n",
      "573 - loss: 0.652\n",
      "574 - loss: 0.652\n",
      "575 - loss: 0.652\n",
      "576 - loss: 0.652\n",
      "577 - loss: 0.652\n",
      "578 - loss: 0.652\n",
      "579 - loss: 0.652\n",
      "580 - loss: 0.652\n",
      "581 - loss: 0.652\n",
      "582 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "584 - loss: 0.652\n",
      "585 - loss: 0.652\n",
      "586 - loss: 0.652\n",
      "587 - loss: 0.652\n",
      "588 - loss: 0.652\n",
      "589 - loss: 0.652\n",
      "590 - loss: 0.652\n",
      "591 - loss: 0.652\n",
      "592 - loss: 0.652\n",
      "593 - loss: 0.652\n",
      "594 - loss: 0.651\n",
      "f_new: 0.672 - f: 0.651 - Backtracking...\n",
      "596 - loss: 0.651\n",
      "597 - loss: 0.651\n",
      "598 - loss: 0.651\n",
      "599 - loss: 0.651\n",
      "600 - loss: 0.651\n",
      "601 - loss: 0.651\n",
      "f_new: 0.651 - f: 0.651 - Backtracking...\n",
      "603 - loss: 0.651\n",
      "604 - loss: 0.651\n",
      "605 - loss: 0.651\n",
      "606 - loss: 0.651\n",
      "607 - loss: 0.651\n",
      "608 - loss: 0.651\n",
      "f_new: 0.651 - f: 0.651 - Backtracking...\n",
      "610 - loss: 0.651\n",
      "611 - loss: 0.651\n",
      "612 - loss: 0.651\n",
      "613 - loss: 0.651\n",
      "614 - loss: 0.651\n",
      "f_new: 0.651 - f: 0.651 - Backtracking...\n",
      "616 - loss: 0.651\n",
      "617 - loss: 0.651\n",
      "618 - loss: 0.651\n",
      "619 - loss: 0.651\n",
      "620 - loss: 0.651\n",
      "621 - loss: 0.651\n",
      "622 - loss: 0.651\n",
      "623 - loss: 0.651\n",
      "624 - loss: 0.651\n",
      "625 - loss: 0.651\n",
      "626 - loss: 0.651\n",
      "627 - loss: 0.651\n",
      "628 - loss: 0.651\n",
      "629 - loss: 0.651\n",
      "630 - loss: 0.651\n",
      "631 - loss: 0.651\n",
      "632 - loss: 0.651\n",
      "633 - loss: 0.651\n",
      "634 - loss: 0.651\n",
      "635 - loss: 0.651\n",
      "636 - loss: 0.651\n",
      "637 - loss: 0.650\n",
      "f_new: 0.738 - f: 0.650 - Backtracking...\n",
      "f_new: 0.650 - f: 0.650 - Backtracking...\n",
      "640 - loss: 0.650\n",
      "641 - loss: 0.650\n",
      "642 - loss: 0.650\n",
      "643 - loss: 0.650\n",
      "644 - loss: 0.650\n",
      "645 - loss: 0.650\n",
      "646 - loss: 0.650\n",
      "647 - loss: 0.650\n",
      "648 - loss: 0.650\n",
      "649 - loss: 0.650\n",
      "650 - loss: 0.650\n",
      "f_new: 0.650 - f: 0.650 - Backtracking...\n",
      "652 - loss: 0.650\n",
      "653 - loss: 0.650\n",
      "654 - loss: 0.650\n",
      "655 - loss: 0.650\n",
      "656 - loss: 0.650\n",
      "657 - loss: 0.650\n",
      "658 - loss: 0.650\n",
      "f_new: 0.650 - f: 0.650 - Backtracking...\n",
      "660 - loss: 0.650\n",
      "661 - loss: 0.650\n",
      "662 - loss: 0.650\n",
      "663 - loss: 0.650\n",
      "664 - loss: 0.650\n",
      "665 - loss: 0.650\n",
      "666 - loss: 0.650\n",
      "667 - loss: 0.650\n",
      "668 - loss: 0.650\n",
      "669 - loss: 0.650\n",
      "670 - loss: 0.649\n",
      "f_new: 0.651 - f: 0.649 - Backtracking...\n",
      "672 - loss: 0.649\n",
      "673 - loss: 0.649\n",
      "674 - loss: 0.649\n",
      "675 - loss: 0.649\n",
      "676 - loss: 0.649\n",
      "677 - loss: 0.649\n",
      "f_new: 0.649 - f: 0.649 - Backtracking...\n",
      "679 - loss: 0.649\n",
      "680 - loss: 0.649\n",
      "681 - loss: 0.649\n",
      "682 - loss: 0.649\n",
      "683 - loss: 0.649\n",
      "684 - loss: 0.649\n",
      "685 - loss: 0.649\n",
      "686 - loss: 0.649\n",
      "687 - loss: 0.649\n",
      "688 - loss: 0.649\n",
      "689 - loss: 0.649\n",
      "690 - loss: 0.649\n",
      "691 - loss: 0.649\n",
      "692 - loss: 0.649\n",
      "693 - loss: 0.649\n",
      "694 - loss: 0.649\n",
      "695 - loss: 0.649\n",
      "f_new: 0.656 - f: 0.649 - Backtracking...\n",
      "697 - loss: 0.649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698 - loss: 0.649\n",
      "699 - loss: 0.649\n",
      "700 - loss: 0.649\n",
      "701 - loss: 0.649\n",
      "f_new: 0.649 - f: 0.649 - Backtracking...\n",
      "703 - loss: 0.649\n",
      "704 - loss: 0.649\n",
      "705 - loss: 0.649\n",
      "706 - loss: 0.649\n",
      "707 - loss: 0.649\n",
      "708 - loss: 0.649\n",
      "709 - loss: 0.649\n",
      "710 - loss: 0.649\n",
      "711 - loss: 0.649\n",
      "712 - loss: 0.649\n",
      "713 - loss: 0.649\n",
      "714 - loss: 0.649\n",
      "715 - loss: 0.649\n",
      "716 - loss: 0.649\n",
      "717 - loss: 0.649\n",
      "718 - loss: 0.649\n",
      "719 - loss: 0.649\n",
      "f_new: 0.649 - f: 0.649 - Backtracking...\n",
      "721 - loss: 0.649\n",
      "722 - loss: 0.649\n",
      "723 - loss: 0.649\n",
      "724 - loss: 0.649\n",
      "725 - loss: 0.649\n",
      "726 - loss: 0.649\n",
      "727 - loss: 0.649\n",
      "728 - loss: 0.649\n",
      "729 - loss: 0.649\n",
      "730 - loss: 0.649\n",
      "731 - loss: 0.649\n",
      "732 - loss: 0.647\n",
      "f_new: 0.663 - f: 0.647 - Backtracking...\n",
      "734 - loss: 0.647\n",
      "735 - loss: 0.647\n",
      "736 - loss: 0.647\n",
      "737 - loss: 0.647\n",
      "738 - loss: 0.647\n",
      "739 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "741 - loss: 0.647\n",
      "742 - loss: 0.647\n",
      "743 - loss: 0.647\n",
      "744 - loss: 0.647\n",
      "745 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "747 - loss: 0.647\n",
      "748 - loss: 0.647\n",
      "749 - loss: 0.647\n",
      "750 - loss: 0.647\n",
      "751 - loss: 0.647\n",
      "752 - loss: 0.647\n",
      "753 - loss: 0.647\n",
      "754 - loss: 0.647\n",
      "755 - loss: 0.647\n",
      "756 - loss: 0.647\n",
      "757 - loss: 0.647\n",
      "758 - loss: 0.647\n",
      "759 - loss: 0.647\n",
      "760 - loss: 0.647\n",
      "761 - loss: 0.647\n",
      "762 - loss: 0.647\n",
      "763 - loss: 0.647\n",
      "764 - loss: 0.647\n",
      "765 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "767 - loss: 0.647\n",
      "768 - loss: 0.647\n",
      "769 - loss: 0.647\n",
      "770 - loss: 0.647\n",
      "771 - loss: 0.647\n",
      "772 - loss: 0.647\n",
      "773 - loss: 0.647\n",
      "774 - loss: 0.647\n",
      "775 - loss: 0.647\n",
      "776 - loss: 0.647\n",
      "777 - loss: 0.647\n",
      "778 - loss: 0.647\n",
      "779 - loss: 0.647\n",
      "780 - loss: 0.647\n",
      "781 - loss: 0.647\n",
      "782 - loss: 0.647\n",
      "783 - loss: 0.647\n",
      "784 - loss: 0.647\n",
      "785 - loss: 0.647\n",
      "786 - loss: 0.647\n",
      "787 - loss: 0.647\n",
      "788 - loss: 0.647\n",
      "789 - loss: 0.647\n",
      "790 - loss: 0.647\n",
      "791 - loss: 0.647\n",
      "792 - loss: 0.647\n",
      "793 - loss: 0.647\n",
      "794 - loss: 0.647\n",
      "795 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "797 - loss: 0.647\n",
      "798 - loss: 0.647\n",
      "799 - loss: 0.647\n",
      "800 - loss: 0.647\n",
      "801 - loss: 0.647\n",
      "802 - loss: 0.647\n",
      "803 - loss: 0.647\n",
      "804 - loss: 0.647\n",
      "805 - loss: 0.647\n",
      "806 - loss: 0.647\n",
      "807 - loss: 0.647\n",
      "808 - loss: 0.647\n",
      "809 - loss: 0.647\n",
      "810 - loss: 0.647\n",
      "811 - loss: 0.647\n",
      "812 - loss: 0.647\n",
      "813 - loss: 0.647\n",
      "814 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "816 - loss: 0.647\n",
      "817 - loss: 0.647\n",
      "818 - loss: 0.647\n",
      "819 - loss: 0.647\n",
      "820 - loss: 0.647\n",
      "821 - loss: 0.647\n",
      "822 - loss: 0.647\n",
      "823 - loss: 0.647\n",
      "824 - loss: 0.647\n",
      "825 - loss: 0.647\n",
      "826 - loss: 0.647\n",
      "827 - loss: 0.647\n",
      "828 - loss: 0.647\n",
      "829 - loss: 0.647\n",
      "830 - loss: 0.647\n",
      "831 - loss: 0.647\n",
      "832 - loss: 0.646\n",
      "f_new: 0.647 - f: 0.646 - Backtracking...\n",
      "834 - loss: 0.646\n",
      "835 - loss: 0.646\n",
      "836 - loss: 0.646\n",
      "837 - loss: 0.646\n",
      "838 - loss: 0.646\n",
      "839 - loss: 0.646\n",
      "840 - loss: 0.646\n",
      "841 - loss: 0.646\n",
      "842 - loss: 0.646\n",
      "843 - loss: 0.646\n",
      "844 - loss: 0.646\n",
      "f_new: 0.646 - f: 0.646 - Backtracking...\n",
      "846 - loss: 0.646\n",
      "847 - loss: 0.646\n",
      "848 - loss: 0.646\n",
      "849 - loss: 0.646\n",
      "850 - loss: 0.646\n",
      "851 - loss: 0.646\n",
      "852 - loss: 0.646\n",
      "853 - loss: 0.646\n",
      "854 - loss: 0.646\n",
      "855 - loss: 0.646\n",
      "856 - loss: 0.646\n",
      "857 - loss: 0.646\n",
      "858 - loss: 0.646\n",
      "859 - loss: 0.646\n",
      "860 - loss: 0.646\n",
      "861 - loss: 0.646\n",
      "862 - loss: 0.646\n",
      "f_new: 0.646 - f: 0.646 - Backtracking...\n",
      "864 - loss: 0.646\n",
      "865 - loss: 0.646\n",
      "866 - loss: 0.646\n",
      "867 - loss: 0.646\n",
      "868 - loss: 0.646\n",
      "869 - loss: 0.646\n",
      "870 - loss: 0.646\n",
      "871 - loss: 0.646\n",
      "872 - loss: 0.646\n",
      "873 - loss: 0.646\n",
      "874 - loss: 0.646\n",
      "875 - loss: 0.646\n",
      "876 - loss: 0.646\n",
      "877 - loss: 0.646\n",
      "878 - loss: 0.646\n",
      "879 - loss: 0.646\n",
      "880 - loss: 0.646\n",
      "881 - loss: 0.646\n",
      "882 - loss: 0.646\n",
      "883 - loss: 0.646\n",
      "884 - loss: 0.646\n",
      "885 - loss: 0.646\n",
      "886 - loss: 0.646\n",
      "887 - loss: 0.646\n",
      "888 - loss: 0.646\n",
      "889 - loss: 0.646\n",
      "890 - loss: 0.646\n",
      "891 - loss: 0.646\n",
      "892 - loss: 0.646\n",
      "893 - loss: 0.646\n",
      "894 - loss: 0.646\n",
      "895 - loss: 0.646\n",
      "896 - loss: 0.646\n",
      "897 - loss: 0.646\n",
      "898 - loss: 0.646\n",
      "899 - loss: 0.646\n",
      "900 - loss: 0.646\n",
      "901 - loss: 0.646\n",
      "902 - loss: 0.646\n",
      "903 - loss: 0.646\n",
      "904 - loss: 0.646\n",
      "905 - loss: 0.646\n",
      "906 - loss: 0.646\n",
      "907 - loss: 0.646\n",
      "908 - loss: 0.646\n",
      "909 - loss: 0.646\n",
      "910 - loss: 0.646\n",
      "911 - loss: 0.646\n",
      "912 - loss: 0.646\n",
      "913 - loss: 0.646\n",
      "914 - loss: 0.646\n",
      "915 - loss: 0.646\n",
      "916 - loss: 0.646\n",
      "917 - loss: 0.646\n",
      "918 - loss: 0.646\n",
      "919 - loss: 0.646\n",
      "920 - loss: 0.646\n",
      "921 - loss: 0.646\n",
      "922 - loss: 0.646\n",
      "923 - loss: 0.646\n",
      "924 - loss: 0.646\n",
      "925 - loss: 0.646\n",
      "926 - loss: 0.646\n",
      "927 - loss: 0.646\n",
      "928 - loss: 0.646\n",
      "f_new: 0.674 - f: 0.646 - Backtracking...\n",
      "f_new: 0.646 - f: 0.646 - Backtracking...\n",
      "931 - loss: 0.646\n",
      "932 - loss: 0.646\n",
      "933 - loss: 0.646\n",
      "934 - loss: 0.646\n",
      "935 - loss: 0.646\n",
      "936 - loss: 0.646\n",
      "937 - loss: 0.646\n",
      "938 - loss: 0.646\n",
      "939 - loss: 0.646\n",
      "940 - loss: 0.646\n",
      "941 - loss: 0.646\n",
      "942 - loss: 0.646\n",
      "943 - loss: 0.646\n",
      "944 - loss: 0.646\n",
      "945 - loss: 0.646\n",
      "946 - loss: 0.646\n",
      "947 - loss: 0.646\n",
      "948 - loss: 0.646\n",
      "949 - loss: 0.646\n",
      "950 - loss: 0.646\n",
      "951 - loss: 0.646\n",
      "952 - loss: 0.646\n",
      "953 - loss: 0.643\n",
      "f_new: 0.748 - f: 0.643 - Backtracking...\n",
      "f_new: 0.643 - f: 0.643 - Backtracking...\n",
      "956 - loss: 0.643\n",
      "957 - loss: 0.643\n",
      "958 - loss: 0.643\n",
      "959 - loss: 0.643\n",
      "960 - loss: 0.642\n",
      "f_new: 0.649 - f: 0.642 - Backtracking...\n",
      "962 - loss: 0.642\n",
      "963 - loss: 0.642\n",
      "964 - loss: 0.642\n",
      "965 - loss: 0.642\n",
      "966 - loss: 0.641\n",
      "f_new: 0.645 - f: 0.641 - Backtracking...\n",
      "968 - loss: 0.641\n",
      "969 - loss: 0.641\n",
      "970 - loss: 0.641\n",
      "971 - loss: 0.641\n",
      "f_new: 0.642 - f: 0.641 - Backtracking...\n",
      "973 - loss: 0.641\n",
      "974 - loss: 0.641\n",
      "975 - loss: 0.641\n",
      "976 - loss: 0.641\n",
      "977 - loss: 0.641\n",
      "f_new: 0.641 - f: 0.641 - Backtracking...\n",
      "979 - loss: 0.641\n",
      "980 - loss: 0.641\n",
      "981 - loss: 0.641\n",
      "982 - loss: 0.641\n",
      "983 - loss: 0.641\n",
      "984 - loss: 0.641\n",
      "985 - loss: 0.641\n",
      "986 - loss: 0.641\n",
      "987 - loss: 0.641\n",
      "988 - loss: 0.641\n",
      "989 - loss: 0.641\n",
      "f_new: 0.641 - f: 0.641 - Backtracking...\n",
      "991 - loss: 0.641\n",
      "992 - loss: 0.641\n",
      "993 - loss: 0.641\n",
      "994 - loss: 0.641\n",
      "995 - loss: 0.641\n",
      "996 - loss: 0.641\n",
      "997 - loss: 0.641\n",
      "998 - loss: 0.641\n",
      "999 - loss: 0.641\n",
      "1000 - loss: 0.641\n",
      "Reached maximum number of function evaluations 1000\n",
      "0.6813054955603551\n"
     ]
    }
   ],
   "source": [
    "#kernel_logreg_poly = LeastSquaresGDKernel(Kernel.kernel_poly, verbose=True, max_evals=1000)\n",
    "#kernel_logreg_poly = LogisticRegressionKernel(Kernel.kernel_poly, verbose=True, max_evals=1000)\n",
    "kernel_logreg_poly = LogisticRegressionKernel(Kernel.kernel_poly, verbose=True, max_evals=1000)\n",
    "p = 4\n",
    "preds = kernel_logreg_poly.predict(y_sub, X_sub, X_sub, p)\n",
    "print(compute_accuracy(preds, y_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_clean = remove_NaN_features(tX, 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegression()\n",
    "logReg.fit(y, tX_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 1289883598.7\n",
      "Iteration 1, loss = 929278938.1\n",
      "Iteration 2, loss = 843606874.7\n",
      "Iteration 3, loss = 832528308.0\n",
      "Iteration 4, loss = 831415267.7\n",
      "Iteration 5, loss = 831306045.8\n",
      "Iteration 6, loss = 831295253.7\n",
      "Iteration 7, loss = 831294167.6\n",
      "Iteration 8, loss = 831294054.6\n",
      "Iteration 9, loss = 831294042.1\n"
     ]
    }
   ],
   "source": [
    "model = AlternativePCA(k=2)\n",
    "model.fit(X)\n",
    "Z_pca = model.compress(X)\n",
    "Xhat_pca = model.expand(Z_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 1521186763.4\n",
      "Iteration 1, loss = 947138369.7\n",
      "Iteration 2, loss = 854324275.1\n",
      "Iteration 3, loss = 835473177.5\n",
      "Iteration 4, loss = 832070513.0\n",
      "Iteration 5, loss = 831441985.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antho\\Google Drive\\EPFL\\MA1\\CS-433 Machine Learning\\ML_course\\projects\\project1\\scripts\\solver.py:65: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  y = g_new - g\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 831322662.2\n",
      "Iteration 7, loss = 831299623.4\n",
      "Iteration 8, loss = 831295133.9\n",
      "Iteration 9, loss = 831294255.0\n",
      "Iteration 0, loss = 114368921.2\n",
      "Iteration 1, loss = 114357028.3\n",
      "Iteration 2, loss = 114357025.0\n",
      "Iteration 3, loss = 114357025.0\n",
      "Iteration 4, loss = 114357025.0\n",
      "Iteration 5, loss = 114357025.0\n",
      "Iteration 6, loss = 114357025.0\n",
      "Iteration 7, loss = 114357025.0\n",
      "Iteration 8, loss = 114357025.0\n",
      "Iteration 9, loss = 114357025.0\n"
     ]
    }
   ],
   "source": [
    "model = RobustPCA(k=2)\n",
    "model.fit(X)\n",
    "Z_robust = model.compress(X)\n",
    "Xhat_robust = model.expand(Z_robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.72588088, -7.31380373],\n",
       "       [ 3.89450982, 10.60756305],\n",
       "       [ 7.77930283, -1.90683368],\n",
       "       ...,\n",
       "       [-2.54520707,  6.4204025 ],\n",
       "       [-8.07558037, 21.54278193],\n",
       "       [-4.59888286, 21.69639677]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "#Need to rescale to do PCA\n",
    "def pca_axes(Z,scale,scale2, verbose=False):\n",
    "    \n",
    "    pcs = \n",
    "    colors = np.arctan2(pcs[0,:], pcs[1,:])\n",
    "    colormap = cm.inferno\n",
    "    norm = Normalize()\n",
    "    norm.autoscale(colors)\n",
    "    plt.rcParams['image.cmap'] = 'Paired'\n",
    "\n",
    "    # Quiver\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    plt.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),\n",
    "           pcs[0,:], pcs[1,:], color=colormap(norm(colors)),\n",
    "           angles='xy', scale_units='xy', scale=scale)\n",
    "    o = 0\n",
    "    for i in range(0,pcs.shape[1]):\n",
    "        #plt.arrow(0, 0, pcs[0,i], pcs[1,i],color='k') \n",
    "        if i==3:\n",
    "            plt.text(pcs[0,i]* scale2, pcs[1,i]*scale2-0.03, X.columns[i], color = 'k', \n",
    "             ha = 'center', va = 'center', fontsize=10, weight='bold')\n",
    "        elif i==12:\n",
    "            plt.text(pcs[0,i]* scale2, pcs[1,i]*scale2-0.03, X.columns[i], color = 'k', \n",
    "             ha = 'center', va = 'center', fontsize=10, weight='bold')\n",
    "        elif np.power(pcs[0,i]-pcs[1,i], 2)<0.001:\n",
    "            plt.text(0, 0-o, ' ', color = 'k', \n",
    "             ha = 'center', va = 'center', fontsize=12, weight='bold')\n",
    "        else:\n",
    "            plt.text(pcs[0,i]* scale2, pcs[1,i] *scale2, X.columns[i], color = 'k', \n",
    "             ha = 'center', va = 'center', fontsize=10, weight='bold')\n",
    "        o=o+0.04\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.xlim([-1.0,1.0])\n",
    "    plt.ylim([-0.5,1.0])\n",
    "    plt.xlabel('Principal component 0')\n",
    "    plt.ylabel('Principal component 1')\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    " \n",
    "    if verbose==True:\n",
    "        for i in range (0,pcs.shape[1]):\n",
    "            print(str(i), df.columns[i+2])\n",
    "        \n",
    "    return\n",
    "\n",
    "pca_axes(Z_pca, 0.65, 1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-524e3a5e580a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel_logreg_poly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\EPFL\\MA1\\CS-433 Machine Learning\\ML_course\\projects\\project1\\scripts\\classifiers.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, y, X, Xtest, lambda_, *args)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mKtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\EPFL\\MA1\\CS-433 Machine Learning\\ML_course\\projects\\project1\\scripts\\classifiers.py\u001b[0m in \u001b[0;36mkernel_RBF\u001b[1;34m(X1, X2, sigma)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mK\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[1;32m-> 2076\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   2077\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test kernel\n",
    "kernel_logreg_poly = LeastSquaresKernel(Kernel.kernel_RBF, verbose=True, max_evals=1000)\n",
    "p = 4\n",
    "sigma = 1\n",
    "accuracies = []\n",
    "for i in range(5):\n",
    "    print(i,  end=' - ')\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    X_sub = (X[shuffle_indices,:])[::100]\n",
    "    y_sub = (y[shuffle_indices])[::100]\n",
    "    \n",
    "    X_pred = (X[shuffle_indices,:])[::100]\n",
    "    y_pred = (y[shuffle_indices])[::100]\n",
    "    \n",
    "    preds = kernel_logreg_poly.predict(y_sub, X_sub, X_pred, sigma, lambda_=1)\n",
    "    accuracy = compute_accuracy(preds, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8757090909090909"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mse, loss_mse = least_squares(y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_mse = compute_accuracy(predict_labels(w_mse, X), y)\n",
    "print(accuracy_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_classifier(lambda_):\n",
    "    return ridge_regression(y, X, lambda_)\n",
    "\n",
    "lambda_ridge, _, _ = find_max_hyperparam(ridge_classifier, [10**c for c in range(-3,3)])\n",
    "print(\"Optimal lambda: %f\" % lambda_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_train(y_train, X_train):\n",
    "    return ridge_regression(y, X, lambda_ridge)\n",
    "\n",
    "def ridge_test(X_test, w):\n",
    "    return np.sign(X_test@w)\n",
    "\n",
    "accuracy_ridge = cross_validate(y, X, ridge_train, ridge_test, 0.8, 100)\n",
    "print(accuracy_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_train(y_train, X_train):\n",
    "    return logistic_regression(y_train, X_train, 0.01*np.ones(X_train.shape[1]), 1000, verbose=False)\n",
    "\n",
    "def log_reg_test(X_test, w):\n",
    "    return np.sign(X_test@w)\n",
    "\n",
    "accuracy_log_reg = cross_validate(y, X_safe, log_reg_train, log_reg_test, 0.7, 20)\n",
    "print(accuracy_log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_log_reg_classifier(lambda_):\n",
    "    return reg_logistic_regression(y, X_safe, lambda_, np.zeros(X_safe.shape[1]), 1000)\n",
    "\n",
    "def log_reg_sparse_classifier(lambda_):\n",
    "    return logistic_regression_sparse(y, X_safe, lambda_, np.zeros(X_safe.shape[1]), 1000)\n",
    "\n",
    "def mse_sparse_classifier(lambda_):\n",
    "    return least_squares_sparse(y, X_safe, lambda_, np.zeros(X_safe.shape[1]), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_log_reg_l2, _, _ = find_max_hyperparam(reg_log_reg_classifier, [10**c for c in range(-3,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_log_reg_l1, _, _ = find_max_hyperparam(log_reg_sparse_classifier, [10**c for c in range(-3,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_mse_l2, _, _ = find_max_hyperparam(ridge_classifier, [10**c for c in range(-6,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_mse_l1, w_mse_l1, _ = find_max_hyperparam(mse_sparse_classifier, [10**c for c in range(-6,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mse_l1, _ = least_squares_sparse(y, X_safe, 0.01, np.zeros(X_safe.shape[1]), 1000)\n",
    "print(\"Non-zero weights: %i / %i\" % (np.sum(w_mse_l1 != 0), len(w_mse_l1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1\n",
    "ypred_kernel = kernel_predict(kernel_poly, y, X_safe, X_safe, p, lambda_=1)\n",
    "print(compute_accuracy(ypred_kernel, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test, _ = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split_indices, X_test_split, _ = split_data(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8075190325180646\n"
     ]
    }
   ],
   "source": [
    "# predictions using new model\n",
    "X_test_split_poly = [ model_split_data(X) for X in X_test_split ]\n",
    "y_pred = np.ones(tX_test.shape[0])\n",
    "\n",
    "for model, X, indices in zip(models, X_test_split_poly, test_split_indices):\n",
    "    y_pred[indices] = model.predict(X)\n",
    "    \n",
    "# print(np.mean(y_test == y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/predictions.csv'\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/predictions.csv'\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
