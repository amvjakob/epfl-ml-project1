{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from helpers import *\n",
    "from classifiers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt', 'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality', 'DER_lep_eta_centrality', 'PRI_tau_pt', 'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi', 'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt']\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids, features = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "undef_feature_for = {\n",
    "    'DER_deltaeta_jet_jet'   : [0, 1],\n",
    "    'DER_mass_jet_jet'       : [0, 1],\n",
    "    'DER_prodeta_jet_jet'    : [0, 1],\n",
    "    'DER_lep_eta_centrality' : [0, 1],\n",
    "    'PRI_jet_num'            : [0, 1, 2, 3],\n",
    "    'PRI_jet_leading_pt'     : [0],\n",
    "    'PRI_jet_leading_eta'    : [0],\n",
    "    'PRI_jet_leading_phi'    : [0],\n",
    "    'PRI_jet_subleading_pt'  : [0, 1],\n",
    "    'PRI_jet_subleading_eta' : [0, 1],\n",
    "    'PRI_jet_subleading_pt'  : [0, 1]\n",
    "}\n",
    "\n",
    "jet_num_feature = \"PRI_jet_num\"\n",
    "\n",
    "features_split = []\n",
    "for jet in np.arange(4):\n",
    "    valid_features = [ f for f in features if not ((f in undef_feature_for) and (jet in undef_feature_for[f])) ]\n",
    "    features_split.append(valid_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y=None):\n",
    "    # features\n",
    "    undef_feature_for = {\n",
    "        'DER_deltaeta_jet_jet'   : [0, 1],\n",
    "        'DER_mass_jet_jet'       : [0, 1],\n",
    "        'DER_prodeta_jet_jet'    : [0, 1],\n",
    "        'DER_lep_eta_centrality' : [0, 1],\n",
    "        'PRI_jet_num'            : [0, 1, 2, 3],\n",
    "        'PRI_jet_leading_pt'     : [0],\n",
    "        'PRI_jet_leading_eta'    : [0],\n",
    "        'PRI_jet_leading_phi'    : [0],\n",
    "        'PRI_jet_subleading_pt'  : [0, 1],\n",
    "        'PRI_jet_subleading_eta' : [0, 1],\n",
    "        'PRI_jet_subleading_phi' : [0, 1],\n",
    "        'PRI_jet_all_pt'         : [0]\n",
    "    }\n",
    "\n",
    "    jet_num_feature = \"PRI_jet_num\"\n",
    "    jet_levels = 4\n",
    "\n",
    "    features_split = []\n",
    "    for jet in np.arange(jet_levels):\n",
    "        valid_features = [ f for f in features if not ((f in undef_feature_for) and (jet in undef_feature_for[f])) ]\n",
    "        features_split.append(valid_features)\n",
    "        \n",
    "    # split data based on jet level (vertical split)\n",
    "    split_indices = [\n",
    "        X[:,features.index(jet_num_feature)] == i for i in np.arange(jet_levels)\n",
    "    ]\n",
    "    X_split = [\n",
    "        X[X[:,features.index(jet_num_feature)] == i,:] for i in np.arange(jet_levels)\n",
    "    ]\n",
    "    if y is None:\n",
    "        y_split = None\n",
    "    else:\n",
    "        y_split = [\n",
    "            y[X[:,features.index(jet_num_feature)] == i] for i in np.arange(jet_levels)\n",
    "        ]\n",
    "\n",
    "    # only keep relevant features (horizontal split)\n",
    "    for i, X_ in enumerate(X_split):\n",
    "        indices = [ features.index(feature) for feature in features_split[i] ]\n",
    "        indices_bool = [ e in indices for e in range(len(features)) ]\n",
    "        X_split[i] = X_[:,indices_bool]\n",
    "        \n",
    "    return split_indices, X_split, y_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly_no_interaction(X, degree):\n",
    "    result = X.copy()\n",
    "    for d in range(2, degree+1):\n",
    "        # faster than np.power()\n",
    "        power = X.copy()\n",
    "        for i in range(d - 1):\n",
    "            power = power * X\n",
    "            \n",
    "        result = np.hstack((result, power))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def build_X(X, d):\n",
    "    X_ = remove_NaN_features(X, 0.2)\n",
    "    X_ = X_[:,:-1]\n",
    "    X_, mean_, std_ = standardize(X_)\n",
    "    #X_ = np.c_[np.sqrt(np.abs(X_)), np.abs(X_) * np.sqrt(np.abs(X_)), build_poly_no_interaction(X_, d)]\n",
    "    X_ = np.c_[np.sqrt(np.abs(X_)), build_poly_no_interaction(X_, d)]\n",
    "    #X_ = build_poly_no_interaction(X_, d)\n",
    "    \n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_split, y_split = split_data(tX, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - 0.7987271005816117\n",
      "2 - 0.8024315479697883\n",
      "3 - 0.8039957465850148\n",
      "4 - 0.8058079209809822\n",
      "5 - 0.8069799998060635\n",
      "6 - 0.8081840630457362\n",
      "7 - 0.8099362046986569\n",
      "8 - 0.8082683399301652\n",
      "9 - 0.8147727040808705\n",
      "10 - 0.8153688008405777\n",
      "11 - 0.8155968301708998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16900c9e588>]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cross_validate\n",
    "total_acc = []\n",
    "k = 10\n",
    "lambda_ = 0.01\n",
    "degrees = range(1, 15)\n",
    "\n",
    "# run that shit\n",
    "for deg in degrees:\n",
    "    print(deg, end=\" - \")\n",
    "    X_split_poly = [ build_X(X, deg) for X in X_split ]\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    # iterate over 4 sub datasets\n",
    "    for i in range(len(X_split_poly)):\n",
    "        classifier = LeastSquaresL2(lambda_)\n",
    "        acc = np.mean(cross_validate_kfold(y_split[i], X_split_poly[i], classifier, k))\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "    # compute mean (weighted)\n",
    "    accuracy = 0\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        accuracy += acc * len(y_split[i])\n",
    "    accuracy /= len(y)\n",
    "        \n",
    "    print(accuracy)\n",
    "    total_acc.append(accuracy)\n",
    "    \n",
    "plt.plot(degrees, total_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model for dataset 0\n",
      "Building model for dataset 1\n",
      "Building model for dataset 2\n",
      "Building model for dataset 3\n"
     ]
    }
   ],
   "source": [
    "# train actual models\n",
    "# degree 9 seems to be best\n",
    "\n",
    "X_split_poly = [ build_X(X, 9) for X in X_split ]\n",
    "lambda_ = 0.01\n",
    "models = []\n",
    "for i in range(len(X_split_poly)):\n",
    "    print(f\"Building model for dataset {i}\")\n",
    "    lse = LeastSquaresL2(lambda_)\n",
    "    lse.fit(y_split[i], X_split_poly[i])\n",
    "    models.append(lse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_new: 76658898567251466163194953728.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 38329449283176485824627736576.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 19164724641363614885782487040.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 9582362320569493429625552896.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 4791181160228590807691558912.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 2395590580086217725163077632.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 1197795290029069748362280960.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 598897645007515248352034816.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 299448822500247879980941312.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 149724411248369067892932608.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 74862205623307097897697280.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 37431102811214830924464128.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 18715551405388056450039808.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 9357775702584346571440128.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 4678887851237333532672000.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 2339443925591246352941056.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 1169721962781913238208512.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 584860981384101649973248.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 292430490688623340421120.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 146215245342597927927808.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 73107622670442084433920.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 36553811334792602451968.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 18276905667182079246336.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 9138452833483928633344.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 4569226416688408821760.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 2284613208317426663424.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 1142306604145324326912.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 571153302065967857664.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 285576651029636710400.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 142788325513144713216.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 71394162755735568384.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 35697081377449381888.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 17848540688515493888.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 8924270344153145344.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 4462135172024272384.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 2231067585985986560.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 1115533792979918336.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 557766896483421696.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 278883448238442208.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 139441724117586736.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 69720862057976232.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 34860431028579564.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 17430215514085542.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 8715107756940701.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 4357553878419383.500 - f: 69254.414 - Backtracking...\n",
      "f_new: 2178776939184305.750 - f: 69254.414 - Backtracking...\n",
      "f_new: 1089388469579598.500 - f: 69254.414 - Backtracking...\n",
      "f_new: 544694234783711.938 - f: 69254.414 - Backtracking...\n",
      "f_new: 272347117389066.719 - f: 69254.414 - Backtracking...\n",
      "f_new: 136173558693473.750 - f: 69254.414 - Backtracking...\n",
      "f_new: 68086779346642.148 - f: 69254.414 - Backtracking...\n",
      "f_new: 34043389673833.574 - f: 69254.414 - Backtracking...\n",
      "f_new: 17021694837892.352 - f: 69254.414 - Backtracking...\n",
      "f_new: 8510847420357.464 - f: 69254.414 - Backtracking...\n",
      "f_new: 4255423712067.646 - f: 69254.414 - Backtracking...\n",
      "f_new: 2127711858493.365 - f: 69254.414 - Backtracking...\n",
      "f_new: 1063855932419.652 - f: 69254.414 - Backtracking...\n",
      "f_new: 531927970307.091 - f: 69254.414 - Backtracking...\n",
      "f_new: 265963990478.546 - f: 69254.414 - Backtracking...\n",
      "f_new: 132982002214.515 - f: 69254.414 - Backtracking...\n",
      "f_new: 66491010303.369 - f: 69254.414 - Backtracking...\n",
      "f_new: 33245517324.573 - f: 69254.414 - Backtracking...\n",
      "f_new: 16622774634.730 - f: 69254.414 - Backtracking...\n",
      "f_new: 8311407338.168 - f: 69254.414 - Backtracking...\n",
      "f_new: 4155727005.679 - f: 69254.414 - Backtracking...\n",
      "f_new: 2077889258.295 - f: 69254.414 - Backtracking...\n",
      "f_new: 1038972215.206 - f: 69254.414 - Backtracking...\n",
      "f_new: 519515086.671 - f: 69254.414 - Backtracking...\n",
      "f_new: 259787523.507 - f: 69254.414 - Backtracking...\n",
      "f_new: 129924456.921 - f: 69254.414 - Backtracking...\n",
      "f_new: 64993486.571 - f: 69254.414 - Backtracking...\n",
      "f_new: 32528485.172 - f: 69254.414 - Backtracking...\n",
      "f_new: 16296420.965 - f: 69254.414 - Backtracking...\n",
      "f_new: 8180791.802 - f: 69254.414 - Backtracking...\n",
      "f_new: 4123352.532 - f: 69254.414 - Backtracking...\n",
      "f_new: 2094974.372 - f: 69254.414 - Backtracking...\n",
      "f_new: 1081082.186 - f: 69254.414 - Backtracking...\n",
      "f_new: 574383.365 - f: 69254.414 - Backtracking...\n",
      "f_new: 321231.732 - f: 69254.414 - Backtracking...\n",
      "f_new: 194809.026 - f: 69254.414 - Backtracking...\n",
      "f_new: 131714.538 - f: 69254.414 - Backtracking...\n",
      "f_new: 100257.696 - f: 69254.414 - Backtracking...\n",
      "f_new: 84596.086 - f: 69254.414 - Backtracking...\n",
      "f_new: 76811.424 - f: 69254.414 - Backtracking...\n",
      "f_new: 72951.059 - f: 69254.414 - Backtracking...\n",
      "f_new: 71043.769 - f: 69254.414 - Backtracking...\n",
      "f_new: 70107.038 - f: 69254.414 - Backtracking...\n",
      "f_new: 69651.210 - f: 69254.414 - Backtracking...\n",
      "f_new: 69432.345 - f: 69254.414 - Backtracking...\n",
      "f_new: 69329.220 - f: 69254.414 - Backtracking...\n",
      "f_new: 69282.072 - f: 69254.414 - Backtracking...\n",
      "f_new: 69261.326 - f: 69254.414 - Backtracking...\n",
      "f_new: 69252.593 - f: 69254.414 - Backtracking...\n",
      "f_new: 69249.404 - f: 69254.414 - Backtracking...\n",
      "f_new: 69248.856 - f: 69254.414 - Backtracking...\n",
      "f_new: 69249.478 - f: 69254.414 - Backtracking...\n",
      "f_new: 69250.558 - f: 69254.414 - Backtracking...\n",
      "f_new: 69251.556 - f: 69254.414 - Backtracking...\n",
      "f_new: 69252.226 - f: 69254.414 - Backtracking...\n",
      "f_new: 69252.610 - f: 69254.414 - Backtracking...\n",
      "f_new: 69252.822 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.227 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.431 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.564 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.640 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.680 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.700 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.711 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.716 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.718 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.720 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.720 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "121 - loss: 69253.721\n",
      "122 - loss: 69253.721\n",
      "123 - loss: 69253.158\n",
      "124 - loss: 69253.102\n",
      "125 - loss: 69253.065\n",
      "126 - loss: 69253.050\n",
      "127 - loss: 69253.043\n",
      "128 - loss: 69253.040\n",
      "129 - loss: 69253.039\n",
      "130 - loss: 69253.039\n",
      "131 - loss: 69253.038\n",
      "132 - loss: 69253.038\n",
      "133 - loss: 69253.038\n",
      "134 - loss: 69253.038\n",
      "135 - loss: 69253.031\n",
      "136 - loss: 69252.548\n",
      "f_new: 69624.582 - f: 69252.548 - Backtracking...\n",
      "138 - loss: 69252.548\n",
      "f_new: 69549.467 - f: 69252.548 - Backtracking...\n",
      "140 - loss: 69252.548\n",
      "f_new: 69541.870 - f: 69252.548 - Backtracking...\n",
      "142 - loss: 69252.548\n",
      "f_new: 69534.511 - f: 69252.548 - Backtracking...\n",
      "144 - loss: 69252.548\n",
      "f_new: 69526.762 - f: 69252.548 - Backtracking...\n",
      "146 - loss: 69252.548\n",
      "f_new: 69518.668 - f: 69252.548 - Backtracking...\n",
      "148 - loss: 69252.547\n",
      "f_new: 69510.288 - f: 69252.547 - Backtracking...\n",
      "150 - loss: 69252.547\n",
      "f_new: 69501.689 - f: 69252.547 - Backtracking...\n",
      "152 - loss: 69252.547\n",
      "f_new: 69492.941 - f: 69252.547 - Backtracking...\n",
      "154 - loss: 69252.547\n",
      "f_new: 69484.115 - f: 69252.547 - Backtracking...\n",
      "156 - loss: 69252.547\n",
      "f_new: 69475.281 - f: 69252.547 - Backtracking...\n",
      "158 - loss: 69252.547\n",
      "f_new: 69466.505 - f: 69252.547 - Backtracking...\n",
      "160 - loss: 69252.547\n",
      "f_new: 69457.845 - f: 69252.547 - Backtracking...\n",
      "162 - loss: 69252.547\n",
      "f_new: 69449.354 - f: 69252.547 - Backtracking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164 - loss: 69252.547\n",
      "f_new: 69441.077 - f: 69252.547 - Backtracking...\n",
      "166 - loss: 69252.547\n",
      "f_new: 69433.048 - f: 69252.547 - Backtracking...\n",
      "168 - loss: 69252.547\n",
      "f_new: 69425.296 - f: 69252.547 - Backtracking...\n",
      "170 - loss: 69252.547\n",
      "f_new: 69417.839 - f: 69252.547 - Backtracking...\n",
      "172 - loss: 69252.547\n",
      "f_new: 69410.690 - f: 69252.547 - Backtracking...\n",
      "174 - loss: 69252.547\n",
      "f_new: 69403.855 - f: 69252.547 - Backtracking...\n",
      "176 - loss: 69252.547\n",
      "f_new: 69397.336 - f: 69252.547 - Backtracking...\n",
      "178 - loss: 69252.546\n",
      "f_new: 69391.131 - f: 69252.546 - Backtracking...\n",
      "180 - loss: 69252.546\n",
      "f_new: 69385.233 - f: 69252.546 - Backtracking...\n",
      "182 - loss: 69252.546\n",
      "f_new: 69379.633 - f: 69252.546 - Backtracking...\n",
      "184 - loss: 69252.546\n",
      "f_new: 69374.322 - f: 69252.546 - Backtracking...\n",
      "186 - loss: 69252.546\n",
      "f_new: 69369.287 - f: 69252.546 - Backtracking...\n",
      "188 - loss: 69252.546\n",
      "f_new: 69364.516 - f: 69252.546 - Backtracking...\n",
      "190 - loss: 69252.546\n",
      "f_new: 69359.995 - f: 69252.546 - Backtracking...\n",
      "192 - loss: 69252.546\n",
      "f_new: 69355.713 - f: 69252.546 - Backtracking...\n",
      "194 - loss: 69252.546\n",
      "f_new: 69351.656 - f: 69252.546 - Backtracking...\n",
      "196 - loss: 69252.546\n",
      "f_new: 69347.812 - f: 69252.546 - Backtracking...\n",
      "198 - loss: 69252.546\n",
      "f_new: 69344.167 - f: 69252.546 - Backtracking...\n",
      "200 - loss: 69252.546\n",
      "f_new: 69340.710 - f: 69252.546 - Backtracking...\n",
      "202 - loss: 69252.546\n",
      "f_new: 69337.431 - f: 69252.546 - Backtracking...\n",
      "204 - loss: 69252.546\n",
      "f_new: 69334.317 - f: 69252.546 - Backtracking...\n",
      "206 - loss: 69252.546\n",
      "f_new: 69331.360 - f: 69252.546 - Backtracking...\n",
      "208 - loss: 69252.546\n",
      "f_new: 69328.549 - f: 69252.546 - Backtracking...\n",
      "210 - loss: 69252.546\n",
      "f_new: 69325.876 - f: 69252.546 - Backtracking...\n",
      "212 - loss: 69252.546\n",
      "f_new: 69323.332 - f: 69252.546 - Backtracking...\n",
      "214 - loss: 69252.546\n",
      "f_new: 69320.909 - f: 69252.546 - Backtracking...\n",
      "216 - loss: 69252.546\n",
      "f_new: 69318.600 - f: 69252.546 - Backtracking...\n",
      "218 - loss: 69252.546\n",
      "f_new: 69316.398 - f: 69252.546 - Backtracking...\n",
      "220 - loss: 69252.546\n",
      "f_new: 69314.296 - f: 69252.546 - Backtracking...\n",
      "222 - loss: 69252.546\n",
      "f_new: 69312.289 - f: 69252.546 - Backtracking...\n",
      "224 - loss: 69252.546\n",
      "f_new: 69310.372 - f: 69252.546 - Backtracking...\n",
      "226 - loss: 69252.546\n",
      "f_new: 69308.537 - f: 69252.546 - Backtracking...\n",
      "228 - loss: 69252.546\n",
      "f_new: 69306.782 - f: 69252.546 - Backtracking...\n",
      "230 - loss: 69252.546\n",
      "f_new: 69305.101 - f: 69252.546 - Backtracking...\n",
      "232 - loss: 69252.546\n",
      "f_new: 69303.491 - f: 69252.546 - Backtracking...\n",
      "234 - loss: 69252.546\n",
      "f_new: 69301.946 - f: 69252.546 - Backtracking...\n",
      "236 - loss: 69252.546\n",
      "f_new: 69300.464 - f: 69252.546 - Backtracking...\n",
      "238 - loss: 69252.546\n",
      "f_new: 69299.041 - f: 69252.546 - Backtracking...\n",
      "240 - loss: 69252.545\n",
      "f_new: 69297.674 - f: 69252.545 - Backtracking...\n",
      "242 - loss: 69252.545\n",
      "f_new: 69296.359 - f: 69252.545 - Backtracking...\n",
      "244 - loss: 69252.545\n",
      "f_new: 69295.095 - f: 69252.545 - Backtracking...\n",
      "246 - loss: 69252.545\n",
      "f_new: 69293.878 - f: 69252.545 - Backtracking...\n",
      "248 - loss: 69252.545\n",
      "f_new: 69292.706 - f: 69252.545 - Backtracking...\n",
      "250 - loss: 69252.545\n",
      "f_new: 69291.577 - f: 69252.545 - Backtracking...\n",
      "252 - loss: 69252.545\n",
      "f_new: 69290.488 - f: 69252.545 - Backtracking...\n",
      "254 - loss: 69252.545\n",
      "f_new: 69289.438 - f: 69252.545 - Backtracking...\n",
      "256 - loss: 69252.545\n",
      "f_new: 69288.424 - f: 69252.545 - Backtracking...\n",
      "258 - loss: 69252.545\n",
      "f_new: 69287.444 - f: 69252.545 - Backtracking...\n",
      "260 - loss: 69252.545\n",
      "f_new: 69286.498 - f: 69252.545 - Backtracking...\n",
      "262 - loss: 69252.545\n",
      "f_new: 69285.584 - f: 69252.545 - Backtracking...\n",
      "264 - loss: 69252.545\n",
      "f_new: 69284.699 - f: 69252.545 - Backtracking...\n",
      "266 - loss: 69252.545\n",
      "f_new: 69283.843 - f: 69252.545 - Backtracking...\n",
      "268 - loss: 69252.545\n",
      "f_new: 69283.014 - f: 69252.545 - Backtracking...\n",
      "270 - loss: 69252.545\n",
      "f_new: 69282.211 - f: 69252.545 - Backtracking...\n",
      "272 - loss: 69252.545\n",
      "f_new: 69281.433 - f: 69252.545 - Backtracking...\n",
      "274 - loss: 69252.545\n",
      "f_new: 69280.679 - f: 69252.545 - Backtracking...\n",
      "276 - loss: 69252.545\n",
      "f_new: 69279.948 - f: 69252.545 - Backtracking...\n",
      "278 - loss: 69252.545\n",
      "f_new: 69279.238 - f: 69252.545 - Backtracking...\n",
      "280 - loss: 69252.545\n",
      "f_new: 69278.549 - f: 69252.545 - Backtracking...\n",
      "282 - loss: 69252.545\n",
      "f_new: 69277.879 - f: 69252.545 - Backtracking...\n",
      "284 - loss: 69252.545\n",
      "f_new: 69277.229 - f: 69252.545 - Backtracking...\n",
      "286 - loss: 69252.545\n",
      "f_new: 69276.597 - f: 69252.545 - Backtracking...\n",
      "288 - loss: 69252.545\n",
      "f_new: 69275.983 - f: 69252.545 - Backtracking...\n",
      "290 - loss: 69252.545\n",
      "f_new: 69275.385 - f: 69252.545 - Backtracking...\n",
      "292 - loss: 69252.545\n",
      "f_new: 69274.803 - f: 69252.545 - Backtracking...\n",
      "294 - loss: 69252.545\n",
      "f_new: 69274.237 - f: 69252.545 - Backtracking...\n",
      "296 - loss: 69252.545\n",
      "f_new: 69273.685 - f: 69252.545 - Backtracking...\n",
      "298 - loss: 69252.545\n",
      "f_new: 69273.148 - f: 69252.545 - Backtracking...\n",
      "300 - loss: 69252.545\n",
      "f_new: 69272.625 - f: 69252.545 - Backtracking...\n",
      "302 - loss: 69252.545\n",
      "f_new: 69272.114 - f: 69252.545 - Backtracking...\n",
      "304 - loss: 69252.545\n",
      "f_new: 69271.616 - f: 69252.545 - Backtracking...\n",
      "306 - loss: 69252.545\n",
      "f_new: 69271.131 - f: 69252.545 - Backtracking...\n",
      "308 - loss: 69252.545\n",
      "f_new: 69270.657 - f: 69252.545 - Backtracking...\n",
      "310 - loss: 69252.545\n",
      "f_new: 69270.194 - f: 69252.545 - Backtracking...\n",
      "312 - loss: 69252.545\n",
      "f_new: 69269.742 - f: 69252.545 - Backtracking...\n",
      "314 - loss: 69252.545\n",
      "f_new: 69269.300 - f: 69252.545 - Backtracking...\n",
      "316 - loss: 69252.545\n",
      "f_new: 69268.869 - f: 69252.545 - Backtracking...\n",
      "318 - loss: 69252.545\n",
      "f_new: 69268.447 - f: 69252.545 - Backtracking...\n",
      "320 - loss: 69252.545\n",
      "f_new: 69268.034 - f: 69252.545 - Backtracking...\n",
      "322 - loss: 69252.545\n",
      "f_new: 69267.631 - f: 69252.545 - Backtracking...\n",
      "324 - loss: 69252.545\n",
      "f_new: 69267.236 - f: 69252.545 - Backtracking...\n",
      "326 - loss: 69252.545\n",
      "f_new: 69266.849 - f: 69252.545 - Backtracking...\n",
      "328 - loss: 69252.545\n",
      "f_new: 69266.470 - f: 69252.545 - Backtracking...\n",
      "330 - loss: 69252.545\n",
      "f_new: 69266.100 - f: 69252.545 - Backtracking...\n",
      "332 - loss: 69252.545\n",
      "f_new: 69265.736 - f: 69252.545 - Backtracking...\n",
      "334 - loss: 69252.545\n",
      "f_new: 69265.380 - f: 69252.545 - Backtracking...\n",
      "336 - loss: 69252.545\n",
      "f_new: 69265.031 - f: 69252.545 - Backtracking...\n",
      "338 - loss: 69252.545\n",
      "f_new: 69264.688 - f: 69252.545 - Backtracking...\n",
      "340 - loss: 69252.545\n",
      "f_new: 69264.352 - f: 69252.545 - Backtracking...\n",
      "342 - loss: 69252.545\n",
      "f_new: 69264.022 - f: 69252.545 - Backtracking...\n",
      "344 - loss: 69252.545\n",
      "f_new: 69263.698 - f: 69252.545 - Backtracking...\n",
      "346 - loss: 69252.545\n",
      "f_new: 69263.380 - f: 69252.545 - Backtracking...\n",
      "348 - loss: 69252.545\n",
      "f_new: 69263.067 - f: 69252.545 - Backtracking...\n",
      "350 - loss: 69252.545\n",
      "f_new: 69262.759 - f: 69252.545 - Backtracking...\n",
      "352 - loss: 69252.545\n",
      "f_new: 69262.457 - f: 69252.545 - Backtracking...\n",
      "354 - loss: 69252.545\n",
      "f_new: 69262.160 - f: 69252.545 - Backtracking...\n",
      "356 - loss: 69252.545\n",
      "f_new: 69261.867 - f: 69252.545 - Backtracking...\n",
      "358 - loss: 69252.545\n",
      "f_new: 69261.579 - f: 69252.545 - Backtracking...\n",
      "360 - loss: 69252.545\n",
      "f_new: 69261.296 - f: 69252.545 - Backtracking...\n",
      "362 - loss: 69252.545\n",
      "f_new: 69261.016 - f: 69252.545 - Backtracking...\n",
      "364 - loss: 69252.545\n",
      "f_new: 69260.741 - f: 69252.545 - Backtracking...\n",
      "366 - loss: 69252.545\n",
      "f_new: 69260.469 - f: 69252.545 - Backtracking...\n",
      "368 - loss: 69252.545\n",
      "f_new: 69260.201 - f: 69252.545 - Backtracking...\n",
      "370 - loss: 69252.545\n",
      "f_new: 69259.937 - f: 69252.545 - Backtracking...\n",
      "372 - loss: 69252.545\n",
      "f_new: 69259.675 - f: 69252.545 - Backtracking...\n",
      "374 - loss: 69252.545\n",
      "f_new: 69259.417 - f: 69252.545 - Backtracking...\n",
      "376 - loss: 69252.545\n",
      "f_new: 69259.162 - f: 69252.545 - Backtracking...\n",
      "378 - loss: 69252.545\n",
      "f_new: 69258.909 - f: 69252.545 - Backtracking...\n",
      "380 - loss: 69252.544\n",
      "f_new: 69258.660 - f: 69252.544 - Backtracking...\n",
      "382 - loss: 69252.544\n",
      "f_new: 69258.412 - f: 69252.544 - Backtracking...\n",
      "384 - loss: 69252.544\n",
      "f_new: 69258.167 - f: 69252.544 - Backtracking...\n",
      "386 - loss: 69252.544\n",
      "f_new: 69257.924 - f: 69252.544 - Backtracking...\n",
      "388 - loss: 69252.544\n",
      "f_new: 69257.682 - f: 69252.544 - Backtracking...\n",
      "390 - loss: 69252.544\n",
      "f_new: 69257.442 - f: 69252.544 - Backtracking...\n",
      "392 - loss: 69252.544\n",
      "f_new: 69257.204 - f: 69252.544 - Backtracking...\n",
      "394 - loss: 69252.544\n",
      "f_new: 69256.967 - f: 69252.544 - Backtracking...\n",
      "396 - loss: 69252.544\n",
      "f_new: 69256.731 - f: 69252.544 - Backtracking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398 - loss: 69252.544\n",
      "f_new: 69256.496 - f: 69252.544 - Backtracking...\n",
      "400 - loss: 69252.544\n",
      "Reached maximum number of function evaluations 400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3692912834165724"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing regularized logistic regression \n",
    "# seems to be absolute shit for this \n",
    "classifier = LogisticRegressionL2(0.01, verbose=True, max_evaluations=400)\n",
    "classifier.fit(y_split[0], X_split_poly[0])\n",
    "np.mean(y_split[0] == classifier.predict(X_split_poly[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.543932\n"
     ]
    }
   ],
   "source": [
    "kernel = LeastSquaresKernel(Kernel.kernel_poly, verbose=True, max_evals=300)\n",
    "p = 4\n",
    "lambda_ = 0\n",
    "incr = 40\n",
    "incr_pred = 30\n",
    "pred = kernel.predict(y[::incr], X__[::incr], X__[::incr_pred], p, lambda_=lambda_)\n",
    "print(np.mean(y[::incr_pred] == pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = replace_NaN_by_median(X)\n",
    "X, _, _ = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub = X[::30]\n",
    "y_sub = y[::30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_new: 351332120652.464 - f: 5776.689 - Backtracking...\n",
      "f_new: 49908376999.952 - f: 5776.689 - Backtracking...\n",
      "f_new: 7089719742.161 - f: 5776.689 - Backtracking...\n",
      "f_new: 1007128546.313 - f: 5776.689 - Backtracking...\n",
      "f_new: 143067923.558 - f: 5776.689 - Backtracking...\n",
      "f_new: 20324057.388 - f: 5776.689 - Backtracking...\n",
      "f_new: 2887715.309 - f: 5776.689 - Backtracking...\n",
      "f_new: 410801.817 - f: 5776.689 - Backtracking...\n",
      "f_new: 58949.841 - f: 5776.689 - Backtracking...\n",
      "f_new: 9667.717 - f: 5776.689 - Backtracking...\n",
      "11 - loss: 5549.281\n",
      "12 - loss: 5405.800\n",
      "13 - loss: 5352.943\n",
      "14 - loss: 5260.879\n",
      "15 - loss: 5057.649\n",
      "f_new: 5188.842 - f: 5057.649 - Backtracking...\n",
      "17 - loss: 5042.088\n",
      "18 - loss: 5030.287\n",
      "19 - loss: 5022.815\n",
      "20 - loss: 4723.083\n",
      "f_new: 5530.003 - f: 4723.083 - Backtracking...\n",
      "22 - loss: 4714.058\n",
      "23 - loss: 4702.917\n",
      "24 - loss: 4700.799\n",
      "25 - loss: 4698.556\n",
      "26 - loss: 4630.235\n",
      "27 - loss: 4608.311\n",
      "f_new: 4631.497 - f: 4608.311 - Backtracking...\n",
      "29 - loss: 4598.083\n",
      "30 - loss: 4596.268\n",
      "31 - loss: 4594.125\n",
      "32 - loss: 4582.093\n",
      "33 - loss: 4575.996\n",
      "34 - loss: 4550.599\n",
      "f_new: 4573.412 - f: 4550.599 - Backtracking...\n",
      "36 - loss: 4549.247\n",
      "37 - loss: 4549.045\n",
      "38 - loss: 4548.844\n",
      "39 - loss: 4529.424\n",
      "f_new: 4534.597 - f: 4529.424 - Backtracking...\n",
      "41 - loss: 4526.565\n",
      "f_new: 4531.257 - f: 4526.565 - Backtracking...\n",
      "43 - loss: 4525.883\n",
      "44 - loss: 4525.601\n",
      "45 - loss: 4525.368\n",
      "46 - loss: 4523.787\n",
      "47 - loss: 4522.931\n",
      "48 - loss: 4522.724\n",
      "49 - loss: 4522.637\n",
      "50 - loss: 4522.573\n",
      "51 - loss: 4521.830\n",
      "52 - loss: 4503.879\n",
      "f_new: 4512.120 - f: 4503.879 - Backtracking...\n",
      "54 - loss: 4503.280\n",
      "f_new: 4504.387 - f: 4503.280 - Backtracking...\n",
      "56 - loss: 4503.135\n",
      "57 - loss: 4503.078\n",
      "58 - loss: 4503.030\n",
      "59 - loss: 4502.749\n",
      "60 - loss: 4502.654\n",
      "61 - loss: 4502.642\n",
      "62 - loss: 4502.635\n",
      "63 - loss: 4502.627\n",
      "64 - loss: 4499.973\n",
      "65 - loss: 4499.873\n",
      "f_new: 4501.201 - f: 4499.873 - Backtracking...\n",
      "67 - loss: 4499.465\n",
      "68 - loss: 4499.452\n",
      "69 - loss: 4499.420\n",
      "70 - loss: 4499.313\n",
      "71 - loss: 4499.241\n",
      "72 - loss: 4499.151\n",
      "73 - loss: 4499.114\n",
      "74 - loss: 4499.100\n",
      "75 - loss: 4499.098\n",
      "76 - loss: 4498.991\n",
      "77 - loss: 4498.931\n",
      "78 - loss: 4498.902\n",
      "f_new: 4498.948 - f: 4498.902 - Backtracking...\n",
      "80 - loss: 4498.897\n",
      "81 - loss: 4498.896\n",
      "82 - loss: 4498.895\n",
      "83 - loss: 4498.882\n",
      "84 - loss: 4498.868\n",
      "85 - loss: 4498.805\n",
      "86 - loss: 4498.798\n",
      "87 - loss: 4498.741\n",
      "88 - loss: 4498.741\n",
      "89 - loss: 4498.740\n",
      "90 - loss: 4498.728\n",
      "91 - loss: 4498.716\n",
      "92 - loss: 4498.636\n",
      "93 - loss: 4498.474\n",
      "94 - loss: 4498.428\n",
      "95 - loss: 4498.427\n",
      "96 - loss: 4498.427\n",
      "97 - loss: 4498.417\n",
      "98 - loss: 4498.407\n",
      "99 - loss: 4497.942\n",
      "f_new: 4498.956 - f: 4497.942 - Backtracking...\n",
      "101 - loss: 4497.912\n",
      "102 - loss: 4497.882\n",
      "103 - loss: 4497.881\n",
      "104 - loss: 4497.880\n",
      "105 - loss: 4497.852\n",
      "106 - loss: 4497.820\n",
      "107 - loss: 4497.806\n",
      "108 - loss: 4497.796\n",
      "109 - loss: 4497.795\n",
      "110 - loss: 4497.794\n",
      "111 - loss: 4497.793\n",
      "112 - loss: 4497.354\n",
      "113 - loss: 4496.800\n",
      "114 - loss: 4496.751\n",
      "f_new: 4496.845 - f: 4496.751 - Backtracking...\n",
      "116 - loss: 4496.748\n",
      "117 - loss: 4496.747\n",
      "118 - loss: 4496.746\n",
      "119 - loss: 4496.737\n",
      "120 - loss: 4496.728\n",
      "121 - loss: 4496.620\n",
      "122 - loss: 4496.497\n",
      "123 - loss: 4496.491\n",
      "124 - loss: 4496.490\n",
      "125 - loss: 4496.489\n",
      "126 - loss: 4496.475\n",
      "127 - loss: 4496.460\n",
      "128 - loss: 4496.432\n",
      "129 - loss: 4496.381\n",
      "130 - loss: 4496.363\n",
      "131 - loss: 4496.360\n",
      "132 - loss: 4496.360\n",
      "133 - loss: 4496.359\n",
      "134 - loss: 4496.353\n",
      "135 - loss: 4496.343\n",
      "136 - loss: 4496.325\n",
      "137 - loss: 4496.039\n",
      "138 - loss: 4496.038\n",
      "139 - loss: 4496.037\n",
      "140 - loss: 4496.036\n",
      "141 - loss: 4496.026\n",
      "142 - loss: 4496.015\n",
      "143 - loss: 4495.933\n",
      "144 - loss: 4493.641\n",
      "f_new: 4493.794 - f: 4493.641 - Backtracking...\n",
      "146 - loss: 4493.636\n",
      "147 - loss: 4493.634\n",
      "148 - loss: 4493.633\n",
      "149 - loss: 4493.621\n",
      "150 - loss: 4493.613\n",
      "151 - loss: 4493.612\n",
      "152 - loss: 4493.611\n",
      "153 - loss: 4493.611\n",
      "154 - loss: 4468.050\n",
      "f_new: 17878.460 - f: 4468.050 - Backtracking...\n",
      "f_new: 4468.051 - f: 4468.050 - Backtracking...\n",
      "157 - loss: 4468.048\n",
      "158 - loss: 4468.046\n",
      "159 - loss: 4468.046\n",
      "160 - loss: 4468.045\n",
      "161 - loss: 4468.032\n",
      "162 - loss: 4468.018\n",
      "163 - loss: 4468.005\n",
      "164 - loss: 4468.003\n",
      "165 - loss: 4468.002\n",
      "166 - loss: 4468.002\n",
      "167 - loss: 4468.000\n",
      "168 - loss: 4467.988\n",
      "169 - loss: 4467.972\n",
      "170 - loss: 4467.958\n",
      "171 - loss: 4467.956\n",
      "172 - loss: 4467.955\n",
      "173 - loss: 4467.954\n",
      "174 - loss: 4467.954\n",
      "175 - loss: 4467.945\n",
      "176 - loss: 4467.935\n",
      "177 - loss: 4467.921\n",
      "178 - loss: 4467.908\n",
      "179 - loss: 4467.908\n",
      "180 - loss: 4467.907\n",
      "181 - loss: 4467.907\n",
      "182 - loss: 4467.899\n",
      "183 - loss: 4467.892\n",
      "184 - loss: 4467.873\n",
      "185 - loss: 4467.829\n",
      "186 - loss: 4467.828\n",
      "187 - loss: 4467.828\n",
      "188 - loss: 4467.828\n",
      "189 - loss: 4467.822\n",
      "190 - loss: 4467.817\n",
      "191 - loss: 4467.605\n",
      "192 - loss: 4467.590\n",
      "193 - loss: 4467.517\n",
      "194 - loss: 4467.516\n",
      "195 - loss: 4467.516\n",
      "196 - loss: 4467.484\n",
      "197 - loss: 4467.446\n",
      "198 - loss: 4467.440\n",
      "199 - loss: 4467.434\n",
      "200 - loss: 4467.433\n",
      "201 - loss: 4467.433\n",
      "202 - loss: 4467.433\n",
      "203 - loss: 4467.224\n",
      "204 - loss: 4461.692\n",
      "f_new: 4465.338 - f: 4461.692 - Backtracking...\n",
      "206 - loss: 4461.682\n",
      "207 - loss: 4461.676\n",
      "208 - loss: 4461.674\n",
      "209 - loss: 4461.673\n",
      "210 - loss: 4461.665\n",
      "211 - loss: 4461.660\n",
      "212 - loss: 4461.659\n",
      "213 - loss: 4461.659\n",
      "214 - loss: 4461.659\n",
      "215 - loss: 4461.646\n",
      "216 - loss: 4461.562\n",
      "217 - loss: 4461.462\n",
      "218 - loss: 4461.461\n",
      "219 - loss: 4461.461\n",
      "220 - loss: 4461.460\n",
      "221 - loss: 4461.445\n",
      "222 - loss: 4461.427\n",
      "223 - loss: 4461.421\n",
      "224 - loss: 4461.416\n",
      "225 - loss: 4461.412\n",
      "226 - loss: 4461.412\n",
      "227 - loss: 4461.411\n",
      "228 - loss: 4461.411\n",
      "229 - loss: 4460.963\n",
      "230 - loss: 4460.522\n",
      "f_new: 4460.592 - f: 4460.522 - Backtracking...\n",
      "232 - loss: 4460.519\n",
      "233 - loss: 4460.517\n",
      "234 - loss: 4460.516\n",
      "235 - loss: 4460.513\n",
      "236 - loss: 4460.506\n",
      "237 - loss: 4460.502\n",
      "238 - loss: 4460.501\n",
      "239 - loss: 4460.500\n",
      "240 - loss: 4460.500\n",
      "241 - loss: 4460.446\n",
      "242 - loss: 4460.396\n",
      "243 - loss: 4460.393\n",
      "244 - loss: 4460.392\n",
      "245 - loss: 4460.392\n",
      "246 - loss: 4460.389\n",
      "247 - loss: 4460.365\n",
      "248 - loss: 4460.297\n",
      "249 - loss: 4460.287\n",
      "250 - loss: 4460.282\n",
      "251 - loss: 4460.280\n",
      "252 - loss: 4460.280\n",
      "253 - loss: 4460.279\n",
      "254 - loss: 4460.246\n",
      "255 - loss: 4453.774\n",
      "f_new: 4465.565 - f: 4453.774 - Backtracking...\n",
      "257 - loss: 4453.764\n",
      "258 - loss: 4453.761\n",
      "259 - loss: 4453.759\n",
      "260 - loss: 4453.759\n",
      "261 - loss: 4453.758\n",
      "262 - loss: 4453.748\n",
      "263 - loss: 4453.736\n",
      "264 - loss: 4453.730\n",
      "265 - loss: 4453.725\n",
      "266 - loss: 4453.721\n",
      "267 - loss: 4453.720\n",
      "268 - loss: 4453.720\n",
      "269 - loss: 4453.720\n",
      "270 - loss: 4453.154\n",
      "271 - loss: 4452.624\n",
      "f_new: 4452.717 - f: 4452.624 - Backtracking...\n",
      "273 - loss: 4452.598\n",
      "274 - loss: 4452.582\n",
      "275 - loss: 4452.577\n",
      "276 - loss: 4452.574\n",
      "277 - loss: 4452.553\n",
      "278 - loss: 4452.546\n",
      "279 - loss: 4452.541\n",
      "280 - loss: 4452.537\n",
      "281 - loss: 4452.536\n",
      "282 - loss: 4452.536\n",
      "283 - loss: 4452.509\n",
      "284 - loss: 4452.487\n",
      "285 - loss: 4452.478\n",
      "286 - loss: 4452.476\n",
      "287 - loss: 4452.476\n",
      "288 - loss: 4452.476\n",
      "289 - loss: 4452.458\n",
      "290 - loss: 4452.438\n",
      "291 - loss: 4452.434\n",
      "292 - loss: 4452.430\n",
      "293 - loss: 4452.428\n",
      "294 - loss: 4452.428\n",
      "295 - loss: 4452.428\n",
      "296 - loss: 4452.420\n",
      "297 - loss: 4452.183\n",
      "298 - loss: 4451.893\n",
      "f_new: 4451.894 - f: 4451.893 - Backtracking...\n",
      "300 - loss: 4451.891\n",
      "301 - loss: 4451.891\n",
      "302 - loss: 4451.891\n",
      "303 - loss: 4451.886\n",
      "304 - loss: 4451.882\n",
      "305 - loss: 4451.871\n",
      "306 - loss: 4451.870\n",
      "307 - loss: 4451.870\n",
      "308 - loss: 4451.869\n",
      "309 - loss: 4451.818\n",
      "310 - loss: 4451.767\n",
      "311 - loss: 4451.711\n",
      "312 - loss: 4451.709\n",
      "313 - loss: 4451.709\n",
      "314 - loss: 4451.709\n",
      "315 - loss: 4451.705\n",
      "316 - loss: 4451.701\n",
      "317 - loss: 4451.073\n",
      "f_new: 4452.226 - f: 4451.073 - Backtracking...\n",
      "319 - loss: 4451.067\n",
      "320 - loss: 4451.061\n",
      "321 - loss: 4451.060\n",
      "322 - loss: 4451.060\n",
      "323 - loss: 4451.017\n",
      "324 - loss: 4450.972\n",
      "325 - loss: 4450.967\n",
      "326 - loss: 4450.963\n",
      "327 - loss: 4450.963\n",
      "328 - loss: 4450.963\n",
      "329 - loss: 4450.962\n",
      "330 - loss: 4450.785\n",
      "331 - loss: 4450.594\n",
      "332 - loss: 4450.583\n",
      "f_new: 4450.584 - f: 4450.583 - Backtracking...\n",
      "334 - loss: 4450.582\n",
      "335 - loss: 4450.581\n",
      "336 - loss: 4450.581\n",
      "337 - loss: 4450.580\n",
      "338 - loss: 4450.575\n",
      "339 - loss: 4450.569\n",
      "340 - loss: 4450.555\n",
      "341 - loss: 4450.551\n",
      "342 - loss: 4450.551\n",
      "343 - loss: 4450.550\n",
      "344 - loss: 4450.550\n",
      "345 - loss: 4450.521\n",
      "346 - loss: 4450.491\n",
      "347 - loss: 4450.487\n",
      "348 - loss: 4450.483\n",
      "349 - loss: 4450.483\n",
      "350 - loss: 4450.482\n",
      "351 - loss: 4450.482\n",
      "352 - loss: 4447.174\n",
      "f_new: 4494.868 - f: 4447.174 - Backtracking...\n",
      "354 - loss: 4446.870\n",
      "355 - loss: 4446.776\n",
      "356 - loss: 4446.682\n",
      "357 - loss: 4446.678\n",
      "358 - loss: 4446.626\n",
      "359 - loss: 4446.595\n",
      "360 - loss: 4446.566\n",
      "f_new: 4446.600 - f: 4446.566 - Backtracking...\n",
      "362 - loss: 4446.564\n",
      "363 - loss: 4446.562\n",
      "364 - loss: 4446.562\n",
      "365 - loss: 4446.561\n",
      "366 - loss: 4446.545\n",
      "367 - loss: 4446.533\n",
      "368 - loss: 4446.529\n",
      "f_new: 4446.530 - f: 4446.529 - Backtracking...\n",
      "370 - loss: 4446.528\n",
      "371 - loss: 4446.528\n",
      "372 - loss: 4446.527\n",
      "373 - loss: 4446.523\n",
      "374 - loss: 4446.519\n",
      "375 - loss: 4446.508\n",
      "376 - loss: 4446.491\n",
      "377 - loss: 4446.491\n",
      "378 - loss: 4446.491\n",
      "379 - loss: 4446.490\n",
      "380 - loss: 4446.487\n",
      "381 - loss: 4446.483\n",
      "382 - loss: 4446.426\n",
      "383 - loss: 4446.209\n",
      "384 - loss: 4446.208\n",
      "385 - loss: 4446.207\n",
      "386 - loss: 4446.207\n",
      "387 - loss: 4446.203\n",
      "388 - loss: 4446.199\n",
      "389 - loss: 4446.186\n",
      "390 - loss: 4446.165\n",
      "391 - loss: 4446.162\n",
      "392 - loss: 4446.162\n",
      "393 - loss: 4446.162\n",
      "394 - loss: 4446.161\n",
      "395 - loss: 4446.144\n",
      "396 - loss: 4446.124\n",
      "397 - loss: 4446.119\n",
      "398 - loss: 4446.116\n",
      "399 - loss: 4446.114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 - loss: 4446.114\n",
      "401 - loss: 4446.114\n",
      "402 - loss: 4446.112\n",
      "403 - loss: 4439.172\n",
      "f_new: 6378.863 - f: 4439.172 - Backtracking...\n",
      "405 - loss: 4439.165\n",
      "406 - loss: 4439.154\n",
      "407 - loss: 4439.154\n",
      "408 - loss: 4439.154\n",
      "409 - loss: 4439.151\n",
      "410 - loss: 4439.148\n",
      "411 - loss: 4439.077\n",
      "f_new: 4439.109 - f: 4439.077 - Backtracking...\n",
      "413 - loss: 4439.072\n",
      "414 - loss: 4439.067\n",
      "415 - loss: 4439.067\n",
      "416 - loss: 4439.066\n",
      "417 - loss: 4438.991\n",
      "418 - loss: 4438.892\n",
      "419 - loss: 4438.888\n",
      "f_new: 4438.888 - f: 4438.888 - Backtracking...\n",
      "421 - loss: 4438.887\n",
      "422 - loss: 4438.887\n",
      "423 - loss: 4438.887\n",
      "424 - loss: 4438.884\n",
      "425 - loss: 4438.881\n",
      "426 - loss: 4438.876\n",
      "427 - loss: 4438.798\n",
      "428 - loss: 4438.691\n",
      "429 - loss: 4438.690\n",
      "430 - loss: 4438.689\n",
      "431 - loss: 4438.689\n",
      "432 - loss: 4438.688\n",
      "433 - loss: 4438.685\n",
      "434 - loss: 4438.682\n",
      "435 - loss: 4438.675\n",
      "436 - loss: 4438.674\n",
      "437 - loss: 4438.674\n",
      "438 - loss: 4438.674\n",
      "439 - loss: 4438.644\n",
      "440 - loss: 4438.614\n",
      "441 - loss: 4438.574\n",
      "442 - loss: 4438.573\n",
      "443 - loss: 4438.572\n",
      "444 - loss: 4438.572\n",
      "445 - loss: 4438.572\n",
      "446 - loss: 4438.569\n",
      "447 - loss: 4438.566\n",
      "448 - loss: 4438.536\n",
      "449 - loss: 4438.519\n",
      "450 - loss: 4438.518\n",
      "451 - loss: 4438.518\n",
      "452 - loss: 4438.518\n",
      "453 - loss: 4438.508\n",
      "454 - loss: 4438.498\n",
      "455 - loss: 4438.494\n",
      "456 - loss: 4438.491\n",
      "457 - loss: 4438.487\n",
      "458 - loss: 4438.487\n",
      "459 - loss: 4438.487\n",
      "460 - loss: 4438.487\n",
      "461 - loss: 4437.533\n",
      "f_new: 4442.972 - f: 4437.533 - Backtracking...\n",
      "463 - loss: 4436.354\n",
      "464 - loss: 4435.380\n",
      "f_new: 4437.712 - f: 4435.380 - Backtracking...\n",
      "466 - loss: 4435.229\n",
      "467 - loss: 4435.229\n",
      "468 - loss: 4435.228\n",
      "469 - loss: 4435.225\n",
      "470 - loss: 4435.222\n",
      "471 - loss: 4435.211\n",
      "472 - loss: 4435.210\n",
      "473 - loss: 4435.210\n",
      "474 - loss: 4435.209\n",
      "475 - loss: 4435.198\n",
      "476 - loss: 4435.186\n",
      "477 - loss: 4435.183\n",
      "478 - loss: 4435.181\n",
      "479 - loss: 4435.180\n",
      "480 - loss: 4435.180\n",
      "481 - loss: 4435.180\n",
      "482 - loss: 4434.978\n",
      "483 - loss: 4434.638\n",
      "484 - loss: 4434.624\n",
      "f_new: 4434.651 - f: 4434.624 - Backtracking...\n",
      "486 - loss: 4434.622\n",
      "487 - loss: 4434.621\n",
      "488 - loss: 4434.620\n",
      "489 - loss: 4434.614\n",
      "490 - loss: 4434.612\n",
      "491 - loss: 4434.611\n",
      "492 - loss: 4434.611\n",
      "493 - loss: 4434.611\n",
      "494 - loss: 4434.592\n",
      "495 - loss: 4434.573\n",
      "496 - loss: 4434.544\n",
      "497 - loss: 4434.543\n",
      "498 - loss: 4434.543\n",
      "499 - loss: 4434.543\n",
      "500 - loss: 4434.539\n",
      "501 - loss: 4434.534\n",
      "502 - loss: 4434.528\n",
      "503 - loss: 4434.518\n",
      "504 - loss: 4434.513\n",
      "505 - loss: 4434.513\n",
      "506 - loss: 4434.513\n",
      "507 - loss: 4434.513\n",
      "508 - loss: 4434.509\n",
      "509 - loss: 4434.506\n",
      "510 - loss: 4434.490\n",
      "511 - loss: 4434.389\n",
      "512 - loss: 4434.388\n",
      "513 - loss: 4434.388\n",
      "514 - loss: 4434.388\n",
      "515 - loss: 4434.385\n",
      "516 - loss: 4434.383\n",
      "517 - loss: 4432.661\n",
      "f_new: 4435.044 - f: 4432.661 - Backtracking...\n",
      "519 - loss: 4432.661\n",
      "520 - loss: 4432.661\n",
      "521 - loss: 4432.661\n",
      "522 - loss: 4432.635\n",
      "523 - loss: 4432.609\n",
      "524 - loss: 4432.606\n",
      "525 - loss: 4432.604\n",
      "526 - loss: 4432.603\n",
      "527 - loss: 4432.603\n",
      "528 - loss: 4432.603\n",
      "529 - loss: 4432.582\n",
      "530 - loss: 4432.445\n",
      "531 - loss: 4431.995\n",
      "f_new: 4432.004 - f: 4431.995 - Backtracking...\n",
      "533 - loss: 4431.993\n",
      "534 - loss: 4431.993\n",
      "535 - loss: 4431.993\n",
      "536 - loss: 4431.989\n",
      "537 - loss: 4431.986\n",
      "538 - loss: 4431.977\n",
      "539 - loss: 4431.977\n",
      "540 - loss: 4431.976\n",
      "541 - loss: 4431.976\n",
      "542 - loss: 4431.966\n",
      "543 - loss: 4431.955\n",
      "544 - loss: 4431.952\n",
      "545 - loss: 4431.950\n",
      "546 - loss: 4431.950\n",
      "547 - loss: 4431.950\n",
      "548 - loss: 4431.949\n",
      "549 - loss: 4431.870\n",
      "550 - loss: 4431.790\n",
      "551 - loss: 4431.775\n",
      "552 - loss: 4431.773\n",
      "553 - loss: 4431.772\n",
      "554 - loss: 4431.772\n",
      "555 - loss: 4431.772\n",
      "556 - loss: 4431.769\n",
      "557 - loss: 4431.767\n",
      "558 - loss: 4431.746\n",
      "559 - loss: 4431.696\n",
      "560 - loss: 4431.688\n",
      "561 - loss: 4431.687\n",
      "562 - loss: 4431.686\n",
      "563 - loss: 4431.686\n",
      "564 - loss: 4431.684\n",
      "565 - loss: 4431.681\n",
      "566 - loss: 4431.554\n",
      "567 - loss: 4431.105\n",
      "f_new: 4431.108 - f: 4431.105 - Backtracking...\n",
      "569 - loss: 4431.103\n",
      "570 - loss: 4431.103\n",
      "571 - loss: 4431.102\n",
      "572 - loss: 4431.091\n",
      "573 - loss: 4431.082\n",
      "574 - loss: 4431.079\n",
      "575 - loss: 4431.077\n",
      "576 - loss: 4431.077\n",
      "577 - loss: 4431.077\n",
      "578 - loss: 4431.076\n",
      "579 - loss: 4431.065\n",
      "580 - loss: 4431.054\n",
      "581 - loss: 4431.052\n",
      "582 - loss: 4431.049\n",
      "583 - loss: 4431.047\n",
      "584 - loss: 4431.047\n",
      "585 - loss: 4431.046\n",
      "586 - loss: 4431.046\n",
      "587 - loss: 4429.409\n",
      "588 - loss: 4428.748\n",
      "f_new: 4435.013 - f: 4428.748 - Backtracking...\n",
      "590 - loss: 4428.595\n",
      "591 - loss: 4428.578\n",
      "592 - loss: 4428.559\n",
      "593 - loss: 4427.957\n",
      "594 - loss: 4427.932\n",
      "f_new: 4428.104 - f: 4427.932 - Backtracking...\n",
      "596 - loss: 4427.866\n",
      "597 - loss: 4427.826\n",
      "598 - loss: 4427.813\n",
      "599 - loss: 4427.805\n",
      "600 - loss: 4427.750\n",
      "601 - loss: 4427.734\n",
      "602 - loss: 4427.729\n",
      "603 - loss: 4427.726\n",
      "604 - loss: 4427.725\n",
      "605 - loss: 4427.722\n",
      "606 - loss: 4427.592\n",
      "607 - loss: 4427.571\n",
      "608 - loss: 4427.564\n",
      "609 - loss: 4427.560\n",
      "610 - loss: 4427.559\n",
      "611 - loss: 4427.555\n",
      "612 - loss: 4427.550\n",
      "613 - loss: 4427.546\n",
      "614 - loss: 4427.542\n",
      "615 - loss: 4427.541\n",
      "616 - loss: 4427.540\n",
      "617 - loss: 4427.540\n",
      "618 - loss: 4427.540\n",
      "619 - loss: 4427.534\n",
      "620 - loss: 4427.527\n",
      "621 - loss: 4427.525\n",
      "622 - loss: 4427.522\n",
      "623 - loss: 4427.522\n",
      "624 - loss: 4427.522\n",
      "625 - loss: 4427.522\n",
      "626 - loss: 4427.505\n",
      "627 - loss: 4427.488\n",
      "628 - loss: 4427.480\n",
      "629 - loss: 4427.478\n",
      "630 - loss: 4427.476\n",
      "631 - loss: 4427.475\n",
      "632 - loss: 4427.475\n",
      "633 - loss: 4427.475\n",
      "634 - loss: 4425.864\n",
      "f_new: 4425.902 - f: 4425.864 - Backtracking...\n",
      "636 - loss: 4425.048\n",
      "f_new: 4431.087 - f: 4425.048 - Backtracking...\n",
      "638 - loss: 4424.960\n",
      "639 - loss: 4424.934\n",
      "640 - loss: 4424.909\n",
      "641 - loss: 4424.625\n",
      "642 - loss: 4424.465\n",
      "643 - loss: 4424.461\n",
      "644 - loss: 4424.348\n",
      "645 - loss: 4424.340\n",
      "646 - loss: 4424.338\n",
      "647 - loss: 4424.272\n",
      "648 - loss: 4424.254\n",
      "649 - loss: 4424.245\n",
      "f_new: 4424.256 - f: 4424.245 - Backtracking...\n",
      "651 - loss: 4424.240\n",
      "652 - loss: 4424.240\n",
      "653 - loss: 4424.239\n",
      "654 - loss: 4424.235\n",
      "655 - loss: 4424.230\n",
      "656 - loss: 4424.221\n",
      "f_new: 4424.222 - f: 4424.221 - Backtracking...\n",
      "658 - loss: 4424.220\n",
      "659 - loss: 4424.219\n",
      "660 - loss: 4424.219\n",
      "661 - loss: 4424.217\n",
      "662 - loss: 4424.201\n",
      "663 - loss: 4424.171\n",
      "f_new: 4424.175 - f: 4424.171 - Backtracking...\n",
      "665 - loss: 4424.170\n",
      "666 - loss: 4424.169\n",
      "667 - loss: 4424.169\n",
      "668 - loss: 4424.168\n",
      "669 - loss: 4424.166\n",
      "670 - loss: 4424.164\n",
      "671 - loss: 4424.135\n",
      "672 - loss: 4424.078\n",
      "673 - loss: 4424.078\n",
      "674 - loss: 4424.077\n",
      "675 - loss: 4424.077\n",
      "676 - loss: 4424.075\n",
      "677 - loss: 4424.073\n",
      "678 - loss: 4424.062\n",
      "679 - loss: 4424.050\n",
      "680 - loss: 4424.049\n",
      "681 - loss: 4424.049\n",
      "682 - loss: 4424.049\n",
      "683 - loss: 4424.035\n",
      "684 - loss: 4424.018\n",
      "685 - loss: 4424.016\n",
      "686 - loss: 4424.014\n",
      "687 - loss: 4424.012\n",
      "688 - loss: 4424.012\n",
      "689 - loss: 4424.012\n",
      "690 - loss: 4424.011\n",
      "691 - loss: 4423.206\n",
      "692 - loss: 4422.442\n",
      "f_new: 4422.453 - f: 4422.442 - Backtracking...\n",
      "694 - loss: 4422.425\n",
      "695 - loss: 4422.425\n",
      "696 - loss: 4422.424\n",
      "697 - loss: 4422.421\n",
      "698 - loss: 4422.419\n",
      "699 - loss: 4422.409\n",
      "700 - loss: 4422.408\n",
      "701 - loss: 4422.408\n",
      "702 - loss: 4422.408\n",
      "703 - loss: 4422.314\n",
      "704 - loss: 4422.227\n",
      "f_new: 4422.263 - f: 4422.227 - Backtracking...\n",
      "706 - loss: 4422.227\n",
      "707 - loss: 4422.227\n",
      "708 - loss: 4422.227\n",
      "709 - loss: 4422.224\n",
      "710 - loss: 4422.221\n",
      "711 - loss: 4422.216\n",
      "712 - loss: 4422.212\n",
      "713 - loss: 4422.211\n",
      "714 - loss: 4422.211\n",
      "715 - loss: 4422.211\n",
      "716 - loss: 4422.208\n",
      "717 - loss: 4422.205\n",
      "718 - loss: 4422.200\n",
      "719 - loss: 4422.192\n",
      "720 - loss: 4422.191\n",
      "721 - loss: 4422.191\n",
      "722 - loss: 4422.190\n",
      "723 - loss: 4422.190\n",
      "724 - loss: 4422.188\n",
      "725 - loss: 4422.185\n",
      "726 - loss: 4422.177\n",
      "727 - loss: 4422.161\n",
      "728 - loss: 4422.161\n",
      "729 - loss: 4422.161\n",
      "730 - loss: 4422.161\n",
      "731 - loss: 4422.159\n",
      "732 - loss: 4422.157\n",
      "733 - loss: 4422.108\n",
      "734 - loss: 4422.046\n",
      "735 - loss: 4422.043\n",
      "736 - loss: 4422.043\n",
      "737 - loss: 4422.042\n",
      "738 - loss: 4422.040\n",
      "739 - loss: 4422.038\n",
      "740 - loss: 4422.021\n",
      "741 - loss: 4421.875\n",
      "742 - loss: 4421.875\n",
      "743 - loss: 4421.873\n",
      "744 - loss: 4421.873\n",
      "745 - loss: 4421.873\n",
      "746 - loss: 4421.861\n",
      "747 - loss: 4421.847\n",
      "748 - loss: 4421.845\n",
      "749 - loss: 4421.843\n",
      "750 - loss: 4421.842\n",
      "751 - loss: 4421.842\n",
      "752 - loss: 4421.842\n",
      "753 - loss: 4421.841\n",
      "754 - loss: 4420.840\n",
      "755 - loss: 4420.055\n",
      "f_new: 4420.469 - f: 4420.055 - Backtracking...\n",
      "757 - loss: 4419.993\n",
      "758 - loss: 4419.989\n",
      "759 - loss: 4419.983\n",
      "760 - loss: 4419.956\n",
      "761 - loss: 4419.937\n",
      "762 - loss: 4419.890\n",
      "763 - loss: 4419.880\n",
      "764 - loss: 4419.874\n",
      "765 - loss: 4419.873\n",
      "766 - loss: 4419.797\n",
      "f_new: 4419.812 - f: 4419.797 - Backtracking...\n",
      "768 - loss: 4419.791\n",
      "769 - loss: 4419.787\n",
      "f_new: 4419.788 - f: 4419.787 - Backtracking...\n",
      "771 - loss: 4419.785\n",
      "772 - loss: 4419.784\n",
      "773 - loss: 4419.784\n",
      "774 - loss: 4419.756\n",
      "775 - loss: 4419.728\n",
      "776 - loss: 4419.725\n",
      "777 - loss: 4419.724\n",
      "778 - loss: 4419.724\n",
      "779 - loss: 4419.723\n",
      "780 - loss: 4419.723\n",
      "781 - loss: 4419.713\n",
      "782 - loss: 4419.701\n",
      "783 - loss: 4419.699\n",
      "784 - loss: 4419.697\n",
      "785 - loss: 4419.697\n",
      "786 - loss: 4419.697\n",
      "787 - loss: 4419.697\n",
      "788 - loss: 4418.609\n",
      "789 - loss: 4418.283\n",
      "f_new: 4419.368 - f: 4418.283 - Backtracking...\n",
      "791 - loss: 4417.996\n",
      "792 - loss: 4417.806\n",
      "793 - loss: 4417.744\n",
      "794 - loss: 4417.683\n",
      "795 - loss: 4417.422\n",
      "f_new: 4417.425 - f: 4417.422 - Backtracking...\n",
      "797 - loss: 4417.421\n",
      "798 - loss: 4417.420\n",
      "799 - loss: 4417.420\n",
      "800 - loss: 4417.419\n",
      "801 - loss: 4417.413\n",
      "802 - loss: 4417.404\n",
      "803 - loss: 4417.398\n",
      "804 - loss: 4417.397\n",
      "805 - loss: 4417.396\n",
      "806 - loss: 4417.396\n",
      "807 - loss: 4417.393\n",
      "808 - loss: 4417.389\n",
      "809 - loss: 4417.378\n",
      "810 - loss: 4417.346\n",
      "f_new: 4417.350 - f: 4417.346 - Backtracking...\n",
      "812 - loss: 4417.345\n",
      "813 - loss: 4417.345\n",
      "814 - loss: 4417.344\n",
      "815 - loss: 4417.342\n",
      "816 - loss: 4417.340\n",
      "817 - loss: 4417.337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "818 - loss: 4417.337\n",
      "819 - loss: 4417.337\n",
      "820 - loss: 4417.337\n",
      "821 - loss: 4417.236\n",
      "822 - loss: 4417.145\n",
      "f_new: 4417.199 - f: 4417.145 - Backtracking...\n",
      "824 - loss: 4417.145\n",
      "825 - loss: 4417.145\n",
      "826 - loss: 4417.145\n",
      "827 - loss: 4417.140\n",
      "828 - loss: 4417.134\n",
      "829 - loss: 4417.132\n",
      "830 - loss: 4417.130\n",
      "831 - loss: 4417.129\n",
      "832 - loss: 4417.128\n",
      "833 - loss: 4417.128\n",
      "834 - loss: 4417.128\n",
      "835 - loss: 4417.107\n",
      "836 - loss: 4417.087\n",
      "837 - loss: 4417.085\n",
      "838 - loss: 4417.085\n",
      "839 - loss: 4417.085\n",
      "840 - loss: 4417.084\n",
      "841 - loss: 4417.083\n",
      "842 - loss: 4417.081\n",
      "843 - loss: 4417.059\n",
      "844 - loss: 4416.973\n",
      "845 - loss: 4416.972\n",
      "846 - loss: 4416.972\n",
      "847 - loss: 4416.972\n",
      "848 - loss: 4416.971\n",
      "849 - loss: 4416.969\n",
      "850 - loss: 4416.967\n",
      "851 - loss: 4416.958\n",
      "852 - loss: 4416.943\n",
      "853 - loss: 4416.941\n",
      "854 - loss: 4416.941\n",
      "855 - loss: 4416.941\n",
      "856 - loss: 4416.941\n",
      "857 - loss: 4416.908\n",
      "858 - loss: 4416.869\n",
      "859 - loss: 4416.867\n",
      "860 - loss: 4416.866\n",
      "861 - loss: 4416.865\n",
      "862 - loss: 4416.865\n",
      "863 - loss: 4416.865\n",
      "864 - loss: 4416.724\n",
      "865 - loss: 4416.566\n",
      "866 - loss: 4416.555\n",
      "f_new: 4416.557 - f: 4416.555 - Backtracking...\n",
      "868 - loss: 4416.554\n",
      "869 - loss: 4416.554\n",
      "870 - loss: 4416.554\n",
      "871 - loss: 4416.552\n",
      "872 - loss: 4416.550\n",
      "873 - loss: 4416.540\n",
      "874 - loss: 4416.536\n",
      "875 - loss: 4416.536\n",
      "876 - loss: 4416.536\n",
      "877 - loss: 4416.536\n",
      "878 - loss: 4415.496\n",
      "879 - loss: 4415.033\n",
      "f_new: 4420.744 - f: 4415.033 - Backtracking...\n",
      "881 - loss: 4414.972\n",
      "882 - loss: 4414.933\n",
      "883 - loss: 4414.909\n",
      "884 - loss: 4414.745\n",
      "885 - loss: 4414.611\n",
      "f_new: 4414.659 - f: 4414.611 - Backtracking...\n",
      "887 - loss: 4414.592\n",
      "888 - loss: 4414.574\n",
      "889 - loss: 4414.569\n",
      "890 - loss: 4414.561\n",
      "891 - loss: 4414.383\n",
      "f_new: 4414.387 - f: 4414.383 - Backtracking...\n",
      "893 - loss: 4414.379\n",
      "f_new: 4414.385 - f: 4414.379 - Backtracking...\n",
      "895 - loss: 4414.379\n",
      "896 - loss: 4414.378\n",
      "897 - loss: 4414.378\n",
      "898 - loss: 4414.377\n",
      "899 - loss: 4414.375\n",
      "900 - loss: 4414.373\n",
      "901 - loss: 4414.373\n",
      "902 - loss: 4414.373\n",
      "903 - loss: 4414.373\n",
      "904 - loss: 4414.359\n",
      "905 - loss: 4414.345\n",
      "906 - loss: 4414.328\n",
      "907 - loss: 4414.327\n",
      "908 - loss: 4414.327\n",
      "909 - loss: 4414.327\n",
      "910 - loss: 4414.326\n",
      "911 - loss: 4414.324\n",
      "912 - loss: 4414.301\n",
      "913 - loss: 4414.275\n",
      "914 - loss: 4414.275\n",
      "915 - loss: 4414.275\n",
      "916 - loss: 4414.275\n",
      "917 - loss: 4414.268\n",
      "918 - loss: 4414.262\n",
      "919 - loss: 4414.260\n",
      "920 - loss: 4414.258\n",
      "921 - loss: 4414.256\n",
      "922 - loss: 4414.256\n",
      "923 - loss: 4414.256\n",
      "924 - loss: 4414.256\n",
      "925 - loss: 4414.037\n",
      "926 - loss: 4413.702\n",
      "f_new: 4413.762 - f: 4413.702 - Backtracking...\n",
      "928 - loss: 4413.701\n",
      "929 - loss: 4413.701\n",
      "930 - loss: 4413.701\n",
      "931 - loss: 4413.699\n",
      "932 - loss: 4413.696\n",
      "933 - loss: 4413.692\n",
      "934 - loss: 4413.692\n",
      "935 - loss: 4413.692\n",
      "936 - loss: 4413.692\n",
      "937 - loss: 4413.686\n",
      "938 - loss: 4413.679\n",
      "939 - loss: 4413.677\n",
      "940 - loss: 4413.676\n",
      "941 - loss: 4413.676\n",
      "942 - loss: 4413.675\n",
      "943 - loss: 4413.675\n",
      "944 - loss: 4413.657\n",
      "945 - loss: 4413.638\n",
      "946 - loss: 4413.633\n",
      "947 - loss: 4413.631\n",
      "948 - loss: 4413.631\n",
      "949 - loss: 4413.631\n",
      "950 - loss: 4413.631\n",
      "951 - loss: 4413.627\n",
      "952 - loss: 4413.622\n",
      "953 - loss: 4413.620\n",
      "954 - loss: 4413.618\n",
      "955 - loss: 4413.616\n",
      "956 - loss: 4413.615\n",
      "957 - loss: 4413.615\n",
      "958 - loss: 4413.615\n",
      "959 - loss: 4413.558\n",
      "960 - loss: 4413.493\n",
      "961 - loss: 4413.491\n",
      "962 - loss: 4413.491\n",
      "963 - loss: 4413.490\n",
      "964 - loss: 4413.490\n",
      "965 - loss: 4413.489\n",
      "966 - loss: 4413.487\n",
      "967 - loss: 4413.483\n",
      "968 - loss: 4413.475\n",
      "969 - loss: 4413.472\n",
      "970 - loss: 4413.472\n",
      "971 - loss: 4413.472\n",
      "972 - loss: 4413.472\n",
      "973 - loss: 4413.470\n",
      "974 - loss: 4413.468\n",
      "975 - loss: 4413.460\n",
      "976 - loss: 4413.416\n",
      "977 - loss: 4413.414\n",
      "978 - loss: 4413.414\n",
      "979 - loss: 4413.414\n",
      "980 - loss: 4413.414\n",
      "981 - loss: 4413.412\n",
      "982 - loss: 4413.410\n",
      "983 - loss: 4413.399\n",
      "984 - loss: 4413.258\n",
      "985 - loss: 4413.252\n",
      "f_new: 4413.252 - f: 4413.252 - Backtracking...\n",
      "987 - loss: 4413.252\n",
      "988 - loss: 4413.252\n",
      "989 - loss: 4413.251\n",
      "990 - loss: 4413.250\n",
      "991 - loss: 4413.249\n",
      "992 - loss: 4413.247\n",
      "993 - loss: 4413.247\n",
      "994 - loss: 4413.247\n",
      "995 - loss: 4413.247\n",
      "996 - loss: 4389.214\n",
      "f_new: 22935.718 - f: 4389.214 - Backtracking...\n",
      "f_new: 4389.214 - f: 4389.214 - Backtracking...\n",
      "999 - loss: 4389.213\n",
      "1000 - loss: 4389.213\n",
      "Reached maximum number of function evaluations 1000\n",
      "0.7283417326613871\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(verbose=True, max_evaluations=1000)\n",
    "logReg.fit(y_sub, np.c_[np.ones(X_sub.shape[0]), X_sub])\n",
    "pred = logReg.predict(np.c_[np.ones(X_sub.shape[0]), X_sub])\n",
    "print(compute_accuracy(pred, y_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_new: 388952141074152048493827070704946719003374518272.000 - f: 0.693 - Backtracking...\n",
      "f_new: 42948940344232256197308449618584789201601953792.000 - f: 0.693 - Backtracking...\n",
      "f_new: 4742515291465520316589241094131375336488697856.000 - f: 0.693 - Backtracking...\n",
      "f_new: 523678840723825601365942021904012656230006784.000 - f: 0.693 - Backtracking...\n",
      "f_new: 57825755188467729788181246775209813142929408.000 - f: 0.693 - Backtracking...\n",
      "f_new: 6385245503703762415557669590722242188148736.000 - f: 0.693 - Backtracking...\n",
      "f_new: 705072679287725974358492902368409655705600.000 - f: 0.693 - Backtracking...\n",
      "f_new: 77855656887368529251737837091182763573248.000 - f: 0.693 - Backtracking...\n",
      "f_new: 8596990760565376003751822352078876966912.000 - f: 0.693 - Backtracking...\n",
      "f_new: 949298395159228346089345013828158488576.000 - f: 0.693 - Backtracking...\n",
      "f_new: 104823590969245276918563331445266317312.000 - f: 0.693 - Backtracking...\n",
      "f_new: 11574848624751544723633017999918628864.000 - f: 0.693 - Backtracking...\n",
      "f_new: 1278119929369916136911791130408910848.000 - f: 0.693 - Backtracking...\n",
      "f_new: 141132779080955290663632729679069184.000 - f: 0.693 - Backtracking...\n",
      "f_new: 15584188051064248468229727084085248.000 - f: 0.693 - Backtracking...\n",
      "f_new: 1720839898374159823323440951066624.000 - f: 0.693 - Backtracking...\n",
      "f_new: 190018879785922593601210233651200.000 - f: 0.693 - Backtracking...\n",
      "f_new: 20982297486948518276197993414656.000 - f: 0.693 - Backtracking...\n",
      "f_new: 2316910868682124542541257768960.000 - f: 0.693 - Backtracking...\n",
      "f_new: 255838331181627255559634812928.000 - f: 0.693 - Backtracking...\n",
      "f_new: 28250224290686777644342575104.000 - f: 0.693 - Backtracking...\n",
      "f_new: 3119451134582065730238283776.000 - f: 0.693 - Backtracking...\n",
      "f_new: 344456570713080468250558464.000 - f: 0.693 - Backtracking...\n",
      "f_new: 38035642806538756811653120.000 - f: 0.693 - Backtracking...\n",
      "f_new: 4199978304120254887886848.000 - f: 0.693 - Backtracking...\n",
      "f_new: 463770728019571584794624.000 - f: 0.693 - Backtracking...\n",
      "f_new: 51210571244333019693056.000 - f: 0.693 - Backtracking...\n",
      "f_new: 5654782522324772978688.000 - f: 0.693 - Backtracking...\n",
      "f_new: 624413370087690076160.000 - f: 0.693 - Backtracking...\n",
      "f_new: 68949080748020621312.000 - f: 0.693 - Backtracking...\n",
      "f_new: 7613507275363817472.000 - f: 0.693 - Backtracking...\n",
      "f_new: 840700012286702080.000 - f: 0.693 - Backtracking...\n",
      "f_new: 92831921622493920.000 - f: 0.693 - Backtracking...\n",
      "f_new: 10250702445792234.000 - f: 0.693 - Backtracking...\n",
      "f_new: 1131904831825757.250 - f: 0.693 - Backtracking...\n",
      "f_new: 124987390384784.203 - f: 0.693 - Backtracking...\n",
      "f_new: 13801379158352.516 - f: 0.693 - Backtracking...\n",
      "f_new: 1523978267617.324 - f: 0.693 - Backtracking...\n",
      "f_new: 168280990872.210 - f: 0.693 - Backtracking...\n",
      "f_new: 18581952571.600 - f: 0.693 - Backtracking...\n",
      "f_new: 2051859568.882 - f: 0.693 - Backtracking...\n",
      "f_new: 226570790.942 - f: 0.693 - Backtracking...\n",
      "f_new: 25018438.979 - f: 0.693 - Backtracking...\n",
      "f_new: 2762590.440 - f: 0.693 - Backtracking...\n",
      "f_new: 305051.297 - f: 0.693 - Backtracking...\n",
      "f_new: 33684.487 - f: 0.693 - Backtracking...\n",
      "f_new: 3719.574 - f: 0.693 - Backtracking...\n",
      "f_new: 410.801 - f: 0.693 - Backtracking...\n",
      "f_new: 45.573 - f: 0.693 - Backtracking...\n",
      "f_new: 5.430 - f: 0.693 - Backtracking...\n",
      "f_new: 1.140 - f: 0.693 - Backtracking...\n",
      "f_new: 0.727 - f: 0.693 - Backtracking...\n",
      "f_new: 0.695 - f: 0.693 - Backtracking...\n",
      "54 - loss: 0.693\n",
      "55 - loss: 0.693\n",
      "56 - loss: 0.693\n",
      "57 - loss: 0.693\n",
      "58 - loss: 0.693\n",
      "59 - loss: 0.692\n",
      "f_new: 0.692 - f: 0.692 - Backtracking...\n",
      "61 - loss: 0.692\n",
      "62 - loss: 0.692\n",
      "63 - loss: 0.692\n",
      "64 - loss: 0.691\n",
      "65 - loss: 0.691\n",
      "f_new: 0.693 - f: 0.691 - Backtracking...\n",
      "67 - loss: 0.691\n",
      "68 - loss: 0.690\n",
      "69 - loss: 0.690\n",
      "70 - loss: 0.690\n",
      "f_new: 0.691 - f: 0.690 - Backtracking...\n",
      "72 - loss: 0.689\n",
      "73 - loss: 0.689\n",
      "74 - loss: 0.689\n",
      "75 - loss: 0.689\n",
      "76 - loss: 0.689\n",
      "77 - loss: 0.686\n",
      "f_new: 0.706 - f: 0.686 - Backtracking...\n",
      "79 - loss: 0.686\n",
      "80 - loss: 0.686\n",
      "81 - loss: 0.686\n",
      "82 - loss: 0.686\n",
      "83 - loss: 0.685\n",
      "f_new: 0.698 - f: 0.685 - Backtracking...\n",
      "85 - loss: 0.685\n",
      "86 - loss: 0.685\n",
      "87 - loss: 0.685\n",
      "88 - loss: 0.685\n",
      "89 - loss: 0.684\n",
      "f_new: 0.687 - f: 0.684 - Backtracking...\n",
      "91 - loss: 0.684\n",
      "92 - loss: 0.684\n",
      "93 - loss: 0.684\n",
      "94 - loss: 0.684\n",
      "95 - loss: 0.684\n",
      "96 - loss: 0.683\n",
      "f_new: 0.686 - f: 0.683 - Backtracking...\n",
      "98 - loss: 0.683\n",
      "99 - loss: 0.683\n",
      "100 - loss: 0.683\n",
      "101 - loss: 0.683\n",
      "102 - loss: 0.683\n",
      "103 - loss: 0.683\n",
      "104 - loss: 0.681\n",
      "f_new: 0.683 - f: 0.681 - Backtracking...\n",
      "106 - loss: 0.681\n",
      "107 - loss: 0.681\n",
      "108 - loss: 0.681\n",
      "109 - loss: 0.681\n",
      "110 - loss: 0.679\n",
      "f_new: 0.748 - f: 0.679 - Backtracking...\n",
      "112 - loss: 0.679\n",
      "113 - loss: 0.679\n",
      "114 - loss: 0.679\n",
      "115 - loss: 0.679\n",
      "116 - loss: 0.679\n",
      "f_new: 0.679 - f: 0.679 - Backtracking...\n",
      "118 - loss: 0.679\n",
      "119 - loss: 0.679\n",
      "120 - loss: 0.679\n",
      "121 - loss: 0.679\n",
      "122 - loss: 0.679\n",
      "123 - loss: 0.678\n",
      "f_new: 0.684 - f: 0.678 - Backtracking...\n",
      "125 - loss: 0.678\n",
      "126 - loss: 0.678\n",
      "127 - loss: 0.678\n",
      "128 - loss: 0.678\n",
      "129 - loss: 0.678\n",
      "130 - loss: 0.678\n",
      "f_new: 0.678 - f: 0.678 - Backtracking...\n",
      "132 - loss: 0.678\n",
      "133 - loss: 0.678\n",
      "134 - loss: 0.678\n",
      "135 - loss: 0.678\n",
      "136 - loss: 0.678\n",
      "137 - loss: 0.677\n",
      "f_new: 0.678 - f: 0.677 - Backtracking...\n",
      "139 - loss: 0.677\n",
      "140 - loss: 0.677\n",
      "141 - loss: 0.677\n",
      "142 - loss: 0.677\n",
      "143 - loss: 0.677\n",
      "f_new: 0.677 - f: 0.677 - Backtracking...\n",
      "145 - loss: 0.677\n",
      "146 - loss: 0.677\n",
      "147 - loss: 0.677\n",
      "148 - loss: 0.677\n",
      "149 - loss: 0.677\n",
      "150 - loss: 0.677\n",
      "f_new: 0.677 - f: 0.677 - Backtracking...\n",
      "152 - loss: 0.677\n",
      "153 - loss: 0.676\n",
      "154 - loss: 0.676\n",
      "155 - loss: 0.676\n",
      "156 - loss: 0.676\n",
      "157 - loss: 0.676\n",
      "158 - loss: 0.676\n",
      "159 - loss: 0.676\n",
      "160 - loss: 0.676\n",
      "161 - loss: 0.676\n",
      "162 - loss: 0.676\n",
      "163 - loss: 0.675\n",
      "f_new: 0.683 - f: 0.675 - Backtracking...\n",
      "165 - loss: 0.675\n",
      "166 - loss: 0.675\n",
      "167 - loss: 0.675\n",
      "168 - loss: 0.675\n",
      "169 - loss: 0.675\n",
      "170 - loss: 0.666\n",
      "f_new: 0.886 - f: 0.666 - Backtracking...\n",
      "f_new: 0.666 - f: 0.666 - Backtracking...\n",
      "173 - loss: 0.666\n",
      "174 - loss: 0.666\n",
      "175 - loss: 0.666\n",
      "176 - loss: 0.666\n",
      "177 - loss: 0.666\n",
      "f_new: 0.667 - f: 0.666 - Backtracking...\n",
      "179 - loss: 0.666\n",
      "180 - loss: 0.666\n",
      "181 - loss: 0.666\n",
      "182 - loss: 0.666\n",
      "f_new: 0.666 - f: 0.666 - Backtracking...\n",
      "184 - loss: 0.666\n",
      "185 - loss: 0.666\n",
      "186 - loss: 0.666\n",
      "187 - loss: 0.666\n",
      "188 - loss: 0.666\n",
      "189 - loss: 0.666\n",
      "190 - loss: 0.666\n",
      "191 - loss: 0.666\n",
      "192 - loss: 0.666\n",
      "193 - loss: 0.666\n",
      "194 - loss: 0.665\n",
      "195 - loss: 0.665\n",
      "196 - loss: 0.665\n",
      "197 - loss: 0.665\n",
      "198 - loss: 0.665\n",
      "199 - loss: 0.665\n",
      "200 - loss: 0.664\n",
      "f_new: 0.721 - f: 0.664 - Backtracking...\n",
      "202 - loss: 0.664\n",
      "203 - loss: 0.664\n",
      "204 - loss: 0.664\n",
      "205 - loss: 0.664\n",
      "206 - loss: 0.664\n",
      "207 - loss: 0.664\n",
      "208 - loss: 0.664\n",
      "209 - loss: 0.664\n",
      "210 - loss: 0.664\n",
      "211 - loss: 0.664\n",
      "212 - loss: 0.664\n",
      "213 - loss: 0.664\n",
      "214 - loss: 0.664\n",
      "215 - loss: 0.664\n",
      "216 - loss: 0.664\n",
      "217 - loss: 0.664\n",
      "218 - loss: 0.663\n",
      "f_new: 0.682 - f: 0.663 - Backtracking...\n",
      "220 - loss: 0.663\n",
      "221 - loss: 0.663\n",
      "222 - loss: 0.663\n",
      "223 - loss: 0.663\n",
      "224 - loss: 0.663\n",
      "f_new: 0.663 - f: 0.663 - Backtracking...\n",
      "226 - loss: 0.663\n",
      "227 - loss: 0.663\n",
      "228 - loss: 0.663\n",
      "229 - loss: 0.663\n",
      "230 - loss: 0.662\n",
      "f_new: 0.663 - f: 0.662 - Backtracking...\n",
      "232 - loss: 0.662\n",
      "233 - loss: 0.662\n",
      "234 - loss: 0.662\n",
      "235 - loss: 0.662\n",
      "236 - loss: 0.662\n",
      "237 - loss: 0.662\n",
      "f_new: 0.663 - f: 0.662 - Backtracking...\n",
      "239 - loss: 0.662\n",
      "240 - loss: 0.662\n",
      "241 - loss: 0.662\n",
      "242 - loss: 0.662\n",
      "243 - loss: 0.662\n",
      "244 - loss: 0.662\n",
      "245 - loss: 0.662\n",
      "246 - loss: 0.662\n",
      "247 - loss: 0.662\n",
      "248 - loss: 0.662\n",
      "249 - loss: 0.662\n",
      "250 - loss: 0.662\n",
      "251 - loss: 0.662\n",
      "252 - loss: 0.662\n",
      "253 - loss: 0.662\n",
      "254 - loss: 0.662\n",
      "255 - loss: 0.662\n",
      "256 - loss: 0.662\n",
      "f_new: 0.662 - f: 0.662 - Backtracking...\n",
      "258 - loss: 0.662\n",
      "259 - loss: 0.662\n",
      "260 - loss: 0.662\n",
      "261 - loss: 0.662\n",
      "262 - loss: 0.662\n",
      "263 - loss: 0.662\n",
      "264 - loss: 0.662\n",
      "265 - loss: 0.662\n",
      "266 - loss: 0.662\n",
      "267 - loss: 0.662\n",
      "268 - loss: 0.661\n",
      "f_new: 0.674 - f: 0.661 - Backtracking...\n",
      "270 - loss: 0.661\n",
      "271 - loss: 0.661\n",
      "272 - loss: 0.661\n",
      "273 - loss: 0.661\n",
      "274 - loss: 0.661\n",
      "275 - loss: 0.661\n",
      "276 - loss: 0.661\n",
      "277 - loss: 0.661\n",
      "278 - loss: 0.661\n",
      "279 - loss: 0.661\n",
      "280 - loss: 0.661\n",
      "281 - loss: 0.661\n",
      "282 - loss: 0.661\n",
      "f_new: 0.661 - f: 0.661 - Backtracking...\n",
      "284 - loss: 0.661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 - loss: 0.661\n",
      "286 - loss: 0.661\n",
      "287 - loss: 0.661\n",
      "288 - loss: 0.661\n",
      "f_new: 0.661 - f: 0.661 - Backtracking...\n",
      "290 - loss: 0.661\n",
      "291 - loss: 0.661\n",
      "292 - loss: 0.661\n",
      "293 - loss: 0.661\n",
      "294 - loss: 0.661\n",
      "f_new: 0.661 - f: 0.661 - Backtracking...\n",
      "296 - loss: 0.661\n",
      "297 - loss: 0.661\n",
      "298 - loss: 0.661\n",
      "299 - loss: 0.661\n",
      "300 - loss: 0.660\n",
      "301 - loss: 0.660\n",
      "302 - loss: 0.660\n",
      "303 - loss: 0.660\n",
      "304 - loss: 0.660\n",
      "305 - loss: 0.660\n",
      "306 - loss: 0.660\n",
      "307 - loss: 0.660\n",
      "308 - loss: 0.660\n",
      "309 - loss: 0.660\n",
      "310 - loss: 0.660\n",
      "311 - loss: 0.660\n",
      "312 - loss: 0.660\n",
      "313 - loss: 0.660\n",
      "314 - loss: 0.660\n",
      "315 - loss: 0.660\n",
      "316 - loss: 0.660\n",
      "317 - loss: 0.660\n",
      "318 - loss: 0.660\n",
      "319 - loss: 0.660\n",
      "320 - loss: 0.660\n",
      "321 - loss: 0.660\n",
      "322 - loss: 0.660\n",
      "323 - loss: 0.660\n",
      "324 - loss: 0.660\n",
      "f_new: 0.660 - f: 0.660 - Backtracking...\n",
      "326 - loss: 0.660\n",
      "327 - loss: 0.660\n",
      "328 - loss: 0.660\n",
      "329 - loss: 0.660\n",
      "330 - loss: 0.660\n",
      "331 - loss: 0.660\n",
      "332 - loss: 0.660\n",
      "333 - loss: 0.660\n",
      "334 - loss: 0.660\n",
      "335 - loss: 0.660\n",
      "336 - loss: 0.660\n",
      "f_new: 0.661 - f: 0.660 - Backtracking...\n",
      "338 - loss: 0.660\n",
      "339 - loss: 0.660\n",
      "340 - loss: 0.660\n",
      "341 - loss: 0.660\n",
      "342 - loss: 0.660\n",
      "343 - loss: 0.660\n",
      "344 - loss: 0.660\n",
      "f_new: 0.660 - f: 0.660 - Backtracking...\n",
      "346 - loss: 0.660\n",
      "347 - loss: 0.660\n",
      "348 - loss: 0.660\n",
      "349 - loss: 0.660\n",
      "350 - loss: 0.660\n",
      "351 - loss: 0.660\n",
      "352 - loss: 0.660\n",
      "353 - loss: 0.660\n",
      "354 - loss: 0.660\n",
      "355 - loss: 0.660\n",
      "356 - loss: 0.658\n",
      "f_new: 0.806 - f: 0.658 - Backtracking...\n",
      "f_new: 0.658 - f: 0.658 - Backtracking...\n",
      "359 - loss: 0.658\n",
      "360 - loss: 0.658\n",
      "361 - loss: 0.658\n",
      "362 - loss: 0.658\n",
      "363 - loss: 0.658\n",
      "f_new: 0.658 - f: 0.658 - Backtracking...\n",
      "365 - loss: 0.658\n",
      "366 - loss: 0.658\n",
      "367 - loss: 0.658\n",
      "368 - loss: 0.658\n",
      "369 - loss: 0.658\n",
      "370 - loss: 0.658\n",
      "371 - loss: 0.658\n",
      "372 - loss: 0.658\n",
      "373 - loss: 0.658\n",
      "374 - loss: 0.658\n",
      "375 - loss: 0.658\n",
      "376 - loss: 0.658\n",
      "377 - loss: 0.658\n",
      "378 - loss: 0.658\n",
      "379 - loss: 0.658\n",
      "380 - loss: 0.658\n",
      "381 - loss: 0.658\n",
      "382 - loss: 0.656\n",
      "f_new: 1.061 - f: 0.656 - Backtracking...\n",
      "f_new: 0.656 - f: 0.656 - Backtracking...\n",
      "385 - loss: 0.656\n",
      "386 - loss: 0.656\n",
      "387 - loss: 0.656\n",
      "388 - loss: 0.656\n",
      "389 - loss: 0.656\n",
      "390 - loss: 0.656\n",
      "391 - loss: 0.656\n",
      "392 - loss: 0.656\n",
      "393 - loss: 0.656\n",
      "394 - loss: 0.656\n",
      "395 - loss: 0.656\n",
      "396 - loss: 0.656\n",
      "397 - loss: 0.656\n",
      "398 - loss: 0.656\n",
      "399 - loss: 0.656\n",
      "400 - loss: 0.656\n",
      "401 - loss: 0.656\n",
      "402 - loss: 0.656\n",
      "403 - loss: 0.656\n",
      "404 - loss: 0.656\n",
      "405 - loss: 0.656\n",
      "406 - loss: 0.656\n",
      "f_new: 0.662 - f: 0.656 - Backtracking...\n",
      "408 - loss: 0.656\n",
      "409 - loss: 0.656\n",
      "410 - loss: 0.656\n",
      "411 - loss: 0.656\n",
      "412 - loss: 0.656\n",
      "413 - loss: 0.656\n",
      "f_new: 0.656 - f: 0.656 - Backtracking...\n",
      "415 - loss: 0.656\n",
      "416 - loss: 0.656\n",
      "417 - loss: 0.656\n",
      "418 - loss: 0.656\n",
      "419 - loss: 0.656\n",
      "420 - loss: 0.656\n",
      "421 - loss: 0.656\n",
      "422 - loss: 0.656\n",
      "423 - loss: 0.656\n",
      "424 - loss: 0.656\n",
      "425 - loss: 0.656\n",
      "426 - loss: 0.656\n",
      "427 - loss: 0.656\n",
      "428 - loss: 0.656\n",
      "429 - loss: 0.656\n",
      "430 - loss: 0.656\n",
      "431 - loss: 0.654\n",
      "f_new: 1.052 - f: 0.654 - Backtracking...\n",
      "f_new: 0.654 - f: 0.654 - Backtracking...\n",
      "434 - loss: 0.654\n",
      "435 - loss: 0.654\n",
      "436 - loss: 0.654\n",
      "437 - loss: 0.654\n",
      "438 - loss: 0.654\n",
      "439 - loss: 0.654\n",
      "440 - loss: 0.654\n",
      "441 - loss: 0.654\n",
      "442 - loss: 0.654\n",
      "443 - loss: 0.654\n",
      "444 - loss: 0.654\n",
      "445 - loss: 0.654\n",
      "446 - loss: 0.654\n",
      "447 - loss: 0.654\n",
      "448 - loss: 0.654\n",
      "449 - loss: 0.654\n",
      "450 - loss: 0.654\n",
      "451 - loss: 0.654\n",
      "452 - loss: 0.654\n",
      "453 - loss: 0.654\n",
      "454 - loss: 0.654\n",
      "455 - loss: 0.654\n",
      "456 - loss: 0.654\n",
      "f_new: 0.654 - f: 0.654 - Backtracking...\n",
      "458 - loss: 0.654\n",
      "459 - loss: 0.654\n",
      "460 - loss: 0.654\n",
      "461 - loss: 0.654\n",
      "462 - loss: 0.654\n",
      "463 - loss: 0.654\n",
      "f_new: 0.654 - f: 0.654 - Backtracking...\n",
      "465 - loss: 0.654\n",
      "466 - loss: 0.654\n",
      "467 - loss: 0.654\n",
      "468 - loss: 0.654\n",
      "469 - loss: 0.653\n",
      "470 - loss: 0.653\n",
      "471 - loss: 0.653\n",
      "472 - loss: 0.653\n",
      "473 - loss: 0.653\n",
      "474 - loss: 0.653\n",
      "475 - loss: 0.653\n",
      "476 - loss: 0.653\n",
      "477 - loss: 0.653\n",
      "478 - loss: 0.653\n",
      "479 - loss: 0.653\n",
      "480 - loss: 0.653\n",
      "481 - loss: 0.653\n",
      "482 - loss: 0.653\n",
      "483 - loss: 0.653\n",
      "484 - loss: 0.653\n",
      "485 - loss: 0.653\n",
      "486 - loss: 0.653\n",
      "487 - loss: 0.653\n",
      "488 - loss: 0.653\n",
      "489 - loss: 0.653\n",
      "490 - loss: 0.653\n",
      "491 - loss: 0.653\n",
      "492 - loss: 0.653\n",
      "493 - loss: 0.653\n",
      "494 - loss: 0.653\n",
      "495 - loss: 0.653\n",
      "496 - loss: 0.653\n",
      "497 - loss: 0.653\n",
      "498 - loss: 0.653\n",
      "499 - loss: 0.653\n",
      "500 - loss: 0.653\n",
      "501 - loss: 0.653\n",
      "502 - loss: 0.653\n",
      "503 - loss: 0.653\n",
      "504 - loss: 0.653\n",
      "505 - loss: 0.653\n",
      "506 - loss: 0.653\n",
      "507 - loss: 0.653\n",
      "508 - loss: 0.653\n",
      "509 - loss: 0.653\n",
      "510 - loss: 0.653\n",
      "511 - loss: 0.653\n",
      "512 - loss: 0.653\n",
      "513 - loss: 0.653\n",
      "514 - loss: 0.653\n",
      "515 - loss: 0.653\n",
      "516 - loss: 0.653\n",
      "517 - loss: 0.653\n",
      "518 - loss: 0.653\n",
      "519 - loss: 0.653\n",
      "520 - loss: 0.653\n",
      "521 - loss: 0.653\n",
      "522 - loss: 0.653\n",
      "523 - loss: 0.653\n",
      "f_new: 0.653 - f: 0.653 - Backtracking...\n",
      "525 - loss: 0.653\n",
      "526 - loss: 0.653\n",
      "527 - loss: 0.653\n",
      "528 - loss: 0.653\n",
      "529 - loss: 0.653\n",
      "530 - loss: 0.653\n",
      "531 - loss: 0.653\n",
      "532 - loss: 0.653\n",
      "533 - loss: 0.653\n",
      "534 - loss: 0.653\n",
      "535 - loss: 0.653\n",
      "536 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "538 - loss: 0.652\n",
      "539 - loss: 0.652\n",
      "540 - loss: 0.652\n",
      "541 - loss: 0.652\n",
      "542 - loss: 0.652\n",
      "543 - loss: 0.652\n",
      "544 - loss: 0.652\n",
      "545 - loss: 0.652\n",
      "546 - loss: 0.652\n",
      "547 - loss: 0.652\n",
      "548 - loss: 0.652\n",
      "549 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "551 - loss: 0.652\n",
      "552 - loss: 0.652\n",
      "553 - loss: 0.652\n",
      "554 - loss: 0.652\n",
      "555 - loss: 0.652\n",
      "556 - loss: 0.652\n",
      "557 - loss: 0.652\n",
      "558 - loss: 0.652\n",
      "559 - loss: 0.652\n",
      "560 - loss: 0.652\n",
      "561 - loss: 0.652\n",
      "562 - loss: 0.652\n",
      "563 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "565 - loss: 0.652\n",
      "566 - loss: 0.652\n",
      "567 - loss: 0.652\n",
      "568 - loss: 0.652\n",
      "569 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "571 - loss: 0.652\n",
      "572 - loss: 0.652\n",
      "573 - loss: 0.652\n",
      "574 - loss: 0.652\n",
      "575 - loss: 0.652\n",
      "576 - loss: 0.652\n",
      "577 - loss: 0.652\n",
      "578 - loss: 0.652\n",
      "579 - loss: 0.652\n",
      "580 - loss: 0.652\n",
      "581 - loss: 0.652\n",
      "582 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "584 - loss: 0.652\n",
      "585 - loss: 0.652\n",
      "586 - loss: 0.652\n",
      "587 - loss: 0.652\n",
      "588 - loss: 0.652\n",
      "589 - loss: 0.652\n",
      "590 - loss: 0.652\n",
      "591 - loss: 0.652\n",
      "592 - loss: 0.652\n",
      "593 - loss: 0.652\n",
      "594 - loss: 0.651\n",
      "f_new: 0.672 - f: 0.651 - Backtracking...\n",
      "596 - loss: 0.651\n",
      "597 - loss: 0.651\n",
      "598 - loss: 0.651\n",
      "599 - loss: 0.651\n",
      "600 - loss: 0.651\n",
      "601 - loss: 0.651\n",
      "f_new: 0.651 - f: 0.651 - Backtracking...\n",
      "603 - loss: 0.651\n",
      "604 - loss: 0.651\n",
      "605 - loss: 0.651\n",
      "606 - loss: 0.651\n",
      "607 - loss: 0.651\n",
      "608 - loss: 0.651\n",
      "f_new: 0.651 - f: 0.651 - Backtracking...\n",
      "610 - loss: 0.651\n",
      "611 - loss: 0.651\n",
      "612 - loss: 0.651\n",
      "613 - loss: 0.651\n",
      "614 - loss: 0.651\n",
      "f_new: 0.651 - f: 0.651 - Backtracking...\n",
      "616 - loss: 0.651\n",
      "617 - loss: 0.651\n",
      "618 - loss: 0.651\n",
      "619 - loss: 0.651\n",
      "620 - loss: 0.651\n",
      "621 - loss: 0.651\n",
      "622 - loss: 0.651\n",
      "623 - loss: 0.651\n",
      "624 - loss: 0.651\n",
      "625 - loss: 0.651\n",
      "626 - loss: 0.651\n",
      "627 - loss: 0.651\n",
      "628 - loss: 0.651\n",
      "629 - loss: 0.651\n",
      "630 - loss: 0.651\n",
      "631 - loss: 0.651\n",
      "632 - loss: 0.651\n",
      "633 - loss: 0.651\n",
      "634 - loss: 0.651\n",
      "635 - loss: 0.651\n",
      "636 - loss: 0.651\n",
      "637 - loss: 0.650\n",
      "f_new: 0.738 - f: 0.650 - Backtracking...\n",
      "f_new: 0.650 - f: 0.650 - Backtracking...\n",
      "640 - loss: 0.650\n",
      "641 - loss: 0.650\n",
      "642 - loss: 0.650\n",
      "643 - loss: 0.650\n",
      "644 - loss: 0.650\n",
      "645 - loss: 0.650\n",
      "646 - loss: 0.650\n",
      "647 - loss: 0.650\n",
      "648 - loss: 0.650\n",
      "649 - loss: 0.650\n",
      "650 - loss: 0.650\n",
      "f_new: 0.650 - f: 0.650 - Backtracking...\n",
      "652 - loss: 0.650\n",
      "653 - loss: 0.650\n",
      "654 - loss: 0.650\n",
      "655 - loss: 0.650\n",
      "656 - loss: 0.650\n",
      "657 - loss: 0.650\n",
      "658 - loss: 0.650\n",
      "f_new: 0.650 - f: 0.650 - Backtracking...\n",
      "660 - loss: 0.650\n",
      "661 - loss: 0.650\n",
      "662 - loss: 0.650\n",
      "663 - loss: 0.650\n",
      "664 - loss: 0.650\n",
      "665 - loss: 0.650\n",
      "666 - loss: 0.650\n",
      "667 - loss: 0.650\n",
      "668 - loss: 0.650\n",
      "669 - loss: 0.650\n",
      "670 - loss: 0.649\n",
      "f_new: 0.651 - f: 0.649 - Backtracking...\n",
      "672 - loss: 0.649\n",
      "673 - loss: 0.649\n",
      "674 - loss: 0.649\n",
      "675 - loss: 0.649\n",
      "676 - loss: 0.649\n",
      "677 - loss: 0.649\n",
      "f_new: 0.649 - f: 0.649 - Backtracking...\n",
      "679 - loss: 0.649\n",
      "680 - loss: 0.649\n",
      "681 - loss: 0.649\n",
      "682 - loss: 0.649\n",
      "683 - loss: 0.649\n",
      "684 - loss: 0.649\n",
      "685 - loss: 0.649\n",
      "686 - loss: 0.649\n",
      "687 - loss: 0.649\n",
      "688 - loss: 0.649\n",
      "689 - loss: 0.649\n",
      "690 - loss: 0.649\n",
      "691 - loss: 0.649\n",
      "692 - loss: 0.649\n",
      "693 - loss: 0.649\n",
      "694 - loss: 0.649\n",
      "695 - loss: 0.649\n",
      "f_new: 0.656 - f: 0.649 - Backtracking...\n",
      "697 - loss: 0.649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698 - loss: 0.649\n",
      "699 - loss: 0.649\n",
      "700 - loss: 0.649\n",
      "701 - loss: 0.649\n",
      "f_new: 0.649 - f: 0.649 - Backtracking...\n",
      "703 - loss: 0.649\n",
      "704 - loss: 0.649\n",
      "705 - loss: 0.649\n",
      "706 - loss: 0.649\n",
      "707 - loss: 0.649\n",
      "708 - loss: 0.649\n",
      "709 - loss: 0.649\n",
      "710 - loss: 0.649\n",
      "711 - loss: 0.649\n",
      "712 - loss: 0.649\n",
      "713 - loss: 0.649\n",
      "714 - loss: 0.649\n",
      "715 - loss: 0.649\n",
      "716 - loss: 0.649\n",
      "717 - loss: 0.649\n",
      "718 - loss: 0.649\n",
      "719 - loss: 0.649\n",
      "f_new: 0.649 - f: 0.649 - Backtracking...\n",
      "721 - loss: 0.649\n",
      "722 - loss: 0.649\n",
      "723 - loss: 0.649\n",
      "724 - loss: 0.649\n",
      "725 - loss: 0.649\n",
      "726 - loss: 0.649\n",
      "727 - loss: 0.649\n",
      "728 - loss: 0.649\n",
      "729 - loss: 0.649\n",
      "730 - loss: 0.649\n",
      "731 - loss: 0.649\n",
      "732 - loss: 0.647\n",
      "f_new: 0.663 - f: 0.647 - Backtracking...\n",
      "734 - loss: 0.647\n",
      "735 - loss: 0.647\n",
      "736 - loss: 0.647\n",
      "737 - loss: 0.647\n",
      "738 - loss: 0.647\n",
      "739 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "741 - loss: 0.647\n",
      "742 - loss: 0.647\n",
      "743 - loss: 0.647\n",
      "744 - loss: 0.647\n",
      "745 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "747 - loss: 0.647\n",
      "748 - loss: 0.647\n",
      "749 - loss: 0.647\n",
      "750 - loss: 0.647\n",
      "751 - loss: 0.647\n",
      "752 - loss: 0.647\n",
      "753 - loss: 0.647\n",
      "754 - loss: 0.647\n",
      "755 - loss: 0.647\n",
      "756 - loss: 0.647\n",
      "757 - loss: 0.647\n",
      "758 - loss: 0.647\n",
      "759 - loss: 0.647\n",
      "760 - loss: 0.647\n",
      "761 - loss: 0.647\n",
      "762 - loss: 0.647\n",
      "763 - loss: 0.647\n",
      "764 - loss: 0.647\n",
      "765 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "767 - loss: 0.647\n",
      "768 - loss: 0.647\n",
      "769 - loss: 0.647\n",
      "770 - loss: 0.647\n",
      "771 - loss: 0.647\n",
      "772 - loss: 0.647\n",
      "773 - loss: 0.647\n",
      "774 - loss: 0.647\n",
      "775 - loss: 0.647\n",
      "776 - loss: 0.647\n",
      "777 - loss: 0.647\n",
      "778 - loss: 0.647\n",
      "779 - loss: 0.647\n",
      "780 - loss: 0.647\n",
      "781 - loss: 0.647\n",
      "782 - loss: 0.647\n",
      "783 - loss: 0.647\n",
      "784 - loss: 0.647\n",
      "785 - loss: 0.647\n",
      "786 - loss: 0.647\n",
      "787 - loss: 0.647\n",
      "788 - loss: 0.647\n",
      "789 - loss: 0.647\n",
      "790 - loss: 0.647\n",
      "791 - loss: 0.647\n",
      "792 - loss: 0.647\n",
      "793 - loss: 0.647\n",
      "794 - loss: 0.647\n",
      "795 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "797 - loss: 0.647\n",
      "798 - loss: 0.647\n",
      "799 - loss: 0.647\n",
      "800 - loss: 0.647\n",
      "801 - loss: 0.647\n",
      "802 - loss: 0.647\n",
      "803 - loss: 0.647\n",
      "804 - loss: 0.647\n",
      "805 - loss: 0.647\n",
      "806 - loss: 0.647\n",
      "807 - loss: 0.647\n",
      "808 - loss: 0.647\n",
      "809 - loss: 0.647\n",
      "810 - loss: 0.647\n",
      "811 - loss: 0.647\n",
      "812 - loss: 0.647\n",
      "813 - loss: 0.647\n",
      "814 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "816 - loss: 0.647\n",
      "817 - loss: 0.647\n",
      "818 - loss: 0.647\n",
      "819 - loss: 0.647\n",
      "820 - loss: 0.647\n",
      "821 - loss: 0.647\n",
      "822 - loss: 0.647\n",
      "823 - loss: 0.647\n",
      "824 - loss: 0.647\n",
      "825 - loss: 0.647\n",
      "826 - loss: 0.647\n",
      "827 - loss: 0.647\n",
      "828 - loss: 0.647\n",
      "829 - loss: 0.647\n",
      "830 - loss: 0.647\n",
      "831 - loss: 0.647\n",
      "832 - loss: 0.646\n",
      "f_new: 0.647 - f: 0.646 - Backtracking...\n",
      "834 - loss: 0.646\n",
      "835 - loss: 0.646\n",
      "836 - loss: 0.646\n",
      "837 - loss: 0.646\n",
      "838 - loss: 0.646\n",
      "839 - loss: 0.646\n",
      "840 - loss: 0.646\n",
      "841 - loss: 0.646\n",
      "842 - loss: 0.646\n",
      "843 - loss: 0.646\n",
      "844 - loss: 0.646\n",
      "f_new: 0.646 - f: 0.646 - Backtracking...\n",
      "846 - loss: 0.646\n",
      "847 - loss: 0.646\n",
      "848 - loss: 0.646\n",
      "849 - loss: 0.646\n",
      "850 - loss: 0.646\n",
      "851 - loss: 0.646\n",
      "852 - loss: 0.646\n",
      "853 - loss: 0.646\n",
      "854 - loss: 0.646\n",
      "855 - loss: 0.646\n",
      "856 - loss: 0.646\n",
      "857 - loss: 0.646\n",
      "858 - loss: 0.646\n",
      "859 - loss: 0.646\n",
      "860 - loss: 0.646\n",
      "861 - loss: 0.646\n",
      "862 - loss: 0.646\n",
      "f_new: 0.646 - f: 0.646 - Backtracking...\n",
      "864 - loss: 0.646\n",
      "865 - loss: 0.646\n",
      "866 - loss: 0.646\n",
      "867 - loss: 0.646\n",
      "868 - loss: 0.646\n",
      "869 - loss: 0.646\n",
      "870 - loss: 0.646\n",
      "871 - loss: 0.646\n",
      "872 - loss: 0.646\n",
      "873 - loss: 0.646\n",
      "874 - loss: 0.646\n",
      "875 - loss: 0.646\n",
      "876 - loss: 0.646\n",
      "877 - loss: 0.646\n",
      "878 - loss: 0.646\n",
      "879 - loss: 0.646\n",
      "880 - loss: 0.646\n",
      "881 - loss: 0.646\n",
      "882 - loss: 0.646\n",
      "883 - loss: 0.646\n",
      "884 - loss: 0.646\n",
      "885 - loss: 0.646\n",
      "886 - loss: 0.646\n",
      "887 - loss: 0.646\n",
      "888 - loss: 0.646\n",
      "889 - loss: 0.646\n",
      "890 - loss: 0.646\n",
      "891 - loss: 0.646\n",
      "892 - loss: 0.646\n",
      "893 - loss: 0.646\n",
      "894 - loss: 0.646\n",
      "895 - loss: 0.646\n",
      "896 - loss: 0.646\n",
      "897 - loss: 0.646\n",
      "898 - loss: 0.646\n",
      "899 - loss: 0.646\n",
      "900 - loss: 0.646\n",
      "901 - loss: 0.646\n",
      "902 - loss: 0.646\n",
      "903 - loss: 0.646\n",
      "904 - loss: 0.646\n",
      "905 - loss: 0.646\n",
      "906 - loss: 0.646\n",
      "907 - loss: 0.646\n",
      "908 - loss: 0.646\n",
      "909 - loss: 0.646\n",
      "910 - loss: 0.646\n",
      "911 - loss: 0.646\n",
      "912 - loss: 0.646\n",
      "913 - loss: 0.646\n",
      "914 - loss: 0.646\n",
      "915 - loss: 0.646\n",
      "916 - loss: 0.646\n",
      "917 - loss: 0.646\n",
      "918 - loss: 0.646\n",
      "919 - loss: 0.646\n",
      "920 - loss: 0.646\n",
      "921 - loss: 0.646\n",
      "922 - loss: 0.646\n",
      "923 - loss: 0.646\n",
      "924 - loss: 0.646\n",
      "925 - loss: 0.646\n",
      "926 - loss: 0.646\n",
      "927 - loss: 0.646\n",
      "928 - loss: 0.646\n",
      "f_new: 0.674 - f: 0.646 - Backtracking...\n",
      "f_new: 0.646 - f: 0.646 - Backtracking...\n",
      "931 - loss: 0.646\n",
      "932 - loss: 0.646\n",
      "933 - loss: 0.646\n",
      "934 - loss: 0.646\n",
      "935 - loss: 0.646\n",
      "936 - loss: 0.646\n",
      "937 - loss: 0.646\n",
      "938 - loss: 0.646\n",
      "939 - loss: 0.646\n",
      "940 - loss: 0.646\n",
      "941 - loss: 0.646\n",
      "942 - loss: 0.646\n",
      "943 - loss: 0.646\n",
      "944 - loss: 0.646\n",
      "945 - loss: 0.646\n",
      "946 - loss: 0.646\n",
      "947 - loss: 0.646\n",
      "948 - loss: 0.646\n",
      "949 - loss: 0.646\n",
      "950 - loss: 0.646\n",
      "951 - loss: 0.646\n",
      "952 - loss: 0.646\n",
      "953 - loss: 0.643\n",
      "f_new: 0.748 - f: 0.643 - Backtracking...\n",
      "f_new: 0.643 - f: 0.643 - Backtracking...\n",
      "956 - loss: 0.643\n",
      "957 - loss: 0.643\n",
      "958 - loss: 0.643\n",
      "959 - loss: 0.643\n",
      "960 - loss: 0.642\n",
      "f_new: 0.649 - f: 0.642 - Backtracking...\n",
      "962 - loss: 0.642\n",
      "963 - loss: 0.642\n",
      "964 - loss: 0.642\n",
      "965 - loss: 0.642\n",
      "966 - loss: 0.641\n",
      "f_new: 0.645 - f: 0.641 - Backtracking...\n",
      "968 - loss: 0.641\n",
      "969 - loss: 0.641\n",
      "970 - loss: 0.641\n",
      "971 - loss: 0.641\n",
      "f_new: 0.642 - f: 0.641 - Backtracking...\n",
      "973 - loss: 0.641\n",
      "974 - loss: 0.641\n",
      "975 - loss: 0.641\n",
      "976 - loss: 0.641\n",
      "977 - loss: 0.641\n",
      "f_new: 0.641 - f: 0.641 - Backtracking...\n",
      "979 - loss: 0.641\n",
      "980 - loss: 0.641\n",
      "981 - loss: 0.641\n",
      "982 - loss: 0.641\n",
      "983 - loss: 0.641\n",
      "984 - loss: 0.641\n",
      "985 - loss: 0.641\n",
      "986 - loss: 0.641\n",
      "987 - loss: 0.641\n",
      "988 - loss: 0.641\n",
      "989 - loss: 0.641\n",
      "f_new: 0.641 - f: 0.641 - Backtracking...\n",
      "991 - loss: 0.641\n",
      "992 - loss: 0.641\n",
      "993 - loss: 0.641\n",
      "994 - loss: 0.641\n",
      "995 - loss: 0.641\n",
      "996 - loss: 0.641\n",
      "997 - loss: 0.641\n",
      "998 - loss: 0.641\n",
      "999 - loss: 0.641\n",
      "1000 - loss: 0.641\n",
      "Reached maximum number of function evaluations 1000\n",
      "0.6813054955603551\n"
     ]
    }
   ],
   "source": [
    "#kernel_logreg_poly = LeastSquaresGDKernel(Kernel.kernel_poly, verbose=True, max_evals=1000)\n",
    "#kernel_logreg_poly = LogisticRegressionKernel(Kernel.kernel_poly, verbose=True, max_evals=1000)\n",
    "kernel_logreg_poly = LogisticRegressionKernel(Kernel.kernel_poly, verbose=True, max_evals=1000)\n",
    "p = 4\n",
    "preds = kernel_logreg_poly.predict(y_sub, X_sub, X_sub, p)\n",
    "print(compute_accuracy(preds, y_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_clean = remove_NaN_features(tX, 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegression()\n",
    "logReg.fit(y, tX_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 1289883598.7\n",
      "Iteration 1, loss = 929278938.1\n",
      "Iteration 2, loss = 843606874.7\n",
      "Iteration 3, loss = 832528308.0\n",
      "Iteration 4, loss = 831415267.7\n",
      "Iteration 5, loss = 831306045.8\n",
      "Iteration 6, loss = 831295253.7\n",
      "Iteration 7, loss = 831294167.6\n",
      "Iteration 8, loss = 831294054.6\n",
      "Iteration 9, loss = 831294042.1\n"
     ]
    }
   ],
   "source": [
    "model = AlternativePCA(k=2)\n",
    "model.fit(X)\n",
    "Z_pca = model.compress(X)\n",
    "Xhat_pca = model.expand(Z_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 1521186763.4\n",
      "Iteration 1, loss = 947138369.7\n",
      "Iteration 2, loss = 854324275.1\n",
      "Iteration 3, loss = 835473177.5\n",
      "Iteration 4, loss = 832070513.0\n",
      "Iteration 5, loss = 831441985.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antho\\Google Drive\\EPFL\\MA1\\CS-433 Machine Learning\\ML_course\\projects\\project1\\scripts\\solver.py:65: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  y = g_new - g\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 831322662.2\n",
      "Iteration 7, loss = 831299623.4\n",
      "Iteration 8, loss = 831295133.9\n",
      "Iteration 9, loss = 831294255.0\n",
      "Iteration 0, loss = 114368921.2\n",
      "Iteration 1, loss = 114357028.3\n",
      "Iteration 2, loss = 114357025.0\n",
      "Iteration 3, loss = 114357025.0\n",
      "Iteration 4, loss = 114357025.0\n",
      "Iteration 5, loss = 114357025.0\n",
      "Iteration 6, loss = 114357025.0\n",
      "Iteration 7, loss = 114357025.0\n",
      "Iteration 8, loss = 114357025.0\n",
      "Iteration 9, loss = 114357025.0\n"
     ]
    }
   ],
   "source": [
    "model = RobustPCA(k=2)\n",
    "model.fit(X)\n",
    "Z_robust = model.compress(X)\n",
    "Xhat_robust = model.expand(Z_robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.72588088, -7.31380373],\n",
       "       [ 3.89450982, 10.60756305],\n",
       "       [ 7.77930283, -1.90683368],\n",
       "       ...,\n",
       "       [-2.54520707,  6.4204025 ],\n",
       "       [-8.07558037, 21.54278193],\n",
       "       [-4.59888286, 21.69639677]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "#Need to rescale to do PCA\n",
    "def pca_axes(Z,scale,scale2, verbose=False):\n",
    "    \n",
    "    pcs = \n",
    "    colors = np.arctan2(pcs[0,:], pcs[1,:])\n",
    "    colormap = cm.inferno\n",
    "    norm = Normalize()\n",
    "    norm.autoscale(colors)\n",
    "    plt.rcParams['image.cmap'] = 'Paired'\n",
    "\n",
    "    # Quiver\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    plt.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),\n",
    "           pcs[0,:], pcs[1,:], color=colormap(norm(colors)),\n",
    "           angles='xy', scale_units='xy', scale=scale)\n",
    "    o = 0\n",
    "    for i in range(0,pcs.shape[1]):\n",
    "        #plt.arrow(0, 0, pcs[0,i], pcs[1,i],color='k') \n",
    "        if i==3:\n",
    "            plt.text(pcs[0,i]* scale2, pcs[1,i]*scale2-0.03, X.columns[i], color = 'k', \n",
    "             ha = 'center', va = 'center', fontsize=10, weight='bold')\n",
    "        elif i==12:\n",
    "            plt.text(pcs[0,i]* scale2, pcs[1,i]*scale2-0.03, X.columns[i], color = 'k', \n",
    "             ha = 'center', va = 'center', fontsize=10, weight='bold')\n",
    "        elif np.power(pcs[0,i]-pcs[1,i], 2)<0.001:\n",
    "            plt.text(0, 0-o, ' ', color = 'k', \n",
    "             ha = 'center', va = 'center', fontsize=12, weight='bold')\n",
    "        else:\n",
    "            plt.text(pcs[0,i]* scale2, pcs[1,i] *scale2, X.columns[i], color = 'k', \n",
    "             ha = 'center', va = 'center', fontsize=10, weight='bold')\n",
    "        o=o+0.04\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.xlim([-1.0,1.0])\n",
    "    plt.ylim([-0.5,1.0])\n",
    "    plt.xlabel('Principal component 0')\n",
    "    plt.ylabel('Principal component 1')\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    " \n",
    "    if verbose==True:\n",
    "        for i in range (0,pcs.shape[1]):\n",
    "            print(str(i), df.columns[i+2])\n",
    "        \n",
    "    return\n",
    "\n",
    "pca_axes(Z_pca, 0.65, 1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-524e3a5e580a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel_logreg_poly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\EPFL\\MA1\\CS-433 Machine Learning\\ML_course\\projects\\project1\\scripts\\classifiers.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, y, X, Xtest, lambda_, *args)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mKtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\EPFL\\MA1\\CS-433 Machine Learning\\ML_course\\projects\\project1\\scripts\\classifiers.py\u001b[0m in \u001b[0;36mkernel_RBF\u001b[1;34m(X1, X2, sigma)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mK\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[1;32m-> 2076\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   2077\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test kernel\n",
    "kernel_logreg_poly = LeastSquaresKernel(Kernel.kernel_RBF, verbose=True, max_evals=1000)\n",
    "p = 4\n",
    "sigma = 1\n",
    "accuracies = []\n",
    "for i in range(5):\n",
    "    print(i,  end=' - ')\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    X_sub = (X[shuffle_indices,:])[::100]\n",
    "    y_sub = (y[shuffle_indices])[::100]\n",
    "    \n",
    "    X_pred = (X[shuffle_indices,:])[::100]\n",
    "    y_pred = (y[shuffle_indices])[::100]\n",
    "    \n",
    "    preds = kernel_logreg_poly.predict(y_sub, X_sub, X_pred, sigma, lambda_=1)\n",
    "    accuracy = compute_accuracy(preds, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8757090909090909"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mse, loss_mse = least_squares(y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_mse = compute_accuracy(predict_labels(w_mse, X), y)\n",
    "print(accuracy_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_classifier(lambda_):\n",
    "    return ridge_regression(y, X, lambda_)\n",
    "\n",
    "lambda_ridge, _, _ = find_max_hyperparam(ridge_classifier, [10**c for c in range(-3,3)])\n",
    "print(\"Optimal lambda: %f\" % lambda_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_train(y_train, X_train):\n",
    "    return ridge_regression(y, X, lambda_ridge)\n",
    "\n",
    "def ridge_test(X_test, w):\n",
    "    return np.sign(X_test@w)\n",
    "\n",
    "accuracy_ridge = cross_validate(y, X, ridge_train, ridge_test, 0.8, 100)\n",
    "print(accuracy_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_train(y_train, X_train):\n",
    "    return logistic_regression(y_train, X_train, 0.01*np.ones(X_train.shape[1]), 1000, verbose=False)\n",
    "\n",
    "def log_reg_test(X_test, w):\n",
    "    return np.sign(X_test@w)\n",
    "\n",
    "accuracy_log_reg = cross_validate(y, X_safe, log_reg_train, log_reg_test, 0.7, 20)\n",
    "print(accuracy_log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_log_reg_classifier(lambda_):\n",
    "    return reg_logistic_regression(y, X_safe, lambda_, np.zeros(X_safe.shape[1]), 1000)\n",
    "\n",
    "def log_reg_sparse_classifier(lambda_):\n",
    "    return logistic_regression_sparse(y, X_safe, lambda_, np.zeros(X_safe.shape[1]), 1000)\n",
    "\n",
    "def mse_sparse_classifier(lambda_):\n",
    "    return least_squares_sparse(y, X_safe, lambda_, np.zeros(X_safe.shape[1]), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_log_reg_l2, _, _ = find_max_hyperparam(reg_log_reg_classifier, [10**c for c in range(-3,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_log_reg_l1, _, _ = find_max_hyperparam(log_reg_sparse_classifier, [10**c for c in range(-3,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_mse_l2, _, _ = find_max_hyperparam(ridge_classifier, [10**c for c in range(-6,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_mse_l1, w_mse_l1, _ = find_max_hyperparam(mse_sparse_classifier, [10**c for c in range(-6,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mse_l1, _ = least_squares_sparse(y, X_safe, 0.01, np.zeros(X_safe.shape[1]), 1000)\n",
    "print(\"Non-zero weights: %i / %i\" % (np.sum(w_mse_l1 != 0), len(w_mse_l1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1\n",
    "ypred_kernel = kernel_predict(kernel_poly, y, X_safe, X_safe, p, lambda_=1)\n",
    "print(compute_accuracy(ypred_kernel, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "y_test, tX_test, ids_test, _ = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions using new model\n",
    "test_split_indices, X_test_split, _ = split_data(tX_test)\n",
    "X_test_split_poly = [ build_X(X, 8) for X in X_test_split ]\n",
    "y_pred = np.ones(tX_test.shape[0])\n",
    "for model, X, indices in zip(models, X_test_split_poly, test_split_indices):\n",
    "    y_pred[indices] = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/predictions.csv'\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/predictions.csv'\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
