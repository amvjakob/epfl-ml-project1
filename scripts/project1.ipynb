{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from helpers import *\n",
    "from classifiers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt', 'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality', 'DER_lep_eta_centrality', 'PRI_tau_pt', 'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi', 'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt']\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids, features = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y=None):\n",
    "    # features\n",
    "    undef_feature_for = {\n",
    "        'DER_deltaeta_jet_jet'   : [0, 1],\n",
    "        'DER_mass_jet_jet'       : [0, 1],\n",
    "        'DER_prodeta_jet_jet'    : [0, 1],\n",
    "        'DER_lep_eta_centrality' : [0, 1],\n",
    "        'PRI_jet_num'            : [0, 1, 2, 3],\n",
    "        'PRI_jet_leading_pt'     : [0],\n",
    "        'PRI_jet_leading_eta'    : [0],\n",
    "        'PRI_jet_leading_phi'    : [0],\n",
    "        'PRI_jet_subleading_pt'  : [0, 1],\n",
    "        'PRI_jet_subleading_eta' : [0, 1],\n",
    "        'PRI_jet_subleading_phi' : [0, 1],\n",
    "        'PRI_jet_all_pt'         : [0]\n",
    "    }\n",
    "\n",
    "    jet_num_feature = \"PRI_jet_num\"\n",
    "    jet_levels = 4\n",
    "\n",
    "    features_split = []\n",
    "    for jet in range(jet_levels):\n",
    "        valid_features = [ f for f in features if not ((f in undef_feature_for) and (jet in undef_feature_for[f])) ]\n",
    "        features_split.append(valid_features)\n",
    "        \n",
    "    # split data based on jet level (vertical split)\n",
    "    split_indices = [\n",
    "        X[:,features.index(jet_num_feature)] == i for i in range(jet_levels)\n",
    "    ]\n",
    "    X_split = [\n",
    "        X[X[:,features.index(jet_num_feature)] == i,:] for i in range(jet_levels)\n",
    "    ]\n",
    "    if y is None:\n",
    "        y_split = None\n",
    "    else:\n",
    "        y_split = [\n",
    "            y[X[:,features.index(jet_num_feature)] == i] for i in range(jet_levels)\n",
    "        ]\n",
    "\n",
    "    # only keep relevant features (horizontal split)\n",
    "    for i, X_ in enumerate(X_split):\n",
    "        indices = [ features.index(feature) for feature in features_split[i] ]\n",
    "        indices_bool = [ e in indices for e in range(len(features)) ]\n",
    "        X_split[i] = X_[:,indices_bool]\n",
    "        \n",
    "    return split_indices, X_split, y_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly_no_interaction(X, degree):\n",
    "    result = X.copy()\n",
    "    for d in range(2, degree+1):\n",
    "        # faster than np.power()\n",
    "        power = X.copy()\n",
    "        for i in range(d - 1):\n",
    "            power = power * X\n",
    "            \n",
    "        result = np.hstack((result, power))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def build_X(X, d_int, d_sq):\n",
    "    \"\"\"\n",
    "    Expands X\n",
    "    \n",
    "    :param X: examples\n",
    "    :param d_int: degree of integer powers\n",
    "    :param d_sq: ceil of degree of half-powers (expansion will be up to d_sq - 0.5)\n",
    "    \n",
    "    \"\"\"\n",
    "    X_ = remove_NaN_features(X, 0.2)\n",
    "    X_, mean_, std_ = standardize(X_)\n",
    "    \n",
    "    ints = []\n",
    "    sqrts = []\n",
    "    \n",
    "    # build integer powers\n",
    "    if d_int > 0:\n",
    "        ints = build_poly_no_interaction(X_, d_int)\n",
    "      \n",
    "    # build half-powers (0.5, 1.5, 2.5, etc.)\n",
    "    if d_sq > 0:\n",
    "        sqrts = np.sqrt(np.abs(X_))\n",
    "        if d_sq > 1:\n",
    "            sqrts = np.c_[sqrts, sqrts * np.abs(build_poly_no_interaction(X_, d_sq - 1))]\n",
    "\n",
    "    # concat\n",
    "    X_ = np.c_[ints, sqrts]\n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_split, y_split = split_data(tX, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - 0.8024675692211434\n",
      "2 - 0.804423723524172\n",
      "3 - 0.8074080601545135\n",
      "4 - 0.807784108675261\n",
      "5 - 0.8089481483860476\n",
      "6 - 0.8096562041201141\n",
      "7 - 0.8103402721255751\n",
      "8 - 0.8134245468179246\n",
      "9 - 0.8151727287741669\n",
      "10 - 0.8154848054573408\n",
      "11 - 0.8157688400589824\n",
      "12 - 0.8153088270858204\n",
      "13 - 0.8094799726467542\n",
      "14 - 0.8048839584699341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1690071b550>]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cross_validate\n",
    "total_acc = []\n",
    "k = 10\n",
    "lambda_ = 0.01\n",
    "degrees = range(1, 15)\n",
    "\n",
    "# run that shit\n",
    "for deg in degrees:\n",
    "    print(deg, end=\" - \")\n",
    "    X_split_poly = [ build_X(X, deg, 2) for X in X_split ]\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    # iterate over 4 sub datasets\n",
    "    for i in range(len(X_split_poly)):\n",
    "        classifier = LeastSquaresL2(lambda_)\n",
    "        acc = np.mean(cross_validate_kfold(y_split[i], X_split_poly[i], classifier, k))\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "    # compute mean (weighted)\n",
    "    accuracy = 0\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        accuracy += acc * len(y_split[i])\n",
    "    accuracy /= len(y)\n",
    "        \n",
    "    print(accuracy)\n",
    "    total_acc.append(accuracy)\n",
    "    \n",
    "plt.plot(degrees, total_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model for dataset 0\n",
      "Building model for dataset 1\n",
      "Building model for dataset 2\n",
      "Building model for dataset 3\n"
     ]
    }
   ],
   "source": [
    "# train actual models\n",
    "# degree 9 seems to be best\n",
    "\n",
    "X_split_poly = [ build_X(X, 9, 2) for X in X_split ]\n",
    "lambda_ = 0.01\n",
    "models = []\n",
    "for i in range(len(X_split_poly)):\n",
    "    print(f\"Building model for dataset {i}\")\n",
    "    lse = LeastSquaresL2(lambda_)\n",
    "    lse.fit(y_split[i], X_split_poly[i])\n",
    "    models.append(lse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_new: 76658898567251466163194953728.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 38329449283176485824627736576.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 19164724641363614885782487040.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 9582362320569493429625552896.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 4791181160228590807691558912.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 2395590580086217725163077632.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 1197795290029069748362280960.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 598897645007515248352034816.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 299448822500247879980941312.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 149724411248369067892932608.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 74862205623307097897697280.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 37431102811214830924464128.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 18715551405388056450039808.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 9357775702584346571440128.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 4678887851237333532672000.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 2339443925591246352941056.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 1169721962781913238208512.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 584860981384101649973248.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 292430490688623340421120.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 146215245342597927927808.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 73107622670442084433920.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 36553811334792602451968.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 18276905667182079246336.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 9138452833483928633344.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 4569226416688408821760.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 2284613208317426663424.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 1142306604145324326912.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 571153302065967857664.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 285576651029636710400.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 142788325513144713216.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 71394162755735568384.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 35697081377449381888.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 17848540688515493888.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 8924270344153145344.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 4462135172024272384.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 2231067585985986560.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 1115533792979918336.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 557766896483421696.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 278883448238442208.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 139441724117586736.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 69720862057976232.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 34860431028579564.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 17430215514085542.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 8715107756940701.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 4357553878419383.500 - f: 69254.414 - Backtracking...\n",
      "f_new: 2178776939184305.750 - f: 69254.414 - Backtracking...\n",
      "f_new: 1089388469579598.500 - f: 69254.414 - Backtracking...\n",
      "f_new: 544694234783711.938 - f: 69254.414 - Backtracking...\n",
      "f_new: 272347117389066.719 - f: 69254.414 - Backtracking...\n",
      "f_new: 136173558693473.750 - f: 69254.414 - Backtracking...\n",
      "f_new: 68086779346642.148 - f: 69254.414 - Backtracking...\n",
      "f_new: 34043389673833.574 - f: 69254.414 - Backtracking...\n",
      "f_new: 17021694837892.352 - f: 69254.414 - Backtracking...\n",
      "f_new: 8510847420357.464 - f: 69254.414 - Backtracking...\n",
      "f_new: 4255423712067.646 - f: 69254.414 - Backtracking...\n",
      "f_new: 2127711858493.365 - f: 69254.414 - Backtracking...\n",
      "f_new: 1063855932419.652 - f: 69254.414 - Backtracking...\n",
      "f_new: 531927970307.091 - f: 69254.414 - Backtracking...\n",
      "f_new: 265963990478.546 - f: 69254.414 - Backtracking...\n",
      "f_new: 132982002214.515 - f: 69254.414 - Backtracking...\n",
      "f_new: 66491010303.369 - f: 69254.414 - Backtracking...\n",
      "f_new: 33245517324.573 - f: 69254.414 - Backtracking...\n",
      "f_new: 16622774634.730 - f: 69254.414 - Backtracking...\n",
      "f_new: 8311407338.168 - f: 69254.414 - Backtracking...\n",
      "f_new: 4155727005.679 - f: 69254.414 - Backtracking...\n",
      "f_new: 2077889258.295 - f: 69254.414 - Backtracking...\n",
      "f_new: 1038972215.206 - f: 69254.414 - Backtracking...\n",
      "f_new: 519515086.671 - f: 69254.414 - Backtracking...\n",
      "f_new: 259787523.507 - f: 69254.414 - Backtracking...\n",
      "f_new: 129924456.921 - f: 69254.414 - Backtracking...\n",
      "f_new: 64993486.571 - f: 69254.414 - Backtracking...\n",
      "f_new: 32528485.172 - f: 69254.414 - Backtracking...\n",
      "f_new: 16296420.965 - f: 69254.414 - Backtracking...\n",
      "f_new: 8180791.802 - f: 69254.414 - Backtracking...\n",
      "f_new: 4123352.532 - f: 69254.414 - Backtracking...\n",
      "f_new: 2094974.372 - f: 69254.414 - Backtracking...\n",
      "f_new: 1081082.186 - f: 69254.414 - Backtracking...\n",
      "f_new: 574383.365 - f: 69254.414 - Backtracking...\n",
      "f_new: 321231.732 - f: 69254.414 - Backtracking...\n",
      "f_new: 194809.026 - f: 69254.414 - Backtracking...\n",
      "f_new: 131714.538 - f: 69254.414 - Backtracking...\n",
      "f_new: 100257.696 - f: 69254.414 - Backtracking...\n",
      "f_new: 84596.086 - f: 69254.414 - Backtracking...\n",
      "f_new: 76811.424 - f: 69254.414 - Backtracking...\n",
      "f_new: 72951.059 - f: 69254.414 - Backtracking...\n",
      "f_new: 71043.769 - f: 69254.414 - Backtracking...\n",
      "f_new: 70107.038 - f: 69254.414 - Backtracking...\n",
      "f_new: 69651.210 - f: 69254.414 - Backtracking...\n",
      "f_new: 69432.345 - f: 69254.414 - Backtracking...\n",
      "f_new: 69329.220 - f: 69254.414 - Backtracking...\n",
      "f_new: 69282.072 - f: 69254.414 - Backtracking...\n",
      "f_new: 69261.326 - f: 69254.414 - Backtracking...\n",
      "f_new: 69252.593 - f: 69254.414 - Backtracking...\n",
      "f_new: 69249.404 - f: 69254.414 - Backtracking...\n",
      "f_new: 69248.856 - f: 69254.414 - Backtracking...\n",
      "f_new: 69249.478 - f: 69254.414 - Backtracking...\n",
      "f_new: 69250.558 - f: 69254.414 - Backtracking...\n",
      "f_new: 69251.556 - f: 69254.414 - Backtracking...\n",
      "f_new: 69252.226 - f: 69254.414 - Backtracking...\n",
      "f_new: 69252.610 - f: 69254.414 - Backtracking...\n",
      "f_new: 69252.822 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.000 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.227 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.431 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.564 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.640 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.680 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.700 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.711 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.716 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.718 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.720 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.720 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "f_new: 69253.721 - f: 69254.414 - Backtracking...\n",
      "121 - loss: 69253.721\n",
      "122 - loss: 69253.721\n",
      "123 - loss: 69253.158\n",
      "124 - loss: 69253.102\n",
      "125 - loss: 69253.065\n",
      "126 - loss: 69253.050\n",
      "127 - loss: 69253.043\n",
      "128 - loss: 69253.040\n",
      "129 - loss: 69253.039\n",
      "130 - loss: 69253.039\n",
      "131 - loss: 69253.038\n",
      "132 - loss: 69253.038\n",
      "133 - loss: 69253.038\n",
      "134 - loss: 69253.038\n",
      "135 - loss: 69253.031\n",
      "136 - loss: 69252.548\n",
      "f_new: 69624.582 - f: 69252.548 - Backtracking...\n",
      "138 - loss: 69252.548\n",
      "f_new: 69549.467 - f: 69252.548 - Backtracking...\n",
      "140 - loss: 69252.548\n",
      "f_new: 69541.870 - f: 69252.548 - Backtracking...\n",
      "142 - loss: 69252.548\n",
      "f_new: 69534.511 - f: 69252.548 - Backtracking...\n",
      "144 - loss: 69252.548\n",
      "f_new: 69526.762 - f: 69252.548 - Backtracking...\n",
      "146 - loss: 69252.548\n",
      "f_new: 69518.668 - f: 69252.548 - Backtracking...\n",
      "148 - loss: 69252.547\n",
      "f_new: 69510.288 - f: 69252.547 - Backtracking...\n",
      "150 - loss: 69252.547\n",
      "f_new: 69501.689 - f: 69252.547 - Backtracking...\n",
      "152 - loss: 69252.547\n",
      "f_new: 69492.941 - f: 69252.547 - Backtracking...\n",
      "154 - loss: 69252.547\n",
      "f_new: 69484.115 - f: 69252.547 - Backtracking...\n",
      "156 - loss: 69252.547\n",
      "f_new: 69475.281 - f: 69252.547 - Backtracking...\n",
      "158 - loss: 69252.547\n",
      "f_new: 69466.505 - f: 69252.547 - Backtracking...\n",
      "160 - loss: 69252.547\n",
      "f_new: 69457.845 - f: 69252.547 - Backtracking...\n",
      "162 - loss: 69252.547\n",
      "f_new: 69449.354 - f: 69252.547 - Backtracking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164 - loss: 69252.547\n",
      "f_new: 69441.077 - f: 69252.547 - Backtracking...\n",
      "166 - loss: 69252.547\n",
      "f_new: 69433.048 - f: 69252.547 - Backtracking...\n",
      "168 - loss: 69252.547\n",
      "f_new: 69425.296 - f: 69252.547 - Backtracking...\n",
      "170 - loss: 69252.547\n",
      "f_new: 69417.839 - f: 69252.547 - Backtracking...\n",
      "172 - loss: 69252.547\n",
      "f_new: 69410.690 - f: 69252.547 - Backtracking...\n",
      "174 - loss: 69252.547\n",
      "f_new: 69403.855 - f: 69252.547 - Backtracking...\n",
      "176 - loss: 69252.547\n",
      "f_new: 69397.336 - f: 69252.547 - Backtracking...\n",
      "178 - loss: 69252.546\n",
      "f_new: 69391.131 - f: 69252.546 - Backtracking...\n",
      "180 - loss: 69252.546\n",
      "f_new: 69385.233 - f: 69252.546 - Backtracking...\n",
      "182 - loss: 69252.546\n",
      "f_new: 69379.633 - f: 69252.546 - Backtracking...\n",
      "184 - loss: 69252.546\n",
      "f_new: 69374.322 - f: 69252.546 - Backtracking...\n",
      "186 - loss: 69252.546\n",
      "f_new: 69369.287 - f: 69252.546 - Backtracking...\n",
      "188 - loss: 69252.546\n",
      "f_new: 69364.516 - f: 69252.546 - Backtracking...\n",
      "190 - loss: 69252.546\n",
      "f_new: 69359.995 - f: 69252.546 - Backtracking...\n",
      "192 - loss: 69252.546\n",
      "f_new: 69355.713 - f: 69252.546 - Backtracking...\n",
      "194 - loss: 69252.546\n",
      "f_new: 69351.656 - f: 69252.546 - Backtracking...\n",
      "196 - loss: 69252.546\n",
      "f_new: 69347.812 - f: 69252.546 - Backtracking...\n",
      "198 - loss: 69252.546\n",
      "f_new: 69344.167 - f: 69252.546 - Backtracking...\n",
      "200 - loss: 69252.546\n",
      "f_new: 69340.710 - f: 69252.546 - Backtracking...\n",
      "202 - loss: 69252.546\n",
      "f_new: 69337.431 - f: 69252.546 - Backtracking...\n",
      "204 - loss: 69252.546\n",
      "f_new: 69334.317 - f: 69252.546 - Backtracking...\n",
      "206 - loss: 69252.546\n",
      "f_new: 69331.360 - f: 69252.546 - Backtracking...\n",
      "208 - loss: 69252.546\n",
      "f_new: 69328.549 - f: 69252.546 - Backtracking...\n",
      "210 - loss: 69252.546\n",
      "f_new: 69325.876 - f: 69252.546 - Backtracking...\n",
      "212 - loss: 69252.546\n",
      "f_new: 69323.332 - f: 69252.546 - Backtracking...\n",
      "214 - loss: 69252.546\n",
      "f_new: 69320.909 - f: 69252.546 - Backtracking...\n",
      "216 - loss: 69252.546\n",
      "f_new: 69318.600 - f: 69252.546 - Backtracking...\n",
      "218 - loss: 69252.546\n",
      "f_new: 69316.398 - f: 69252.546 - Backtracking...\n",
      "220 - loss: 69252.546\n",
      "f_new: 69314.296 - f: 69252.546 - Backtracking...\n",
      "222 - loss: 69252.546\n",
      "f_new: 69312.289 - f: 69252.546 - Backtracking...\n",
      "224 - loss: 69252.546\n",
      "f_new: 69310.372 - f: 69252.546 - Backtracking...\n",
      "226 - loss: 69252.546\n",
      "f_new: 69308.537 - f: 69252.546 - Backtracking...\n",
      "228 - loss: 69252.546\n",
      "f_new: 69306.782 - f: 69252.546 - Backtracking...\n",
      "230 - loss: 69252.546\n",
      "f_new: 69305.101 - f: 69252.546 - Backtracking...\n",
      "232 - loss: 69252.546\n",
      "f_new: 69303.491 - f: 69252.546 - Backtracking...\n",
      "234 - loss: 69252.546\n",
      "f_new: 69301.946 - f: 69252.546 - Backtracking...\n",
      "236 - loss: 69252.546\n",
      "f_new: 69300.464 - f: 69252.546 - Backtracking...\n",
      "238 - loss: 69252.546\n",
      "f_new: 69299.041 - f: 69252.546 - Backtracking...\n",
      "240 - loss: 69252.545\n",
      "f_new: 69297.674 - f: 69252.545 - Backtracking...\n",
      "242 - loss: 69252.545\n",
      "f_new: 69296.359 - f: 69252.545 - Backtracking...\n",
      "244 - loss: 69252.545\n",
      "f_new: 69295.095 - f: 69252.545 - Backtracking...\n",
      "246 - loss: 69252.545\n",
      "f_new: 69293.878 - f: 69252.545 - Backtracking...\n",
      "248 - loss: 69252.545\n",
      "f_new: 69292.706 - f: 69252.545 - Backtracking...\n",
      "250 - loss: 69252.545\n",
      "f_new: 69291.577 - f: 69252.545 - Backtracking...\n",
      "252 - loss: 69252.545\n",
      "f_new: 69290.488 - f: 69252.545 - Backtracking...\n",
      "254 - loss: 69252.545\n",
      "f_new: 69289.438 - f: 69252.545 - Backtracking...\n",
      "256 - loss: 69252.545\n",
      "f_new: 69288.424 - f: 69252.545 - Backtracking...\n",
      "258 - loss: 69252.545\n",
      "f_new: 69287.444 - f: 69252.545 - Backtracking...\n",
      "260 - loss: 69252.545\n",
      "f_new: 69286.498 - f: 69252.545 - Backtracking...\n",
      "262 - loss: 69252.545\n",
      "f_new: 69285.584 - f: 69252.545 - Backtracking...\n",
      "264 - loss: 69252.545\n",
      "f_new: 69284.699 - f: 69252.545 - Backtracking...\n",
      "266 - loss: 69252.545\n",
      "f_new: 69283.843 - f: 69252.545 - Backtracking...\n",
      "268 - loss: 69252.545\n",
      "f_new: 69283.014 - f: 69252.545 - Backtracking...\n",
      "270 - loss: 69252.545\n",
      "f_new: 69282.211 - f: 69252.545 - Backtracking...\n",
      "272 - loss: 69252.545\n",
      "f_new: 69281.433 - f: 69252.545 - Backtracking...\n",
      "274 - loss: 69252.545\n",
      "f_new: 69280.679 - f: 69252.545 - Backtracking...\n",
      "276 - loss: 69252.545\n",
      "f_new: 69279.948 - f: 69252.545 - Backtracking...\n",
      "278 - loss: 69252.545\n",
      "f_new: 69279.238 - f: 69252.545 - Backtracking...\n",
      "280 - loss: 69252.545\n",
      "f_new: 69278.549 - f: 69252.545 - Backtracking...\n",
      "282 - loss: 69252.545\n",
      "f_new: 69277.879 - f: 69252.545 - Backtracking...\n",
      "284 - loss: 69252.545\n",
      "f_new: 69277.229 - f: 69252.545 - Backtracking...\n",
      "286 - loss: 69252.545\n",
      "f_new: 69276.597 - f: 69252.545 - Backtracking...\n",
      "288 - loss: 69252.545\n",
      "f_new: 69275.983 - f: 69252.545 - Backtracking...\n",
      "290 - loss: 69252.545\n",
      "f_new: 69275.385 - f: 69252.545 - Backtracking...\n",
      "292 - loss: 69252.545\n",
      "f_new: 69274.803 - f: 69252.545 - Backtracking...\n",
      "294 - loss: 69252.545\n",
      "f_new: 69274.237 - f: 69252.545 - Backtracking...\n",
      "296 - loss: 69252.545\n",
      "f_new: 69273.685 - f: 69252.545 - Backtracking...\n",
      "298 - loss: 69252.545\n",
      "f_new: 69273.148 - f: 69252.545 - Backtracking...\n",
      "300 - loss: 69252.545\n",
      "f_new: 69272.625 - f: 69252.545 - Backtracking...\n",
      "302 - loss: 69252.545\n",
      "f_new: 69272.114 - f: 69252.545 - Backtracking...\n",
      "304 - loss: 69252.545\n",
      "f_new: 69271.616 - f: 69252.545 - Backtracking...\n",
      "306 - loss: 69252.545\n",
      "f_new: 69271.131 - f: 69252.545 - Backtracking...\n",
      "308 - loss: 69252.545\n",
      "f_new: 69270.657 - f: 69252.545 - Backtracking...\n",
      "310 - loss: 69252.545\n",
      "f_new: 69270.194 - f: 69252.545 - Backtracking...\n",
      "312 - loss: 69252.545\n",
      "f_new: 69269.742 - f: 69252.545 - Backtracking...\n",
      "314 - loss: 69252.545\n",
      "f_new: 69269.300 - f: 69252.545 - Backtracking...\n",
      "316 - loss: 69252.545\n",
      "f_new: 69268.869 - f: 69252.545 - Backtracking...\n",
      "318 - loss: 69252.545\n",
      "f_new: 69268.447 - f: 69252.545 - Backtracking...\n",
      "320 - loss: 69252.545\n",
      "f_new: 69268.034 - f: 69252.545 - Backtracking...\n",
      "322 - loss: 69252.545\n",
      "f_new: 69267.631 - f: 69252.545 - Backtracking...\n",
      "324 - loss: 69252.545\n",
      "f_new: 69267.236 - f: 69252.545 - Backtracking...\n",
      "326 - loss: 69252.545\n",
      "f_new: 69266.849 - f: 69252.545 - Backtracking...\n",
      "328 - loss: 69252.545\n",
      "f_new: 69266.470 - f: 69252.545 - Backtracking...\n",
      "330 - loss: 69252.545\n",
      "f_new: 69266.100 - f: 69252.545 - Backtracking...\n",
      "332 - loss: 69252.545\n",
      "f_new: 69265.736 - f: 69252.545 - Backtracking...\n",
      "334 - loss: 69252.545\n",
      "f_new: 69265.380 - f: 69252.545 - Backtracking...\n",
      "336 - loss: 69252.545\n",
      "f_new: 69265.031 - f: 69252.545 - Backtracking...\n",
      "338 - loss: 69252.545\n",
      "f_new: 69264.688 - f: 69252.545 - Backtracking...\n",
      "340 - loss: 69252.545\n",
      "f_new: 69264.352 - f: 69252.545 - Backtracking...\n",
      "342 - loss: 69252.545\n",
      "f_new: 69264.022 - f: 69252.545 - Backtracking...\n",
      "344 - loss: 69252.545\n",
      "f_new: 69263.698 - f: 69252.545 - Backtracking...\n",
      "346 - loss: 69252.545\n",
      "f_new: 69263.380 - f: 69252.545 - Backtracking...\n",
      "348 - loss: 69252.545\n",
      "f_new: 69263.067 - f: 69252.545 - Backtracking...\n",
      "350 - loss: 69252.545\n",
      "f_new: 69262.759 - f: 69252.545 - Backtracking...\n",
      "352 - loss: 69252.545\n",
      "f_new: 69262.457 - f: 69252.545 - Backtracking...\n",
      "354 - loss: 69252.545\n",
      "f_new: 69262.160 - f: 69252.545 - Backtracking...\n",
      "356 - loss: 69252.545\n",
      "f_new: 69261.867 - f: 69252.545 - Backtracking...\n",
      "358 - loss: 69252.545\n",
      "f_new: 69261.579 - f: 69252.545 - Backtracking...\n",
      "360 - loss: 69252.545\n",
      "f_new: 69261.296 - f: 69252.545 - Backtracking...\n",
      "362 - loss: 69252.545\n",
      "f_new: 69261.016 - f: 69252.545 - Backtracking...\n",
      "364 - loss: 69252.545\n",
      "f_new: 69260.741 - f: 69252.545 - Backtracking...\n",
      "366 - loss: 69252.545\n",
      "f_new: 69260.469 - f: 69252.545 - Backtracking...\n",
      "368 - loss: 69252.545\n",
      "f_new: 69260.201 - f: 69252.545 - Backtracking...\n",
      "370 - loss: 69252.545\n",
      "f_new: 69259.937 - f: 69252.545 - Backtracking...\n",
      "372 - loss: 69252.545\n",
      "f_new: 69259.675 - f: 69252.545 - Backtracking...\n",
      "374 - loss: 69252.545\n",
      "f_new: 69259.417 - f: 69252.545 - Backtracking...\n",
      "376 - loss: 69252.545\n",
      "f_new: 69259.162 - f: 69252.545 - Backtracking...\n",
      "378 - loss: 69252.545\n",
      "f_new: 69258.909 - f: 69252.545 - Backtracking...\n",
      "380 - loss: 69252.544\n",
      "f_new: 69258.660 - f: 69252.544 - Backtracking...\n",
      "382 - loss: 69252.544\n",
      "f_new: 69258.412 - f: 69252.544 - Backtracking...\n",
      "384 - loss: 69252.544\n",
      "f_new: 69258.167 - f: 69252.544 - Backtracking...\n",
      "386 - loss: 69252.544\n",
      "f_new: 69257.924 - f: 69252.544 - Backtracking...\n",
      "388 - loss: 69252.544\n",
      "f_new: 69257.682 - f: 69252.544 - Backtracking...\n",
      "390 - loss: 69252.544\n",
      "f_new: 69257.442 - f: 69252.544 - Backtracking...\n",
      "392 - loss: 69252.544\n",
      "f_new: 69257.204 - f: 69252.544 - Backtracking...\n",
      "394 - loss: 69252.544\n",
      "f_new: 69256.967 - f: 69252.544 - Backtracking...\n",
      "396 - loss: 69252.544\n",
      "f_new: 69256.731 - f: 69252.544 - Backtracking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398 - loss: 69252.544\n",
      "f_new: 69256.496 - f: 69252.544 - Backtracking...\n",
      "400 - loss: 69252.544\n",
      "Reached maximum number of function evaluations 400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3692912834165724"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing regularized logistic regression \n",
    "# seems to be absolute shit for this \n",
    "classifier = LogisticRegressionL2(0.01, verbose=True, max_evaluations=400)\n",
    "classifier.fit(y_split[0], X_split_poly[0])\n",
    "np.mean(y_split[0] == classifier.predict(X_split_poly[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.543932\n"
     ]
    }
   ],
   "source": [
    "kernel = LeastSquaresKernel(Kernel.kernel_poly, verbose=True, max_evals=300)\n",
    "p = 4\n",
    "lambda_ = 0\n",
    "incr = 40\n",
    "incr_pred = 30\n",
    "pred = kernel.predict(y[::incr], X__[::incr], X__[::incr_pred], p, lambda_=lambda_)\n",
    "print(np.mean(y[::incr_pred] == pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = replace_NaN_by_median(X)\n",
    "X, _, _ = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub = X[::30]\n",
    "y_sub = y[::30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_new: 351332120652.464 - f: 5776.689 - Backtracking...\n",
      "f_new: 49908376999.952 - f: 5776.689 - Backtracking...\n",
      "f_new: 7089719742.161 - f: 5776.689 - Backtracking...\n",
      "f_new: 1007128546.313 - f: 5776.689 - Backtracking...\n",
      "f_new: 143067923.558 - f: 5776.689 - Backtracking...\n",
      "f_new: 20324057.388 - f: 5776.689 - Backtracking...\n",
      "f_new: 2887715.309 - f: 5776.689 - Backtracking...\n",
      "f_new: 410801.817 - f: 5776.689 - Backtracking...\n",
      "f_new: 58949.841 - f: 5776.689 - Backtracking...\n",
      "f_new: 9667.717 - f: 5776.689 - Backtracking...\n",
      "11 - loss: 5549.281\n",
      "12 - loss: 5405.800\n",
      "13 - loss: 5352.943\n",
      "14 - loss: 5260.879\n",
      "15 - loss: 5057.649\n",
      "f_new: 5188.842 - f: 5057.649 - Backtracking...\n",
      "17 - loss: 5042.088\n",
      "18 - loss: 5030.287\n",
      "19 - loss: 5022.815\n",
      "20 - loss: 4723.083\n",
      "f_new: 5530.003 - f: 4723.083 - Backtracking...\n",
      "22 - loss: 4714.058\n",
      "23 - loss: 4702.917\n",
      "24 - loss: 4700.799\n",
      "25 - loss: 4698.556\n",
      "26 - loss: 4630.235\n",
      "27 - loss: 4608.311\n",
      "f_new: 4631.497 - f: 4608.311 - Backtracking...\n",
      "29 - loss: 4598.083\n",
      "30 - loss: 4596.268\n",
      "31 - loss: 4594.125\n",
      "32 - loss: 4582.093\n",
      "33 - loss: 4575.996\n",
      "34 - loss: 4550.599\n",
      "f_new: 4573.412 - f: 4550.599 - Backtracking...\n",
      "36 - loss: 4549.247\n",
      "37 - loss: 4549.045\n",
      "38 - loss: 4548.844\n",
      "39 - loss: 4529.424\n",
      "f_new: 4534.597 - f: 4529.424 - Backtracking...\n",
      "41 - loss: 4526.565\n",
      "f_new: 4531.257 - f: 4526.565 - Backtracking...\n",
      "43 - loss: 4525.883\n",
      "44 - loss: 4525.601\n",
      "45 - loss: 4525.368\n",
      "46 - loss: 4523.787\n",
      "47 - loss: 4522.931\n",
      "48 - loss: 4522.724\n",
      "49 - loss: 4522.637\n",
      "50 - loss: 4522.573\n",
      "51 - loss: 4521.830\n",
      "52 - loss: 4503.879\n",
      "f_new: 4512.120 - f: 4503.879 - Backtracking...\n",
      "54 - loss: 4503.280\n",
      "f_new: 4504.387 - f: 4503.280 - Backtracking...\n",
      "56 - loss: 4503.135\n",
      "57 - loss: 4503.078\n",
      "58 - loss: 4503.030\n",
      "59 - loss: 4502.749\n",
      "60 - loss: 4502.654\n",
      "61 - loss: 4502.642\n",
      "62 - loss: 4502.635\n",
      "63 - loss: 4502.627\n",
      "64 - loss: 4499.973\n",
      "65 - loss: 4499.873\n",
      "f_new: 4501.201 - f: 4499.873 - Backtracking...\n",
      "67 - loss: 4499.465\n",
      "68 - loss: 4499.452\n",
      "69 - loss: 4499.420\n",
      "70 - loss: 4499.313\n",
      "71 - loss: 4499.241\n",
      "72 - loss: 4499.151\n",
      "73 - loss: 4499.114\n",
      "74 - loss: 4499.100\n",
      "75 - loss: 4499.098\n",
      "76 - loss: 4498.991\n",
      "77 - loss: 4498.931\n",
      "78 - loss: 4498.902\n",
      "f_new: 4498.948 - f: 4498.902 - Backtracking...\n",
      "80 - loss: 4498.897\n",
      "81 - loss: 4498.896\n",
      "82 - loss: 4498.895\n",
      "83 - loss: 4498.882\n",
      "84 - loss: 4498.868\n",
      "85 - loss: 4498.805\n",
      "86 - loss: 4498.798\n",
      "87 - loss: 4498.741\n",
      "88 - loss: 4498.741\n",
      "89 - loss: 4498.740\n",
      "90 - loss: 4498.728\n",
      "91 - loss: 4498.716\n",
      "92 - loss: 4498.636\n",
      "93 - loss: 4498.474\n",
      "94 - loss: 4498.428\n",
      "95 - loss: 4498.427\n",
      "96 - loss: 4498.427\n",
      "97 - loss: 4498.417\n",
      "98 - loss: 4498.407\n",
      "99 - loss: 4497.942\n",
      "f_new: 4498.956 - f: 4497.942 - Backtracking...\n",
      "101 - loss: 4497.912\n",
      "102 - loss: 4497.882\n",
      "103 - loss: 4497.881\n",
      "104 - loss: 4497.880\n",
      "105 - loss: 4497.852\n",
      "106 - loss: 4497.820\n",
      "107 - loss: 4497.806\n",
      "108 - loss: 4497.796\n",
      "109 - loss: 4497.795\n",
      "110 - loss: 4497.794\n",
      "111 - loss: 4497.793\n",
      "112 - loss: 4497.354\n",
      "113 - loss: 4496.800\n",
      "114 - loss: 4496.751\n",
      "f_new: 4496.845 - f: 4496.751 - Backtracking...\n",
      "116 - loss: 4496.748\n",
      "117 - loss: 4496.747\n",
      "118 - loss: 4496.746\n",
      "119 - loss: 4496.737\n",
      "120 - loss: 4496.728\n",
      "121 - loss: 4496.620\n",
      "122 - loss: 4496.497\n",
      "123 - loss: 4496.491\n",
      "124 - loss: 4496.490\n",
      "125 - loss: 4496.489\n",
      "126 - loss: 4496.475\n",
      "127 - loss: 4496.460\n",
      "128 - loss: 4496.432\n",
      "129 - loss: 4496.381\n",
      "130 - loss: 4496.363\n",
      "131 - loss: 4496.360\n",
      "132 - loss: 4496.360\n",
      "133 - loss: 4496.359\n",
      "134 - loss: 4496.353\n",
      "135 - loss: 4496.343\n",
      "136 - loss: 4496.325\n",
      "137 - loss: 4496.039\n",
      "138 - loss: 4496.038\n",
      "139 - loss: 4496.037\n",
      "140 - loss: 4496.036\n",
      "141 - loss: 4496.026\n",
      "142 - loss: 4496.015\n",
      "143 - loss: 4495.933\n",
      "144 - loss: 4493.641\n",
      "f_new: 4493.794 - f: 4493.641 - Backtracking...\n",
      "146 - loss: 4493.636\n",
      "147 - loss: 4493.634\n",
      "148 - loss: 4493.633\n",
      "149 - loss: 4493.621\n",
      "150 - loss: 4493.613\n",
      "151 - loss: 4493.612\n",
      "152 - loss: 4493.611\n",
      "153 - loss: 4493.611\n",
      "154 - loss: 4468.050\n",
      "f_new: 17878.460 - f: 4468.050 - Backtracking...\n",
      "f_new: 4468.051 - f: 4468.050 - Backtracking...\n",
      "157 - loss: 4468.048\n",
      "158 - loss: 4468.046\n",
      "159 - loss: 4468.046\n",
      "160 - loss: 4468.045\n",
      "161 - loss: 4468.032\n",
      "162 - loss: 4468.018\n",
      "163 - loss: 4468.005\n",
      "164 - loss: 4468.003\n",
      "165 - loss: 4468.002\n",
      "166 - loss: 4468.002\n",
      "167 - loss: 4468.000\n",
      "168 - loss: 4467.988\n",
      "169 - loss: 4467.972\n",
      "170 - loss: 4467.958\n",
      "171 - loss: 4467.956\n",
      "172 - loss: 4467.955\n",
      "173 - loss: 4467.954\n",
      "174 - loss: 4467.954\n",
      "175 - loss: 4467.945\n",
      "176 - loss: 4467.935\n",
      "177 - loss: 4467.921\n",
      "178 - loss: 4467.908\n",
      "179 - loss: 4467.908\n",
      "180 - loss: 4467.907\n",
      "181 - loss: 4467.907\n",
      "182 - loss: 4467.899\n",
      "183 - loss: 4467.892\n",
      "184 - loss: 4467.873\n",
      "185 - loss: 4467.829\n",
      "186 - loss: 4467.828\n",
      "187 - loss: 4467.828\n",
      "188 - loss: 4467.828\n",
      "189 - loss: 4467.822\n",
      "190 - loss: 4467.817\n",
      "191 - loss: 4467.605\n",
      "192 - loss: 4467.590\n",
      "193 - loss: 4467.517\n",
      "194 - loss: 4467.516\n",
      "195 - loss: 4467.516\n",
      "196 - loss: 4467.484\n",
      "197 - loss: 4467.446\n",
      "198 - loss: 4467.440\n",
      "199 - loss: 4467.434\n",
      "200 - loss: 4467.433\n",
      "201 - loss: 4467.433\n",
      "202 - loss: 4467.433\n",
      "203 - loss: 4467.224\n",
      "204 - loss: 4461.692\n",
      "f_new: 4465.338 - f: 4461.692 - Backtracking...\n",
      "206 - loss: 4461.682\n",
      "207 - loss: 4461.676\n",
      "208 - loss: 4461.674\n",
      "209 - loss: 4461.673\n",
      "210 - loss: 4461.665\n",
      "211 - loss: 4461.660\n",
      "212 - loss: 4461.659\n",
      "213 - loss: 4461.659\n",
      "214 - loss: 4461.659\n",
      "215 - loss: 4461.646\n",
      "216 - loss: 4461.562\n",
      "217 - loss: 4461.462\n",
      "218 - loss: 4461.461\n",
      "219 - loss: 4461.461\n",
      "220 - loss: 4461.460\n",
      "221 - loss: 4461.445\n",
      "222 - loss: 4461.427\n",
      "223 - loss: 4461.421\n",
      "224 - loss: 4461.416\n",
      "225 - loss: 4461.412\n",
      "226 - loss: 4461.412\n",
      "227 - loss: 4461.411\n",
      "228 - loss: 4461.411\n",
      "229 - loss: 4460.963\n",
      "230 - loss: 4460.522\n",
      "f_new: 4460.592 - f: 4460.522 - Backtracking...\n",
      "232 - loss: 4460.519\n",
      "233 - loss: 4460.517\n",
      "234 - loss: 4460.516\n",
      "235 - loss: 4460.513\n",
      "236 - loss: 4460.506\n",
      "237 - loss: 4460.502\n",
      "238 - loss: 4460.501\n",
      "239 - loss: 4460.500\n",
      "240 - loss: 4460.500\n",
      "241 - loss: 4460.446\n",
      "242 - loss: 4460.396\n",
      "243 - loss: 4460.393\n",
      "244 - loss: 4460.392\n",
      "245 - loss: 4460.392\n",
      "246 - loss: 4460.389\n",
      "247 - loss: 4460.365\n",
      "248 - loss: 4460.297\n",
      "249 - loss: 4460.287\n",
      "250 - loss: 4460.282\n",
      "251 - loss: 4460.280\n",
      "252 - loss: 4460.280\n",
      "253 - loss: 4460.279\n",
      "254 - loss: 4460.246\n",
      "255 - loss: 4453.774\n",
      "f_new: 4465.565 - f: 4453.774 - Backtracking...\n",
      "257 - loss: 4453.764\n",
      "258 - loss: 4453.761\n",
      "259 - loss: 4453.759\n",
      "260 - loss: 4453.759\n",
      "261 - loss: 4453.758\n",
      "262 - loss: 4453.748\n",
      "263 - loss: 4453.736\n",
      "264 - loss: 4453.730\n",
      "265 - loss: 4453.725\n",
      "266 - loss: 4453.721\n",
      "267 - loss: 4453.720\n",
      "268 - loss: 4453.720\n",
      "269 - loss: 4453.720\n",
      "270 - loss: 4453.154\n",
      "271 - loss: 4452.624\n",
      "f_new: 4452.717 - f: 4452.624 - Backtracking...\n",
      "273 - loss: 4452.598\n",
      "274 - loss: 4452.582\n",
      "275 - loss: 4452.577\n",
      "276 - loss: 4452.574\n",
      "277 - loss: 4452.553\n",
      "278 - loss: 4452.546\n",
      "279 - loss: 4452.541\n",
      "280 - loss: 4452.537\n",
      "281 - loss: 4452.536\n",
      "282 - loss: 4452.536\n",
      "283 - loss: 4452.509\n",
      "284 - loss: 4452.487\n",
      "285 - loss: 4452.478\n",
      "286 - loss: 4452.476\n",
      "287 - loss: 4452.476\n",
      "288 - loss: 4452.476\n",
      "289 - loss: 4452.458\n",
      "290 - loss: 4452.438\n",
      "291 - loss: 4452.434\n",
      "292 - loss: 4452.430\n",
      "293 - loss: 4452.428\n",
      "294 - loss: 4452.428\n",
      "295 - loss: 4452.428\n",
      "296 - loss: 4452.420\n",
      "297 - loss: 4452.183\n",
      "298 - loss: 4451.893\n",
      "f_new: 4451.894 - f: 4451.893 - Backtracking...\n",
      "300 - loss: 4451.891\n",
      "301 - loss: 4451.891\n",
      "302 - loss: 4451.891\n",
      "303 - loss: 4451.886\n",
      "304 - loss: 4451.882\n",
      "305 - loss: 4451.871\n",
      "306 - loss: 4451.870\n",
      "307 - loss: 4451.870\n",
      "308 - loss: 4451.869\n",
      "309 - loss: 4451.818\n",
      "310 - loss: 4451.767\n",
      "311 - loss: 4451.711\n",
      "312 - loss: 4451.709\n",
      "313 - loss: 4451.709\n",
      "314 - loss: 4451.709\n",
      "315 - loss: 4451.705\n",
      "316 - loss: 4451.701\n",
      "317 - loss: 4451.073\n",
      "f_new: 4452.226 - f: 4451.073 - Backtracking...\n",
      "319 - loss: 4451.067\n",
      "320 - loss: 4451.061\n",
      "321 - loss: 4451.060\n",
      "322 - loss: 4451.060\n",
      "323 - loss: 4451.017\n",
      "324 - loss: 4450.972\n",
      "325 - loss: 4450.967\n",
      "326 - loss: 4450.963\n",
      "327 - loss: 4450.963\n",
      "328 - loss: 4450.963\n",
      "329 - loss: 4450.962\n",
      "330 - loss: 4450.785\n",
      "331 - loss: 4450.594\n",
      "332 - loss: 4450.583\n",
      "f_new: 4450.584 - f: 4450.583 - Backtracking...\n",
      "334 - loss: 4450.582\n",
      "335 - loss: 4450.581\n",
      "336 - loss: 4450.581\n",
      "337 - loss: 4450.580\n",
      "338 - loss: 4450.575\n",
      "339 - loss: 4450.569\n",
      "340 - loss: 4450.555\n",
      "341 - loss: 4450.551\n",
      "342 - loss: 4450.551\n",
      "343 - loss: 4450.550\n",
      "344 - loss: 4450.550\n",
      "345 - loss: 4450.521\n",
      "346 - loss: 4450.491\n",
      "347 - loss: 4450.487\n",
      "348 - loss: 4450.483\n",
      "349 - loss: 4450.483\n",
      "350 - loss: 4450.482\n",
      "351 - loss: 4450.482\n",
      "352 - loss: 4447.174\n",
      "f_new: 4494.868 - f: 4447.174 - Backtracking...\n",
      "354 - loss: 4446.870\n",
      "355 - loss: 4446.776\n",
      "356 - loss: 4446.682\n",
      "357 - loss: 4446.678\n",
      "358 - loss: 4446.626\n",
      "359 - loss: 4446.595\n",
      "360 - loss: 4446.566\n",
      "f_new: 4446.600 - f: 4446.566 - Backtracking...\n",
      "362 - loss: 4446.564\n",
      "363 - loss: 4446.562\n",
      "364 - loss: 4446.562\n",
      "365 - loss: 4446.561\n",
      "366 - loss: 4446.545\n",
      "367 - loss: 4446.533\n",
      "368 - loss: 4446.529\n",
      "f_new: 4446.530 - f: 4446.529 - Backtracking...\n",
      "370 - loss: 4446.528\n",
      "371 - loss: 4446.528\n",
      "372 - loss: 4446.527\n",
      "373 - loss: 4446.523\n",
      "374 - loss: 4446.519\n",
      "375 - loss: 4446.508\n",
      "376 - loss: 4446.491\n",
      "377 - loss: 4446.491\n",
      "378 - loss: 4446.491\n",
      "379 - loss: 4446.490\n",
      "380 - loss: 4446.487\n",
      "381 - loss: 4446.483\n",
      "382 - loss: 4446.426\n",
      "383 - loss: 4446.209\n",
      "384 - loss: 4446.208\n",
      "385 - loss: 4446.207\n",
      "386 - loss: 4446.207\n",
      "387 - loss: 4446.203\n",
      "388 - loss: 4446.199\n",
      "389 - loss: 4446.186\n",
      "390 - loss: 4446.165\n",
      "391 - loss: 4446.162\n",
      "392 - loss: 4446.162\n",
      "393 - loss: 4446.162\n",
      "394 - loss: 4446.161\n",
      "395 - loss: 4446.144\n",
      "396 - loss: 4446.124\n",
      "397 - loss: 4446.119\n",
      "398 - loss: 4446.116\n",
      "399 - loss: 4446.114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 - loss: 4446.114\n",
      "401 - loss: 4446.114\n",
      "402 - loss: 4446.112\n",
      "403 - loss: 4439.172\n",
      "f_new: 6378.863 - f: 4439.172 - Backtracking...\n",
      "405 - loss: 4439.165\n",
      "406 - loss: 4439.154\n",
      "407 - loss: 4439.154\n",
      "408 - loss: 4439.154\n",
      "409 - loss: 4439.151\n",
      "410 - loss: 4439.148\n",
      "411 - loss: 4439.077\n",
      "f_new: 4439.109 - f: 4439.077 - Backtracking...\n",
      "413 - loss: 4439.072\n",
      "414 - loss: 4439.067\n",
      "415 - loss: 4439.067\n",
      "416 - loss: 4439.066\n",
      "417 - loss: 4438.991\n",
      "418 - loss: 4438.892\n",
      "419 - loss: 4438.888\n",
      "f_new: 4438.888 - f: 4438.888 - Backtracking...\n",
      "421 - loss: 4438.887\n",
      "422 - loss: 4438.887\n",
      "423 - loss: 4438.887\n",
      "424 - loss: 4438.884\n",
      "425 - loss: 4438.881\n",
      "426 - loss: 4438.876\n",
      "427 - loss: 4438.798\n",
      "428 - loss: 4438.691\n",
      "429 - loss: 4438.690\n",
      "430 - loss: 4438.689\n",
      "431 - loss: 4438.689\n",
      "432 - loss: 4438.688\n",
      "433 - loss: 4438.685\n",
      "434 - loss: 4438.682\n",
      "435 - loss: 4438.675\n",
      "436 - loss: 4438.674\n",
      "437 - loss: 4438.674\n",
      "438 - loss: 4438.674\n",
      "439 - loss: 4438.644\n",
      "440 - loss: 4438.614\n",
      "441 - loss: 4438.574\n",
      "442 - loss: 4438.573\n",
      "443 - loss: 4438.572\n",
      "444 - loss: 4438.572\n",
      "445 - loss: 4438.572\n",
      "446 - loss: 4438.569\n",
      "447 - loss: 4438.566\n",
      "448 - loss: 4438.536\n",
      "449 - loss: 4438.519\n",
      "450 - loss: 4438.518\n",
      "451 - loss: 4438.518\n",
      "452 - loss: 4438.518\n",
      "453 - loss: 4438.508\n",
      "454 - loss: 4438.498\n",
      "455 - loss: 4438.494\n",
      "456 - loss: 4438.491\n",
      "457 - loss: 4438.487\n",
      "458 - loss: 4438.487\n",
      "459 - loss: 4438.487\n",
      "460 - loss: 4438.487\n",
      "461 - loss: 4437.533\n",
      "f_new: 4442.972 - f: 4437.533 - Backtracking...\n",
      "463 - loss: 4436.354\n",
      "464 - loss: 4435.380\n",
      "f_new: 4437.712 - f: 4435.380 - Backtracking...\n",
      "466 - loss: 4435.229\n",
      "467 - loss: 4435.229\n",
      "468 - loss: 4435.228\n",
      "469 - loss: 4435.225\n",
      "470 - loss: 4435.222\n",
      "471 - loss: 4435.211\n",
      "472 - loss: 4435.210\n",
      "473 - loss: 4435.210\n",
      "474 - loss: 4435.209\n",
      "475 - loss: 4435.198\n",
      "476 - loss: 4435.186\n",
      "477 - loss: 4435.183\n",
      "478 - loss: 4435.181\n",
      "479 - loss: 4435.180\n",
      "480 - loss: 4435.180\n",
      "481 - loss: 4435.180\n",
      "482 - loss: 4434.978\n",
      "483 - loss: 4434.638\n",
      "484 - loss: 4434.624\n",
      "f_new: 4434.651 - f: 4434.624 - Backtracking...\n",
      "486 - loss: 4434.622\n",
      "487 - loss: 4434.621\n",
      "488 - loss: 4434.620\n",
      "489 - loss: 4434.614\n",
      "490 - loss: 4434.612\n",
      "491 - loss: 4434.611\n",
      "492 - loss: 4434.611\n",
      "493 - loss: 4434.611\n",
      "494 - loss: 4434.592\n",
      "495 - loss: 4434.573\n",
      "496 - loss: 4434.544\n",
      "497 - loss: 4434.543\n",
      "498 - loss: 4434.543\n",
      "499 - loss: 4434.543\n",
      "500 - loss: 4434.539\n",
      "501 - loss: 4434.534\n",
      "502 - loss: 4434.528\n",
      "503 - loss: 4434.518\n",
      "504 - loss: 4434.513\n",
      "505 - loss: 4434.513\n",
      "506 - loss: 4434.513\n",
      "507 - loss: 4434.513\n",
      "508 - loss: 4434.509\n",
      "509 - loss: 4434.506\n",
      "510 - loss: 4434.490\n",
      "511 - loss: 4434.389\n",
      "512 - loss: 4434.388\n",
      "513 - loss: 4434.388\n",
      "514 - loss: 4434.388\n",
      "515 - loss: 4434.385\n",
      "516 - loss: 4434.383\n",
      "517 - loss: 4432.661\n",
      "f_new: 4435.044 - f: 4432.661 - Backtracking...\n",
      "519 - loss: 4432.661\n",
      "520 - loss: 4432.661\n",
      "521 - loss: 4432.661\n",
      "522 - loss: 4432.635\n",
      "523 - loss: 4432.609\n",
      "524 - loss: 4432.606\n",
      "525 - loss: 4432.604\n",
      "526 - loss: 4432.603\n",
      "527 - loss: 4432.603\n",
      "528 - loss: 4432.603\n",
      "529 - loss: 4432.582\n",
      "530 - loss: 4432.445\n",
      "531 - loss: 4431.995\n",
      "f_new: 4432.004 - f: 4431.995 - Backtracking...\n",
      "533 - loss: 4431.993\n",
      "534 - loss: 4431.993\n",
      "535 - loss: 4431.993\n",
      "536 - loss: 4431.989\n",
      "537 - loss: 4431.986\n",
      "538 - loss: 4431.977\n",
      "539 - loss: 4431.977\n",
      "540 - loss: 4431.976\n",
      "541 - loss: 4431.976\n",
      "542 - loss: 4431.966\n",
      "543 - loss: 4431.955\n",
      "544 - loss: 4431.952\n",
      "545 - loss: 4431.950\n",
      "546 - loss: 4431.950\n",
      "547 - loss: 4431.950\n",
      "548 - loss: 4431.949\n",
      "549 - loss: 4431.870\n",
      "550 - loss: 4431.790\n",
      "551 - loss: 4431.775\n",
      "552 - loss: 4431.773\n",
      "553 - loss: 4431.772\n",
      "554 - loss: 4431.772\n",
      "555 - loss: 4431.772\n",
      "556 - loss: 4431.769\n",
      "557 - loss: 4431.767\n",
      "558 - loss: 4431.746\n",
      "559 - loss: 4431.696\n",
      "560 - loss: 4431.688\n",
      "561 - loss: 4431.687\n",
      "562 - loss: 4431.686\n",
      "563 - loss: 4431.686\n",
      "564 - loss: 4431.684\n",
      "565 - loss: 4431.681\n",
      "566 - loss: 4431.554\n",
      "567 - loss: 4431.105\n",
      "f_new: 4431.108 - f: 4431.105 - Backtracking...\n",
      "569 - loss: 4431.103\n",
      "570 - loss: 4431.103\n",
      "571 - loss: 4431.102\n",
      "572 - loss: 4431.091\n",
      "573 - loss: 4431.082\n",
      "574 - loss: 4431.079\n",
      "575 - loss: 4431.077\n",
      "576 - loss: 4431.077\n",
      "577 - loss: 4431.077\n",
      "578 - loss: 4431.076\n",
      "579 - loss: 4431.065\n",
      "580 - loss: 4431.054\n",
      "581 - loss: 4431.052\n",
      "582 - loss: 4431.049\n",
      "583 - loss: 4431.047\n",
      "584 - loss: 4431.047\n",
      "585 - loss: 4431.046\n",
      "586 - loss: 4431.046\n",
      "587 - loss: 4429.409\n",
      "588 - loss: 4428.748\n",
      "f_new: 4435.013 - f: 4428.748 - Backtracking...\n",
      "590 - loss: 4428.595\n",
      "591 - loss: 4428.578\n",
      "592 - loss: 4428.559\n",
      "593 - loss: 4427.957\n",
      "594 - loss: 4427.932\n",
      "f_new: 4428.104 - f: 4427.932 - Backtracking...\n",
      "596 - loss: 4427.866\n",
      "597 - loss: 4427.826\n",
      "598 - loss: 4427.813\n",
      "599 - loss: 4427.805\n",
      "600 - loss: 4427.750\n",
      "601 - loss: 4427.734\n",
      "602 - loss: 4427.729\n",
      "603 - loss: 4427.726\n",
      "604 - loss: 4427.725\n",
      "605 - loss: 4427.722\n",
      "606 - loss: 4427.592\n",
      "607 - loss: 4427.571\n",
      "608 - loss: 4427.564\n",
      "609 - loss: 4427.560\n",
      "610 - loss: 4427.559\n",
      "611 - loss: 4427.555\n",
      "612 - loss: 4427.550\n",
      "613 - loss: 4427.546\n",
      "614 - loss: 4427.542\n",
      "615 - loss: 4427.541\n",
      "616 - loss: 4427.540\n",
      "617 - loss: 4427.540\n",
      "618 - loss: 4427.540\n",
      "619 - loss: 4427.534\n",
      "620 - loss: 4427.527\n",
      "621 - loss: 4427.525\n",
      "622 - loss: 4427.522\n",
      "623 - loss: 4427.522\n",
      "624 - loss: 4427.522\n",
      "625 - loss: 4427.522\n",
      "626 - loss: 4427.505\n",
      "627 - loss: 4427.488\n",
      "628 - loss: 4427.480\n",
      "629 - loss: 4427.478\n",
      "630 - loss: 4427.476\n",
      "631 - loss: 4427.475\n",
      "632 - loss: 4427.475\n",
      "633 - loss: 4427.475\n",
      "634 - loss: 4425.864\n",
      "f_new: 4425.902 - f: 4425.864 - Backtracking...\n",
      "636 - loss: 4425.048\n",
      "f_new: 4431.087 - f: 4425.048 - Backtracking...\n",
      "638 - loss: 4424.960\n",
      "639 - loss: 4424.934\n",
      "640 - loss: 4424.909\n",
      "641 - loss: 4424.625\n",
      "642 - loss: 4424.465\n",
      "643 - loss: 4424.461\n",
      "644 - loss: 4424.348\n",
      "645 - loss: 4424.340\n",
      "646 - loss: 4424.338\n",
      "647 - loss: 4424.272\n",
      "648 - loss: 4424.254\n",
      "649 - loss: 4424.245\n",
      "f_new: 4424.256 - f: 4424.245 - Backtracking...\n",
      "651 - loss: 4424.240\n",
      "652 - loss: 4424.240\n",
      "653 - loss: 4424.239\n",
      "654 - loss: 4424.235\n",
      "655 - loss: 4424.230\n",
      "656 - loss: 4424.221\n",
      "f_new: 4424.222 - f: 4424.221 - Backtracking...\n",
      "658 - loss: 4424.220\n",
      "659 - loss: 4424.219\n",
      "660 - loss: 4424.219\n",
      "661 - loss: 4424.217\n",
      "662 - loss: 4424.201\n",
      "663 - loss: 4424.171\n",
      "f_new: 4424.175 - f: 4424.171 - Backtracking...\n",
      "665 - loss: 4424.170\n",
      "666 - loss: 4424.169\n",
      "667 - loss: 4424.169\n",
      "668 - loss: 4424.168\n",
      "669 - loss: 4424.166\n",
      "670 - loss: 4424.164\n",
      "671 - loss: 4424.135\n",
      "672 - loss: 4424.078\n",
      "673 - loss: 4424.078\n",
      "674 - loss: 4424.077\n",
      "675 - loss: 4424.077\n",
      "676 - loss: 4424.075\n",
      "677 - loss: 4424.073\n",
      "678 - loss: 4424.062\n",
      "679 - loss: 4424.050\n",
      "680 - loss: 4424.049\n",
      "681 - loss: 4424.049\n",
      "682 - loss: 4424.049\n",
      "683 - loss: 4424.035\n",
      "684 - loss: 4424.018\n",
      "685 - loss: 4424.016\n",
      "686 - loss: 4424.014\n",
      "687 - loss: 4424.012\n",
      "688 - loss: 4424.012\n",
      "689 - loss: 4424.012\n",
      "690 - loss: 4424.011\n",
      "691 - loss: 4423.206\n",
      "692 - loss: 4422.442\n",
      "f_new: 4422.453 - f: 4422.442 - Backtracking...\n",
      "694 - loss: 4422.425\n",
      "695 - loss: 4422.425\n",
      "696 - loss: 4422.424\n",
      "697 - loss: 4422.421\n",
      "698 - loss: 4422.419\n",
      "699 - loss: 4422.409\n",
      "700 - loss: 4422.408\n",
      "701 - loss: 4422.408\n",
      "702 - loss: 4422.408\n",
      "703 - loss: 4422.314\n",
      "704 - loss: 4422.227\n",
      "f_new: 4422.263 - f: 4422.227 - Backtracking...\n",
      "706 - loss: 4422.227\n",
      "707 - loss: 4422.227\n",
      "708 - loss: 4422.227\n",
      "709 - loss: 4422.224\n",
      "710 - loss: 4422.221\n",
      "711 - loss: 4422.216\n",
      "712 - loss: 4422.212\n",
      "713 - loss: 4422.211\n",
      "714 - loss: 4422.211\n",
      "715 - loss: 4422.211\n",
      "716 - loss: 4422.208\n",
      "717 - loss: 4422.205\n",
      "718 - loss: 4422.200\n",
      "719 - loss: 4422.192\n",
      "720 - loss: 4422.191\n",
      "721 - loss: 4422.191\n",
      "722 - loss: 4422.190\n",
      "723 - loss: 4422.190\n",
      "724 - loss: 4422.188\n",
      "725 - loss: 4422.185\n",
      "726 - loss: 4422.177\n",
      "727 - loss: 4422.161\n",
      "728 - loss: 4422.161\n",
      "729 - loss: 4422.161\n",
      "730 - loss: 4422.161\n",
      "731 - loss: 4422.159\n",
      "732 - loss: 4422.157\n",
      "733 - loss: 4422.108\n",
      "734 - loss: 4422.046\n",
      "735 - loss: 4422.043\n",
      "736 - loss: 4422.043\n",
      "737 - loss: 4422.042\n",
      "738 - loss: 4422.040\n",
      "739 - loss: 4422.038\n",
      "740 - loss: 4422.021\n",
      "741 - loss: 4421.875\n",
      "742 - loss: 4421.875\n",
      "743 - loss: 4421.873\n",
      "744 - loss: 4421.873\n",
      "745 - loss: 4421.873\n",
      "746 - loss: 4421.861\n",
      "747 - loss: 4421.847\n",
      "748 - loss: 4421.845\n",
      "749 - loss: 4421.843\n",
      "750 - loss: 4421.842\n",
      "751 - loss: 4421.842\n",
      "752 - loss: 4421.842\n",
      "753 - loss: 4421.841\n",
      "754 - loss: 4420.840\n",
      "755 - loss: 4420.055\n",
      "f_new: 4420.469 - f: 4420.055 - Backtracking...\n",
      "757 - loss: 4419.993\n",
      "758 - loss: 4419.989\n",
      "759 - loss: 4419.983\n",
      "760 - loss: 4419.956\n",
      "761 - loss: 4419.937\n",
      "762 - loss: 4419.890\n",
      "763 - loss: 4419.880\n",
      "764 - loss: 4419.874\n",
      "765 - loss: 4419.873\n",
      "766 - loss: 4419.797\n",
      "f_new: 4419.812 - f: 4419.797 - Backtracking...\n",
      "768 - loss: 4419.791\n",
      "769 - loss: 4419.787\n",
      "f_new: 4419.788 - f: 4419.787 - Backtracking...\n",
      "771 - loss: 4419.785\n",
      "772 - loss: 4419.784\n",
      "773 - loss: 4419.784\n",
      "774 - loss: 4419.756\n",
      "775 - loss: 4419.728\n",
      "776 - loss: 4419.725\n",
      "777 - loss: 4419.724\n",
      "778 - loss: 4419.724\n",
      "779 - loss: 4419.723\n",
      "780 - loss: 4419.723\n",
      "781 - loss: 4419.713\n",
      "782 - loss: 4419.701\n",
      "783 - loss: 4419.699\n",
      "784 - loss: 4419.697\n",
      "785 - loss: 4419.697\n",
      "786 - loss: 4419.697\n",
      "787 - loss: 4419.697\n",
      "788 - loss: 4418.609\n",
      "789 - loss: 4418.283\n",
      "f_new: 4419.368 - f: 4418.283 - Backtracking...\n",
      "791 - loss: 4417.996\n",
      "792 - loss: 4417.806\n",
      "793 - loss: 4417.744\n",
      "794 - loss: 4417.683\n",
      "795 - loss: 4417.422\n",
      "f_new: 4417.425 - f: 4417.422 - Backtracking...\n",
      "797 - loss: 4417.421\n",
      "798 - loss: 4417.420\n",
      "799 - loss: 4417.420\n",
      "800 - loss: 4417.419\n",
      "801 - loss: 4417.413\n",
      "802 - loss: 4417.404\n",
      "803 - loss: 4417.398\n",
      "804 - loss: 4417.397\n",
      "805 - loss: 4417.396\n",
      "806 - loss: 4417.396\n",
      "807 - loss: 4417.393\n",
      "808 - loss: 4417.389\n",
      "809 - loss: 4417.378\n",
      "810 - loss: 4417.346\n",
      "f_new: 4417.350 - f: 4417.346 - Backtracking...\n",
      "812 - loss: 4417.345\n",
      "813 - loss: 4417.345\n",
      "814 - loss: 4417.344\n",
      "815 - loss: 4417.342\n",
      "816 - loss: 4417.340\n",
      "817 - loss: 4417.337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "818 - loss: 4417.337\n",
      "819 - loss: 4417.337\n",
      "820 - loss: 4417.337\n",
      "821 - loss: 4417.236\n",
      "822 - loss: 4417.145\n",
      "f_new: 4417.199 - f: 4417.145 - Backtracking...\n",
      "824 - loss: 4417.145\n",
      "825 - loss: 4417.145\n",
      "826 - loss: 4417.145\n",
      "827 - loss: 4417.140\n",
      "828 - loss: 4417.134\n",
      "829 - loss: 4417.132\n",
      "830 - loss: 4417.130\n",
      "831 - loss: 4417.129\n",
      "832 - loss: 4417.128\n",
      "833 - loss: 4417.128\n",
      "834 - loss: 4417.128\n",
      "835 - loss: 4417.107\n",
      "836 - loss: 4417.087\n",
      "837 - loss: 4417.085\n",
      "838 - loss: 4417.085\n",
      "839 - loss: 4417.085\n",
      "840 - loss: 4417.084\n",
      "841 - loss: 4417.083\n",
      "842 - loss: 4417.081\n",
      "843 - loss: 4417.059\n",
      "844 - loss: 4416.973\n",
      "845 - loss: 4416.972\n",
      "846 - loss: 4416.972\n",
      "847 - loss: 4416.972\n",
      "848 - loss: 4416.971\n",
      "849 - loss: 4416.969\n",
      "850 - loss: 4416.967\n",
      "851 - loss: 4416.958\n",
      "852 - loss: 4416.943\n",
      "853 - loss: 4416.941\n",
      "854 - loss: 4416.941\n",
      "855 - loss: 4416.941\n",
      "856 - loss: 4416.941\n",
      "857 - loss: 4416.908\n",
      "858 - loss: 4416.869\n",
      "859 - loss: 4416.867\n",
      "860 - loss: 4416.866\n",
      "861 - loss: 4416.865\n",
      "862 - loss: 4416.865\n",
      "863 - loss: 4416.865\n",
      "864 - loss: 4416.724\n",
      "865 - loss: 4416.566\n",
      "866 - loss: 4416.555\n",
      "f_new: 4416.557 - f: 4416.555 - Backtracking...\n",
      "868 - loss: 4416.554\n",
      "869 - loss: 4416.554\n",
      "870 - loss: 4416.554\n",
      "871 - loss: 4416.552\n",
      "872 - loss: 4416.550\n",
      "873 - loss: 4416.540\n",
      "874 - loss: 4416.536\n",
      "875 - loss: 4416.536\n",
      "876 - loss: 4416.536\n",
      "877 - loss: 4416.536\n",
      "878 - loss: 4415.496\n",
      "879 - loss: 4415.033\n",
      "f_new: 4420.744 - f: 4415.033 - Backtracking...\n",
      "881 - loss: 4414.972\n",
      "882 - loss: 4414.933\n",
      "883 - loss: 4414.909\n",
      "884 - loss: 4414.745\n",
      "885 - loss: 4414.611\n",
      "f_new: 4414.659 - f: 4414.611 - Backtracking...\n",
      "887 - loss: 4414.592\n",
      "888 - loss: 4414.574\n",
      "889 - loss: 4414.569\n",
      "890 - loss: 4414.561\n",
      "891 - loss: 4414.383\n",
      "f_new: 4414.387 - f: 4414.383 - Backtracking...\n",
      "893 - loss: 4414.379\n",
      "f_new: 4414.385 - f: 4414.379 - Backtracking...\n",
      "895 - loss: 4414.379\n",
      "896 - loss: 4414.378\n",
      "897 - loss: 4414.378\n",
      "898 - loss: 4414.377\n",
      "899 - loss: 4414.375\n",
      "900 - loss: 4414.373\n",
      "901 - loss: 4414.373\n",
      "902 - loss: 4414.373\n",
      "903 - loss: 4414.373\n",
      "904 - loss: 4414.359\n",
      "905 - loss: 4414.345\n",
      "906 - loss: 4414.328\n",
      "907 - loss: 4414.327\n",
      "908 - loss: 4414.327\n",
      "909 - loss: 4414.327\n",
      "910 - loss: 4414.326\n",
      "911 - loss: 4414.324\n",
      "912 - loss: 4414.301\n",
      "913 - loss: 4414.275\n",
      "914 - loss: 4414.275\n",
      "915 - loss: 4414.275\n",
      "916 - loss: 4414.275\n",
      "917 - loss: 4414.268\n",
      "918 - loss: 4414.262\n",
      "919 - loss: 4414.260\n",
      "920 - loss: 4414.258\n",
      "921 - loss: 4414.256\n",
      "922 - loss: 4414.256\n",
      "923 - loss: 4414.256\n",
      "924 - loss: 4414.256\n",
      "925 - loss: 4414.037\n",
      "926 - loss: 4413.702\n",
      "f_new: 4413.762 - f: 4413.702 - Backtracking...\n",
      "928 - loss: 4413.701\n",
      "929 - loss: 4413.701\n",
      "930 - loss: 4413.701\n",
      "931 - loss: 4413.699\n",
      "932 - loss: 4413.696\n",
      "933 - loss: 4413.692\n",
      "934 - loss: 4413.692\n",
      "935 - loss: 4413.692\n",
      "936 - loss: 4413.692\n",
      "937 - loss: 4413.686\n",
      "938 - loss: 4413.679\n",
      "939 - loss: 4413.677\n",
      "940 - loss: 4413.676\n",
      "941 - loss: 4413.676\n",
      "942 - loss: 4413.675\n",
      "943 - loss: 4413.675\n",
      "944 - loss: 4413.657\n",
      "945 - loss: 4413.638\n",
      "946 - loss: 4413.633\n",
      "947 - loss: 4413.631\n",
      "948 - loss: 4413.631\n",
      "949 - loss: 4413.631\n",
      "950 - loss: 4413.631\n",
      "951 - loss: 4413.627\n",
      "952 - loss: 4413.622\n",
      "953 - loss: 4413.620\n",
      "954 - loss: 4413.618\n",
      "955 - loss: 4413.616\n",
      "956 - loss: 4413.615\n",
      "957 - loss: 4413.615\n",
      "958 - loss: 4413.615\n",
      "959 - loss: 4413.558\n",
      "960 - loss: 4413.493\n",
      "961 - loss: 4413.491\n",
      "962 - loss: 4413.491\n",
      "963 - loss: 4413.490\n",
      "964 - loss: 4413.490\n",
      "965 - loss: 4413.489\n",
      "966 - loss: 4413.487\n",
      "967 - loss: 4413.483\n",
      "968 - loss: 4413.475\n",
      "969 - loss: 4413.472\n",
      "970 - loss: 4413.472\n",
      "971 - loss: 4413.472\n",
      "972 - loss: 4413.472\n",
      "973 - loss: 4413.470\n",
      "974 - loss: 4413.468\n",
      "975 - loss: 4413.460\n",
      "976 - loss: 4413.416\n",
      "977 - loss: 4413.414\n",
      "978 - loss: 4413.414\n",
      "979 - loss: 4413.414\n",
      "980 - loss: 4413.414\n",
      "981 - loss: 4413.412\n",
      "982 - loss: 4413.410\n",
      "983 - loss: 4413.399\n",
      "984 - loss: 4413.258\n",
      "985 - loss: 4413.252\n",
      "f_new: 4413.252 - f: 4413.252 - Backtracking...\n",
      "987 - loss: 4413.252\n",
      "988 - loss: 4413.252\n",
      "989 - loss: 4413.251\n",
      "990 - loss: 4413.250\n",
      "991 - loss: 4413.249\n",
      "992 - loss: 4413.247\n",
      "993 - loss: 4413.247\n",
      "994 - loss: 4413.247\n",
      "995 - loss: 4413.247\n",
      "996 - loss: 4389.214\n",
      "f_new: 22935.718 - f: 4389.214 - Backtracking...\n",
      "f_new: 4389.214 - f: 4389.214 - Backtracking...\n",
      "999 - loss: 4389.213\n",
      "1000 - loss: 4389.213\n",
      "Reached maximum number of function evaluations 1000\n",
      "0.7283417326613871\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(verbose=True, max_evaluations=1000)\n",
    "logReg.fit(y_sub, np.c_[np.ones(X_sub.shape[0]), X_sub])\n",
    "pred = logReg.predict(np.c_[np.ones(X_sub.shape[0]), X_sub])\n",
    "print(compute_accuracy(pred, y_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_new: 388952141074152048493827070704946719003374518272.000 - f: 0.693 - Backtracking...\n",
      "f_new: 42948940344232256197308449618584789201601953792.000 - f: 0.693 - Backtracking...\n",
      "f_new: 4742515291465520316589241094131375336488697856.000 - f: 0.693 - Backtracking...\n",
      "f_new: 523678840723825601365942021904012656230006784.000 - f: 0.693 - Backtracking...\n",
      "f_new: 57825755188467729788181246775209813142929408.000 - f: 0.693 - Backtracking...\n",
      "f_new: 6385245503703762415557669590722242188148736.000 - f: 0.693 - Backtracking...\n",
      "f_new: 705072679287725974358492902368409655705600.000 - f: 0.693 - Backtracking...\n",
      "f_new: 77855656887368529251737837091182763573248.000 - f: 0.693 - Backtracking...\n",
      "f_new: 8596990760565376003751822352078876966912.000 - f: 0.693 - Backtracking...\n",
      "f_new: 949298395159228346089345013828158488576.000 - f: 0.693 - Backtracking...\n",
      "f_new: 104823590969245276918563331445266317312.000 - f: 0.693 - Backtracking...\n",
      "f_new: 11574848624751544723633017999918628864.000 - f: 0.693 - Backtracking...\n",
      "f_new: 1278119929369916136911791130408910848.000 - f: 0.693 - Backtracking...\n",
      "f_new: 141132779080955290663632729679069184.000 - f: 0.693 - Backtracking...\n",
      "f_new: 15584188051064248468229727084085248.000 - f: 0.693 - Backtracking...\n",
      "f_new: 1720839898374159823323440951066624.000 - f: 0.693 - Backtracking...\n",
      "f_new: 190018879785922593601210233651200.000 - f: 0.693 - Backtracking...\n",
      "f_new: 20982297486948518276197993414656.000 - f: 0.693 - Backtracking...\n",
      "f_new: 2316910868682124542541257768960.000 - f: 0.693 - Backtracking...\n",
      "f_new: 255838331181627255559634812928.000 - f: 0.693 - Backtracking...\n",
      "f_new: 28250224290686777644342575104.000 - f: 0.693 - Backtracking...\n",
      "f_new: 3119451134582065730238283776.000 - f: 0.693 - Backtracking...\n",
      "f_new: 344456570713080468250558464.000 - f: 0.693 - Backtracking...\n",
      "f_new: 38035642806538756811653120.000 - f: 0.693 - Backtracking...\n",
      "f_new: 4199978304120254887886848.000 - f: 0.693 - Backtracking...\n",
      "f_new: 463770728019571584794624.000 - f: 0.693 - Backtracking...\n",
      "f_new: 51210571244333019693056.000 - f: 0.693 - Backtracking...\n",
      "f_new: 5654782522324772978688.000 - f: 0.693 - Backtracking...\n",
      "f_new: 624413370087690076160.000 - f: 0.693 - Backtracking...\n",
      "f_new: 68949080748020621312.000 - f: 0.693 - Backtracking...\n",
      "f_new: 7613507275363817472.000 - f: 0.693 - Backtracking...\n",
      "f_new: 840700012286702080.000 - f: 0.693 - Backtracking...\n",
      "f_new: 92831921622493920.000 - f: 0.693 - Backtracking...\n",
      "f_new: 10250702445792234.000 - f: 0.693 - Backtracking...\n",
      "f_new: 1131904831825757.250 - f: 0.693 - Backtracking...\n",
      "f_new: 124987390384784.203 - f: 0.693 - Backtracking...\n",
      "f_new: 13801379158352.516 - f: 0.693 - Backtracking...\n",
      "f_new: 1523978267617.324 - f: 0.693 - Backtracking...\n",
      "f_new: 168280990872.210 - f: 0.693 - Backtracking...\n",
      "f_new: 18581952571.600 - f: 0.693 - Backtracking...\n",
      "f_new: 2051859568.882 - f: 0.693 - Backtracking...\n",
      "f_new: 226570790.942 - f: 0.693 - Backtracking...\n",
      "f_new: 25018438.979 - f: 0.693 - Backtracking...\n",
      "f_new: 2762590.440 - f: 0.693 - Backtracking...\n",
      "f_new: 305051.297 - f: 0.693 - Backtracking...\n",
      "f_new: 33684.487 - f: 0.693 - Backtracking...\n",
      "f_new: 3719.574 - f: 0.693 - Backtracking...\n",
      "f_new: 410.801 - f: 0.693 - Backtracking...\n",
      "f_new: 45.573 - f: 0.693 - Backtracking...\n",
      "f_new: 5.430 - f: 0.693 - Backtracking...\n",
      "f_new: 1.140 - f: 0.693 - Backtracking...\n",
      "f_new: 0.727 - f: 0.693 - Backtracking...\n",
      "f_new: 0.695 - f: 0.693 - Backtracking...\n",
      "54 - loss: 0.693\n",
      "55 - loss: 0.693\n",
      "56 - loss: 0.693\n",
      "57 - loss: 0.693\n",
      "58 - loss: 0.693\n",
      "59 - loss: 0.692\n",
      "f_new: 0.692 - f: 0.692 - Backtracking...\n",
      "61 - loss: 0.692\n",
      "62 - loss: 0.692\n",
      "63 - loss: 0.692\n",
      "64 - loss: 0.691\n",
      "65 - loss: 0.691\n",
      "f_new: 0.693 - f: 0.691 - Backtracking...\n",
      "67 - loss: 0.691\n",
      "68 - loss: 0.690\n",
      "69 - loss: 0.690\n",
      "70 - loss: 0.690\n",
      "f_new: 0.691 - f: 0.690 - Backtracking...\n",
      "72 - loss: 0.689\n",
      "73 - loss: 0.689\n",
      "74 - loss: 0.689\n",
      "75 - loss: 0.689\n",
      "76 - loss: 0.689\n",
      "77 - loss: 0.686\n",
      "f_new: 0.706 - f: 0.686 - Backtracking...\n",
      "79 - loss: 0.686\n",
      "80 - loss: 0.686\n",
      "81 - loss: 0.686\n",
      "82 - loss: 0.686\n",
      "83 - loss: 0.685\n",
      "f_new: 0.698 - f: 0.685 - Backtracking...\n",
      "85 - loss: 0.685\n",
      "86 - loss: 0.685\n",
      "87 - loss: 0.685\n",
      "88 - loss: 0.685\n",
      "89 - loss: 0.684\n",
      "f_new: 0.687 - f: 0.684 - Backtracking...\n",
      "91 - loss: 0.684\n",
      "92 - loss: 0.684\n",
      "93 - loss: 0.684\n",
      "94 - loss: 0.684\n",
      "95 - loss: 0.684\n",
      "96 - loss: 0.683\n",
      "f_new: 0.686 - f: 0.683 - Backtracking...\n",
      "98 - loss: 0.683\n",
      "99 - loss: 0.683\n",
      "100 - loss: 0.683\n",
      "101 - loss: 0.683\n",
      "102 - loss: 0.683\n",
      "103 - loss: 0.683\n",
      "104 - loss: 0.681\n",
      "f_new: 0.683 - f: 0.681 - Backtracking...\n",
      "106 - loss: 0.681\n",
      "107 - loss: 0.681\n",
      "108 - loss: 0.681\n",
      "109 - loss: 0.681\n",
      "110 - loss: 0.679\n",
      "f_new: 0.748 - f: 0.679 - Backtracking...\n",
      "112 - loss: 0.679\n",
      "113 - loss: 0.679\n",
      "114 - loss: 0.679\n",
      "115 - loss: 0.679\n",
      "116 - loss: 0.679\n",
      "f_new: 0.679 - f: 0.679 - Backtracking...\n",
      "118 - loss: 0.679\n",
      "119 - loss: 0.679\n",
      "120 - loss: 0.679\n",
      "121 - loss: 0.679\n",
      "122 - loss: 0.679\n",
      "123 - loss: 0.678\n",
      "f_new: 0.684 - f: 0.678 - Backtracking...\n",
      "125 - loss: 0.678\n",
      "126 - loss: 0.678\n",
      "127 - loss: 0.678\n",
      "128 - loss: 0.678\n",
      "129 - loss: 0.678\n",
      "130 - loss: 0.678\n",
      "f_new: 0.678 - f: 0.678 - Backtracking...\n",
      "132 - loss: 0.678\n",
      "133 - loss: 0.678\n",
      "134 - loss: 0.678\n",
      "135 - loss: 0.678\n",
      "136 - loss: 0.678\n",
      "137 - loss: 0.677\n",
      "f_new: 0.678 - f: 0.677 - Backtracking...\n",
      "139 - loss: 0.677\n",
      "140 - loss: 0.677\n",
      "141 - loss: 0.677\n",
      "142 - loss: 0.677\n",
      "143 - loss: 0.677\n",
      "f_new: 0.677 - f: 0.677 - Backtracking...\n",
      "145 - loss: 0.677\n",
      "146 - loss: 0.677\n",
      "147 - loss: 0.677\n",
      "148 - loss: 0.677\n",
      "149 - loss: 0.677\n",
      "150 - loss: 0.677\n",
      "f_new: 0.677 - f: 0.677 - Backtracking...\n",
      "152 - loss: 0.677\n",
      "153 - loss: 0.676\n",
      "154 - loss: 0.676\n",
      "155 - loss: 0.676\n",
      "156 - loss: 0.676\n",
      "157 - loss: 0.676\n",
      "158 - loss: 0.676\n",
      "159 - loss: 0.676\n",
      "160 - loss: 0.676\n",
      "161 - loss: 0.676\n",
      "162 - loss: 0.676\n",
      "163 - loss: 0.675\n",
      "f_new: 0.683 - f: 0.675 - Backtracking...\n",
      "165 - loss: 0.675\n",
      "166 - loss: 0.675\n",
      "167 - loss: 0.675\n",
      "168 - loss: 0.675\n",
      "169 - loss: 0.675\n",
      "170 - loss: 0.666\n",
      "f_new: 0.886 - f: 0.666 - Backtracking...\n",
      "f_new: 0.666 - f: 0.666 - Backtracking...\n",
      "173 - loss: 0.666\n",
      "174 - loss: 0.666\n",
      "175 - loss: 0.666\n",
      "176 - loss: 0.666\n",
      "177 - loss: 0.666\n",
      "f_new: 0.667 - f: 0.666 - Backtracking...\n",
      "179 - loss: 0.666\n",
      "180 - loss: 0.666\n",
      "181 - loss: 0.666\n",
      "182 - loss: 0.666\n",
      "f_new: 0.666 - f: 0.666 - Backtracking...\n",
      "184 - loss: 0.666\n",
      "185 - loss: 0.666\n",
      "186 - loss: 0.666\n",
      "187 - loss: 0.666\n",
      "188 - loss: 0.666\n",
      "189 - loss: 0.666\n",
      "190 - loss: 0.666\n",
      "191 - loss: 0.666\n",
      "192 - loss: 0.666\n",
      "193 - loss: 0.666\n",
      "194 - loss: 0.665\n",
      "195 - loss: 0.665\n",
      "196 - loss: 0.665\n",
      "197 - loss: 0.665\n",
      "198 - loss: 0.665\n",
      "199 - loss: 0.665\n",
      "200 - loss: 0.664\n",
      "f_new: 0.721 - f: 0.664 - Backtracking...\n",
      "202 - loss: 0.664\n",
      "203 - loss: 0.664\n",
      "204 - loss: 0.664\n",
      "205 - loss: 0.664\n",
      "206 - loss: 0.664\n",
      "207 - loss: 0.664\n",
      "208 - loss: 0.664\n",
      "209 - loss: 0.664\n",
      "210 - loss: 0.664\n",
      "211 - loss: 0.664\n",
      "212 - loss: 0.664\n",
      "213 - loss: 0.664\n",
      "214 - loss: 0.664\n",
      "215 - loss: 0.664\n",
      "216 - loss: 0.664\n",
      "217 - loss: 0.664\n",
      "218 - loss: 0.663\n",
      "f_new: 0.682 - f: 0.663 - Backtracking...\n",
      "220 - loss: 0.663\n",
      "221 - loss: 0.663\n",
      "222 - loss: 0.663\n",
      "223 - loss: 0.663\n",
      "224 - loss: 0.663\n",
      "f_new: 0.663 - f: 0.663 - Backtracking...\n",
      "226 - loss: 0.663\n",
      "227 - loss: 0.663\n",
      "228 - loss: 0.663\n",
      "229 - loss: 0.663\n",
      "230 - loss: 0.662\n",
      "f_new: 0.663 - f: 0.662 - Backtracking...\n",
      "232 - loss: 0.662\n",
      "233 - loss: 0.662\n",
      "234 - loss: 0.662\n",
      "235 - loss: 0.662\n",
      "236 - loss: 0.662\n",
      "237 - loss: 0.662\n",
      "f_new: 0.663 - f: 0.662 - Backtracking...\n",
      "239 - loss: 0.662\n",
      "240 - loss: 0.662\n",
      "241 - loss: 0.662\n",
      "242 - loss: 0.662\n",
      "243 - loss: 0.662\n",
      "244 - loss: 0.662\n",
      "245 - loss: 0.662\n",
      "246 - loss: 0.662\n",
      "247 - loss: 0.662\n",
      "248 - loss: 0.662\n",
      "249 - loss: 0.662\n",
      "250 - loss: 0.662\n",
      "251 - loss: 0.662\n",
      "252 - loss: 0.662\n",
      "253 - loss: 0.662\n",
      "254 - loss: 0.662\n",
      "255 - loss: 0.662\n",
      "256 - loss: 0.662\n",
      "f_new: 0.662 - f: 0.662 - Backtracking...\n",
      "258 - loss: 0.662\n",
      "259 - loss: 0.662\n",
      "260 - loss: 0.662\n",
      "261 - loss: 0.662\n",
      "262 - loss: 0.662\n",
      "263 - loss: 0.662\n",
      "264 - loss: 0.662\n",
      "265 - loss: 0.662\n",
      "266 - loss: 0.662\n",
      "267 - loss: 0.662\n",
      "268 - loss: 0.661\n",
      "f_new: 0.674 - f: 0.661 - Backtracking...\n",
      "270 - loss: 0.661\n",
      "271 - loss: 0.661\n",
      "272 - loss: 0.661\n",
      "273 - loss: 0.661\n",
      "274 - loss: 0.661\n",
      "275 - loss: 0.661\n",
      "276 - loss: 0.661\n",
      "277 - loss: 0.661\n",
      "278 - loss: 0.661\n",
      "279 - loss: 0.661\n",
      "280 - loss: 0.661\n",
      "281 - loss: 0.661\n",
      "282 - loss: 0.661\n",
      "f_new: 0.661 - f: 0.661 - Backtracking...\n",
      "284 - loss: 0.661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 - loss: 0.661\n",
      "286 - loss: 0.661\n",
      "287 - loss: 0.661\n",
      "288 - loss: 0.661\n",
      "f_new: 0.661 - f: 0.661 - Backtracking...\n",
      "290 - loss: 0.661\n",
      "291 - loss: 0.661\n",
      "292 - loss: 0.661\n",
      "293 - loss: 0.661\n",
      "294 - loss: 0.661\n",
      "f_new: 0.661 - f: 0.661 - Backtracking...\n",
      "296 - loss: 0.661\n",
      "297 - loss: 0.661\n",
      "298 - loss: 0.661\n",
      "299 - loss: 0.661\n",
      "300 - loss: 0.660\n",
      "301 - loss: 0.660\n",
      "302 - loss: 0.660\n",
      "303 - loss: 0.660\n",
      "304 - loss: 0.660\n",
      "305 - loss: 0.660\n",
      "306 - loss: 0.660\n",
      "307 - loss: 0.660\n",
      "308 - loss: 0.660\n",
      "309 - loss: 0.660\n",
      "310 - loss: 0.660\n",
      "311 - loss: 0.660\n",
      "312 - loss: 0.660\n",
      "313 - loss: 0.660\n",
      "314 - loss: 0.660\n",
      "315 - loss: 0.660\n",
      "316 - loss: 0.660\n",
      "317 - loss: 0.660\n",
      "318 - loss: 0.660\n",
      "319 - loss: 0.660\n",
      "320 - loss: 0.660\n",
      "321 - loss: 0.660\n",
      "322 - loss: 0.660\n",
      "323 - loss: 0.660\n",
      "324 - loss: 0.660\n",
      "f_new: 0.660 - f: 0.660 - Backtracking...\n",
      "326 - loss: 0.660\n",
      "327 - loss: 0.660\n",
      "328 - loss: 0.660\n",
      "329 - loss: 0.660\n",
      "330 - loss: 0.660\n",
      "331 - loss: 0.660\n",
      "332 - loss: 0.660\n",
      "333 - loss: 0.660\n",
      "334 - loss: 0.660\n",
      "335 - loss: 0.660\n",
      "336 - loss: 0.660\n",
      "f_new: 0.661 - f: 0.660 - Backtracking...\n",
      "338 - loss: 0.660\n",
      "339 - loss: 0.660\n",
      "340 - loss: 0.660\n",
      "341 - loss: 0.660\n",
      "342 - loss: 0.660\n",
      "343 - loss: 0.660\n",
      "344 - loss: 0.660\n",
      "f_new: 0.660 - f: 0.660 - Backtracking...\n",
      "346 - loss: 0.660\n",
      "347 - loss: 0.660\n",
      "348 - loss: 0.660\n",
      "349 - loss: 0.660\n",
      "350 - loss: 0.660\n",
      "351 - loss: 0.660\n",
      "352 - loss: 0.660\n",
      "353 - loss: 0.660\n",
      "354 - loss: 0.660\n",
      "355 - loss: 0.660\n",
      "356 - loss: 0.658\n",
      "f_new: 0.806 - f: 0.658 - Backtracking...\n",
      "f_new: 0.658 - f: 0.658 - Backtracking...\n",
      "359 - loss: 0.658\n",
      "360 - loss: 0.658\n",
      "361 - loss: 0.658\n",
      "362 - loss: 0.658\n",
      "363 - loss: 0.658\n",
      "f_new: 0.658 - f: 0.658 - Backtracking...\n",
      "365 - loss: 0.658\n",
      "366 - loss: 0.658\n",
      "367 - loss: 0.658\n",
      "368 - loss: 0.658\n",
      "369 - loss: 0.658\n",
      "370 - loss: 0.658\n",
      "371 - loss: 0.658\n",
      "372 - loss: 0.658\n",
      "373 - loss: 0.658\n",
      "374 - loss: 0.658\n",
      "375 - loss: 0.658\n",
      "376 - loss: 0.658\n",
      "377 - loss: 0.658\n",
      "378 - loss: 0.658\n",
      "379 - loss: 0.658\n",
      "380 - loss: 0.658\n",
      "381 - loss: 0.658\n",
      "382 - loss: 0.656\n",
      "f_new: 1.061 - f: 0.656 - Backtracking...\n",
      "f_new: 0.656 - f: 0.656 - Backtracking...\n",
      "385 - loss: 0.656\n",
      "386 - loss: 0.656\n",
      "387 - loss: 0.656\n",
      "388 - loss: 0.656\n",
      "389 - loss: 0.656\n",
      "390 - loss: 0.656\n",
      "391 - loss: 0.656\n",
      "392 - loss: 0.656\n",
      "393 - loss: 0.656\n",
      "394 - loss: 0.656\n",
      "395 - loss: 0.656\n",
      "396 - loss: 0.656\n",
      "397 - loss: 0.656\n",
      "398 - loss: 0.656\n",
      "399 - loss: 0.656\n",
      "400 - loss: 0.656\n",
      "401 - loss: 0.656\n",
      "402 - loss: 0.656\n",
      "403 - loss: 0.656\n",
      "404 - loss: 0.656\n",
      "405 - loss: 0.656\n",
      "406 - loss: 0.656\n",
      "f_new: 0.662 - f: 0.656 - Backtracking...\n",
      "408 - loss: 0.656\n",
      "409 - loss: 0.656\n",
      "410 - loss: 0.656\n",
      "411 - loss: 0.656\n",
      "412 - loss: 0.656\n",
      "413 - loss: 0.656\n",
      "f_new: 0.656 - f: 0.656 - Backtracking...\n",
      "415 - loss: 0.656\n",
      "416 - loss: 0.656\n",
      "417 - loss: 0.656\n",
      "418 - loss: 0.656\n",
      "419 - loss: 0.656\n",
      "420 - loss: 0.656\n",
      "421 - loss: 0.656\n",
      "422 - loss: 0.656\n",
      "423 - loss: 0.656\n",
      "424 - loss: 0.656\n",
      "425 - loss: 0.656\n",
      "426 - loss: 0.656\n",
      "427 - loss: 0.656\n",
      "428 - loss: 0.656\n",
      "429 - loss: 0.656\n",
      "430 - loss: 0.656\n",
      "431 - loss: 0.654\n",
      "f_new: 1.052 - f: 0.654 - Backtracking...\n",
      "f_new: 0.654 - f: 0.654 - Backtracking...\n",
      "434 - loss: 0.654\n",
      "435 - loss: 0.654\n",
      "436 - loss: 0.654\n",
      "437 - loss: 0.654\n",
      "438 - loss: 0.654\n",
      "439 - loss: 0.654\n",
      "440 - loss: 0.654\n",
      "441 - loss: 0.654\n",
      "442 - loss: 0.654\n",
      "443 - loss: 0.654\n",
      "444 - loss: 0.654\n",
      "445 - loss: 0.654\n",
      "446 - loss: 0.654\n",
      "447 - loss: 0.654\n",
      "448 - loss: 0.654\n",
      "449 - loss: 0.654\n",
      "450 - loss: 0.654\n",
      "451 - loss: 0.654\n",
      "452 - loss: 0.654\n",
      "453 - loss: 0.654\n",
      "454 - loss: 0.654\n",
      "455 - loss: 0.654\n",
      "456 - loss: 0.654\n",
      "f_new: 0.654 - f: 0.654 - Backtracking...\n",
      "458 - loss: 0.654\n",
      "459 - loss: 0.654\n",
      "460 - loss: 0.654\n",
      "461 - loss: 0.654\n",
      "462 - loss: 0.654\n",
      "463 - loss: 0.654\n",
      "f_new: 0.654 - f: 0.654 - Backtracking...\n",
      "465 - loss: 0.654\n",
      "466 - loss: 0.654\n",
      "467 - loss: 0.654\n",
      "468 - loss: 0.654\n",
      "469 - loss: 0.653\n",
      "470 - loss: 0.653\n",
      "471 - loss: 0.653\n",
      "472 - loss: 0.653\n",
      "473 - loss: 0.653\n",
      "474 - loss: 0.653\n",
      "475 - loss: 0.653\n",
      "476 - loss: 0.653\n",
      "477 - loss: 0.653\n",
      "478 - loss: 0.653\n",
      "479 - loss: 0.653\n",
      "480 - loss: 0.653\n",
      "481 - loss: 0.653\n",
      "482 - loss: 0.653\n",
      "483 - loss: 0.653\n",
      "484 - loss: 0.653\n",
      "485 - loss: 0.653\n",
      "486 - loss: 0.653\n",
      "487 - loss: 0.653\n",
      "488 - loss: 0.653\n",
      "489 - loss: 0.653\n",
      "490 - loss: 0.653\n",
      "491 - loss: 0.653\n",
      "492 - loss: 0.653\n",
      "493 - loss: 0.653\n",
      "494 - loss: 0.653\n",
      "495 - loss: 0.653\n",
      "496 - loss: 0.653\n",
      "497 - loss: 0.653\n",
      "498 - loss: 0.653\n",
      "499 - loss: 0.653\n",
      "500 - loss: 0.653\n",
      "501 - loss: 0.653\n",
      "502 - loss: 0.653\n",
      "503 - loss: 0.653\n",
      "504 - loss: 0.653\n",
      "505 - loss: 0.653\n",
      "506 - loss: 0.653\n",
      "507 - loss: 0.653\n",
      "508 - loss: 0.653\n",
      "509 - loss: 0.653\n",
      "510 - loss: 0.653\n",
      "511 - loss: 0.653\n",
      "512 - loss: 0.653\n",
      "513 - loss: 0.653\n",
      "514 - loss: 0.653\n",
      "515 - loss: 0.653\n",
      "516 - loss: 0.653\n",
      "517 - loss: 0.653\n",
      "518 - loss: 0.653\n",
      "519 - loss: 0.653\n",
      "520 - loss: 0.653\n",
      "521 - loss: 0.653\n",
      "522 - loss: 0.653\n",
      "523 - loss: 0.653\n",
      "f_new: 0.653 - f: 0.653 - Backtracking...\n",
      "525 - loss: 0.653\n",
      "526 - loss: 0.653\n",
      "527 - loss: 0.653\n",
      "528 - loss: 0.653\n",
      "529 - loss: 0.653\n",
      "530 - loss: 0.653\n",
      "531 - loss: 0.653\n",
      "532 - loss: 0.653\n",
      "533 - loss: 0.653\n",
      "534 - loss: 0.653\n",
      "535 - loss: 0.653\n",
      "536 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "538 - loss: 0.652\n",
      "539 - loss: 0.652\n",
      "540 - loss: 0.652\n",
      "541 - loss: 0.652\n",
      "542 - loss: 0.652\n",
      "543 - loss: 0.652\n",
      "544 - loss: 0.652\n",
      "545 - loss: 0.652\n",
      "546 - loss: 0.652\n",
      "547 - loss: 0.652\n",
      "548 - loss: 0.652\n",
      "549 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "551 - loss: 0.652\n",
      "552 - loss: 0.652\n",
      "553 - loss: 0.652\n",
      "554 - loss: 0.652\n",
      "555 - loss: 0.652\n",
      "556 - loss: 0.652\n",
      "557 - loss: 0.652\n",
      "558 - loss: 0.652\n",
      "559 - loss: 0.652\n",
      "560 - loss: 0.652\n",
      "561 - loss: 0.652\n",
      "562 - loss: 0.652\n",
      "563 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "565 - loss: 0.652\n",
      "566 - loss: 0.652\n",
      "567 - loss: 0.652\n",
      "568 - loss: 0.652\n",
      "569 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "571 - loss: 0.652\n",
      "572 - loss: 0.652\n",
      "573 - loss: 0.652\n",
      "574 - loss: 0.652\n",
      "575 - loss: 0.652\n",
      "576 - loss: 0.652\n",
      "577 - loss: 0.652\n",
      "578 - loss: 0.652\n",
      "579 - loss: 0.652\n",
      "580 - loss: 0.652\n",
      "581 - loss: 0.652\n",
      "582 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "584 - loss: 0.652\n",
      "585 - loss: 0.652\n",
      "586 - loss: 0.652\n",
      "587 - loss: 0.652\n",
      "588 - loss: 0.652\n",
      "589 - loss: 0.652\n",
      "590 - loss: 0.652\n",
      "591 - loss: 0.652\n",
      "592 - loss: 0.652\n",
      "593 - loss: 0.652\n",
      "594 - loss: 0.651\n",
      "f_new: 0.672 - f: 0.651 - Backtracking...\n",
      "596 - loss: 0.651\n",
      "597 - loss: 0.651\n",
      "598 - loss: 0.651\n",
      "599 - loss: 0.651\n",
      "600 - loss: 0.651\n",
      "601 - loss: 0.651\n",
      "f_new: 0.651 - f: 0.651 - Backtracking...\n",
      "603 - loss: 0.651\n",
      "604 - loss: 0.651\n",
      "605 - loss: 0.651\n",
      "606 - loss: 0.651\n",
      "607 - loss: 0.651\n",
      "608 - loss: 0.651\n",
      "f_new: 0.651 - f: 0.651 - Backtracking...\n",
      "610 - loss: 0.651\n",
      "611 - loss: 0.651\n",
      "612 - loss: 0.651\n",
      "613 - loss: 0.651\n",
      "614 - loss: 0.651\n",
      "f_new: 0.651 - f: 0.651 - Backtracking...\n",
      "616 - loss: 0.651\n",
      "617 - loss: 0.651\n",
      "618 - loss: 0.651\n",
      "619 - loss: 0.651\n",
      "620 - loss: 0.651\n",
      "621 - loss: 0.651\n",
      "622 - loss: 0.651\n",
      "623 - loss: 0.651\n",
      "624 - loss: 0.651\n",
      "625 - loss: 0.651\n",
      "626 - loss: 0.651\n",
      "627 - loss: 0.651\n",
      "628 - loss: 0.651\n",
      "629 - loss: 0.651\n",
      "630 - loss: 0.651\n",
      "631 - loss: 0.651\n",
      "632 - loss: 0.651\n",
      "633 - loss: 0.651\n",
      "634 - loss: 0.651\n",
      "635 - loss: 0.651\n",
      "636 - loss: 0.651\n",
      "637 - loss: 0.650\n",
      "f_new: 0.738 - f: 0.650 - Backtracking...\n",
      "f_new: 0.650 - f: 0.650 - Backtracking...\n",
      "640 - loss: 0.650\n",
      "641 - loss: 0.650\n",
      "642 - loss: 0.650\n",
      "643 - loss: 0.650\n",
      "644 - loss: 0.650\n",
      "645 - loss: 0.650\n",
      "646 - loss: 0.650\n",
      "647 - loss: 0.650\n",
      "648 - loss: 0.650\n",
      "649 - loss: 0.650\n",
      "650 - loss: 0.650\n",
      "f_new: 0.650 - f: 0.650 - Backtracking...\n",
      "652 - loss: 0.650\n",
      "653 - loss: 0.650\n",
      "654 - loss: 0.650\n",
      "655 - loss: 0.650\n",
      "656 - loss: 0.650\n",
      "657 - loss: 0.650\n",
      "658 - loss: 0.650\n",
      "f_new: 0.650 - f: 0.650 - Backtracking...\n",
      "660 - loss: 0.650\n",
      "661 - loss: 0.650\n",
      "662 - loss: 0.650\n",
      "663 - loss: 0.650\n",
      "664 - loss: 0.650\n",
      "665 - loss: 0.650\n",
      "666 - loss: 0.650\n",
      "667 - loss: 0.650\n",
      "668 - loss: 0.650\n",
      "669 - loss: 0.650\n",
      "670 - loss: 0.649\n",
      "f_new: 0.651 - f: 0.649 - Backtracking...\n",
      "672 - loss: 0.649\n",
      "673 - loss: 0.649\n",
      "674 - loss: 0.649\n",
      "675 - loss: 0.649\n",
      "676 - loss: 0.649\n",
      "677 - loss: 0.649\n",
      "f_new: 0.649 - f: 0.649 - Backtracking...\n",
      "679 - loss: 0.649\n",
      "680 - loss: 0.649\n",
      "681 - loss: 0.649\n",
      "682 - loss: 0.649\n",
      "683 - loss: 0.649\n",
      "684 - loss: 0.649\n",
      "685 - loss: 0.649\n",
      "686 - loss: 0.649\n",
      "687 - loss: 0.649\n",
      "688 - loss: 0.649\n",
      "689 - loss: 0.649\n",
      "690 - loss: 0.649\n",
      "691 - loss: 0.649\n",
      "692 - loss: 0.649\n",
      "693 - loss: 0.649\n",
      "694 - loss: 0.649\n",
      "695 - loss: 0.649\n",
      "f_new: 0.656 - f: 0.649 - Backtracking...\n",
      "697 - loss: 0.649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698 - loss: 0.649\n",
      "699 - loss: 0.649\n",
      "700 - loss: 0.649\n",
      "701 - loss: 0.649\n",
      "f_new: 0.649 - f: 0.649 - Backtracking...\n",
      "703 - loss: 0.649\n",
      "704 - loss: 0.649\n",
      "705 - loss: 0.649\n",
      "706 - loss: 0.649\n",
      "707 - loss: 0.649\n",
      "708 - loss: 0.649\n",
      "709 - loss: 0.649\n",
      "710 - loss: 0.649\n",
      "711 - loss: 0.649\n",
      "712 - loss: 0.649\n",
      "713 - loss: 0.649\n",
      "714 - loss: 0.649\n",
      "715 - loss: 0.649\n",
      "716 - loss: 0.649\n",
      "717 - loss: 0.649\n",
      "718 - loss: 0.649\n",
      "719 - loss: 0.649\n",
      "f_new: 0.649 - f: 0.649 - Backtracking...\n",
      "721 - loss: 0.649\n",
      "722 - loss: 0.649\n",
      "723 - loss: 0.649\n",
      "724 - loss: 0.649\n",
      "725 - loss: 0.649\n",
      "726 - loss: 0.649\n",
      "727 - loss: 0.649\n",
      "728 - loss: 0.649\n",
      "729 - loss: 0.649\n",
      "730 - loss: 0.649\n",
      "731 - loss: 0.649\n",
      "732 - loss: 0.647\n",
      "f_new: 0.663 - f: 0.647 - Backtracking...\n",
      "734 - loss: 0.647\n",
      "735 - loss: 0.647\n",
      "736 - loss: 0.647\n",
      "737 - loss: 0.647\n",
      "738 - loss: 0.647\n",
      "739 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "741 - loss: 0.647\n",
      "742 - loss: 0.647\n",
      "743 - loss: 0.647\n",
      "744 - loss: 0.647\n",
      "745 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "747 - loss: 0.647\n",
      "748 - loss: 0.647\n",
      "749 - loss: 0.647\n",
      "750 - loss: 0.647\n",
      "751 - loss: 0.647\n",
      "752 - loss: 0.647\n",
      "753 - loss: 0.647\n",
      "754 - loss: 0.647\n",
      "755 - loss: 0.647\n",
      "756 - loss: 0.647\n",
      "757 - loss: 0.647\n",
      "758 - loss: 0.647\n",
      "759 - loss: 0.647\n",
      "760 - loss: 0.647\n",
      "761 - loss: 0.647\n",
      "762 - loss: 0.647\n",
      "763 - loss: 0.647\n",
      "764 - loss: 0.647\n",
      "765 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "767 - loss: 0.647\n",
      "768 - loss: 0.647\n",
      "769 - loss: 0.647\n",
      "770 - loss: 0.647\n",
      "771 - loss: 0.647\n",
      "772 - loss: 0.647\n",
      "773 - loss: 0.647\n",
      "774 - loss: 0.647\n",
      "775 - loss: 0.647\n",
      "776 - loss: 0.647\n",
      "777 - loss: 0.647\n",
      "778 - loss: 0.647\n",
      "779 - loss: 0.647\n",
      "780 - loss: 0.647\n",
      "781 - loss: 0.647\n",
      "782 - loss: 0.647\n",
      "783 - loss: 0.647\n",
      "784 - loss: 0.647\n",
      "785 - loss: 0.647\n",
      "786 - loss: 0.647\n",
      "787 - loss: 0.647\n",
      "788 - loss: 0.647\n",
      "789 - loss: 0.647\n",
      "790 - loss: 0.647\n",
      "791 - loss: 0.647\n",
      "792 - loss: 0.647\n",
      "793 - loss: 0.647\n",
      "794 - loss: 0.647\n",
      "795 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "797 - loss: 0.647\n",
      "798 - loss: 0.647\n",
      "799 - loss: 0.647\n",
      "800 - loss: 0.647\n",
      "801 - loss: 0.647\n",
      "802 - loss: 0.647\n",
      "803 - loss: 0.647\n",
      "804 - loss: 0.647\n",
      "805 - loss: 0.647\n",
      "806 - loss: 0.647\n",
      "807 - loss: 0.647\n",
      "808 - loss: 0.647\n",
      "809 - loss: 0.647\n",
      "810 - loss: 0.647\n",
      "811 - loss: 0.647\n",
      "812 - loss: 0.647\n",
      "813 - loss: 0.647\n",
      "814 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "816 - loss: 0.647\n",
      "817 - loss: 0.647\n",
      "818 - loss: 0.647\n",
      "819 - loss: 0.647\n",
      "820 - loss: 0.647\n",
      "821 - loss: 0.647\n",
      "822 - loss: 0.647\n",
      "823 - loss: 0.647\n",
      "824 - loss: 0.647\n",
      "825 - loss: 0.647\n",
      "826 - loss: 0.647\n",
      "827 - loss: 0.647\n",
      "828 - loss: 0.647\n",
      "829 - loss: 0.647\n",
      "830 - loss: 0.647\n",
      "831 - loss: 0.647\n",
      "832 - loss: 0.646\n",
      "f_new: 0.647 - f: 0.646 - Backtracking...\n",
      "834 - loss: 0.646\n",
      "835 - loss: 0.646\n",
      "836 - loss: 0.646\n",
      "837 - loss: 0.646\n",
      "838 - loss: 0.646\n",
      "839 - loss: 0.646\n",
      "840 - loss: 0.646\n",
      "841 - loss: 0.646\n",
      "842 - loss: 0.646\n",
      "843 - loss: 0.646\n",
      "844 - loss: 0.646\n",
      "f_new: 0.646 - f: 0.646 - Backtracking...\n",
      "846 - loss: 0.646\n",
      "847 - loss: 0.646\n",
      "848 - loss: 0.646\n",
      "849 - loss: 0.646\n",
      "850 - loss: 0.646\n",
      "851 - loss: 0.646\n",
      "852 - loss: 0.646\n",
      "853 - loss: 0.646\n",
      "854 - loss: 0.646\n",
      "855 - loss: 0.646\n",
      "856 - loss: 0.646\n",
      "857 - loss: 0.646\n",
      "858 - loss: 0.646\n",
      "859 - loss: 0.646\n",
      "860 - loss: 0.646\n",
      "861 - loss: 0.646\n",
      "862 - loss: 0.646\n",
      "f_new: 0.646 - f: 0.646 - Backtracking...\n",
      "864 - loss: 0.646\n",
      "865 - loss: 0.646\n",
      "866 - loss: 0.646\n",
      "867 - loss: 0.646\n",
      "868 - loss: 0.646\n",
      "869 - loss: 0.646\n",
      "870 - loss: 0.646\n",
      "871 - loss: 0.646\n",
      "872 - loss: 0.646\n",
      "873 - loss: 0.646\n",
      "874 - loss: 0.646\n",
      "875 - loss: 0.646\n",
      "876 - loss: 0.646\n",
      "877 - loss: 0.646\n",
      "878 - loss: 0.646\n",
      "879 - loss: 0.646\n",
      "880 - loss: 0.646\n",
      "881 - loss: 0.646\n",
      "882 - loss: 0.646\n",
      "883 - loss: 0.646\n",
      "884 - loss: 0.646\n",
      "885 - loss: 0.646\n",
      "886 - loss: 0.646\n",
      "887 - loss: 0.646\n",
      "888 - loss: 0.646\n",
      "889 - loss: 0.646\n",
      "890 - loss: 0.646\n",
      "891 - loss: 0.646\n",
      "892 - loss: 0.646\n",
      "893 - loss: 0.646\n",
      "894 - loss: 0.646\n",
      "895 - loss: 0.646\n",
      "896 - loss: 0.646\n",
      "897 - loss: 0.646\n",
      "898 - loss: 0.646\n",
      "899 - loss: 0.646\n",
      "900 - loss: 0.646\n",
      "901 - loss: 0.646\n",
      "902 - loss: 0.646\n",
      "903 - loss: 0.646\n",
      "904 - loss: 0.646\n",
      "905 - loss: 0.646\n",
      "906 - loss: 0.646\n",
      "907 - loss: 0.646\n",
      "908 - loss: 0.646\n",
      "909 - loss: 0.646\n",
      "910 - loss: 0.646\n",
      "911 - loss: 0.646\n",
      "912 - loss: 0.646\n",
      "913 - loss: 0.646\n",
      "914 - loss: 0.646\n",
      "915 - loss: 0.646\n",
      "916 - loss: 0.646\n",
      "917 - loss: 0.646\n",
      "918 - loss: 0.646\n",
      "919 - loss: 0.646\n",
      "920 - loss: 0.646\n",
      "921 - loss: 0.646\n",
      "922 - loss: 0.646\n",
      "923 - loss: 0.646\n",
      "924 - loss: 0.646\n",
      "925 - loss: 0.646\n",
      "926 - loss: 0.646\n",
      "927 - loss: 0.646\n",
      "928 - loss: 0.646\n",
      "f_new: 0.674 - f: 0.646 - Backtracking...\n",
      "f_new: 0.646 - f: 0.646 - Backtracking...\n",
      "931 - loss: 0.646\n",
      "932 - loss: 0.646\n",
      "933 - loss: 0.646\n",
      "934 - loss: 0.646\n",
      "935 - loss: 0.646\n",
      "936 - loss: 0.646\n",
      "937 - loss: 0.646\n",
      "938 - loss: 0.646\n",
      "939 - loss: 0.646\n",
      "940 - loss: 0.646\n",
      "941 - loss: 0.646\n",
      "942 - loss: 0.646\n",
      "943 - loss: 0.646\n",
      "944 - loss: 0.646\n",
      "945 - loss: 0.646\n",
      "946 - loss: 0.646\n",
      "947 - loss: 0.646\n",
      "948 - loss: 0.646\n",
      "949 - loss: 0.646\n",
      "950 - loss: 0.646\n",
      "951 - loss: 0.646\n",
      "952 - loss: 0.646\n",
      "953 - loss: 0.643\n",
      "f_new: 0.748 - f: 0.643 - Backtracking...\n",
      "f_new: 0.643 - f: 0.643 - Backtracking...\n",
      "956 - loss: 0.643\n",
      "957 - loss: 0.643\n",
      "958 - loss: 0.643\n",
      "959 - loss: 0.643\n",
      "960 - loss: 0.642\n",
      "f_new: 0.649 - f: 0.642 - Backtracking...\n",
      "962 - loss: 0.642\n",
      "963 - loss: 0.642\n",
      "964 - loss: 0.642\n",
      "965 - loss: 0.642\n",
      "966 - loss: 0.641\n",
      "f_new: 0.645 - f: 0.641 - Backtracking...\n",
      "968 - loss: 0.641\n",
      "969 - loss: 0.641\n",
      "970 - loss: 0.641\n",
      "971 - loss: 0.641\n",
      "f_new: 0.642 - f: 0.641 - Backtracking...\n",
      "973 - loss: 0.641\n",
      "974 - loss: 0.641\n",
      "975 - loss: 0.641\n",
      "976 - loss: 0.641\n",
      "977 - loss: 0.641\n",
      "f_new: 0.641 - f: 0.641 - Backtracking...\n",
      "979 - loss: 0.641\n",
      "980 - loss: 0.641\n",
      "981 - loss: 0.641\n",
      "982 - loss: 0.641\n",
      "983 - loss: 0.641\n",
      "984 - loss: 0.641\n",
      "985 - loss: 0.641\n",
      "986 - loss: 0.641\n",
      "987 - loss: 0.641\n",
      "988 - loss: 0.641\n",
      "989 - loss: 0.641\n",
      "f_new: 0.641 - f: 0.641 - Backtracking...\n",
      "991 - loss: 0.641\n",
      "992 - loss: 0.641\n",
      "993 - loss: 0.641\n",
      "994 - loss: 0.641\n",
      "995 - loss: 0.641\n",
      "996 - loss: 0.641\n",
      "997 - loss: 0.641\n",
      "998 - loss: 0.641\n",
      "999 - loss: 0.641\n",
      "1000 - loss: 0.641\n",
      "Reached maximum number of function evaluations 1000\n",
      "0.6813054955603551\n"
     ]
    }
   ],
   "source": [
    "#kernel_logreg_poly = LeastSquaresGDKernel(Kernel.kernel_poly, verbose=True, max_evals=1000)\n",
    "#kernel_logreg_poly = LogisticRegressionKernel(Kernel.kernel_poly, verbose=True, max_evals=1000)\n",
    "kernel_logreg_poly = LogisticRegressionKernel(Kernel.kernel_poly, verbose=True, max_evals=1000)\n",
    "p = 4\n",
    "preds = kernel_logreg_poly.predict(y_sub, X_sub, X_sub, p)\n",
    "print(compute_accuracy(preds, y_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_clean = remove_NaN_features(tX, 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegression()\n",
    "logReg.fit(y, tX_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 1289883598.7\n",
      "Iteration 1, loss = 929278938.1\n",
      "Iteration 2, loss = 843606874.7\n",
      "Iteration 3, loss = 832528308.0\n",
      "Iteration 4, loss = 831415267.7\n",
      "Iteration 5, loss = 831306045.8\n",
      "Iteration 6, loss = 831295253.7\n",
      "Iteration 7, loss = 831294167.6\n",
      "Iteration 8, loss = 831294054.6\n",
      "Iteration 9, loss = 831294042.1\n"
     ]
    }
   ],
   "source": [
    "model = AlternativePCA(k=2)\n",
    "model.fit(X)\n",
    "Z_pca = model.compress(X)\n",
    "Xhat_pca = model.expand(Z_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 1521186763.4\n",
      "Iteration 1, loss = 947138369.7\n",
      "Iteration 2, loss = 854324275.1\n",
      "Iteration 3, loss = 835473177.5\n",
      "Iteration 4, loss = 832070513.0\n",
      "Iteration 5, loss = 831441985.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antho\\Google Drive\\EPFL\\MA1\\CS-433 Machine Learning\\ML_course\\projects\\project1\\scripts\\solver.py:65: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  y = g_new - g\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 831322662.2\n",
      "Iteration 7, loss = 831299623.4\n",
      "Iteration 8, loss = 831295133.9\n",
      "Iteration 9, loss = 831294255.0\n",
      "Iteration 0, loss = 114368921.2\n",
      "Iteration 1, loss = 114357028.3\n",
      "Iteration 2, loss = 114357025.0\n",
      "Iteration 3, loss = 114357025.0\n",
      "Iteration 4, loss = 114357025.0\n",
      "Iteration 5, loss = 114357025.0\n",
      "Iteration 6, loss = 114357025.0\n",
      "Iteration 7, loss = 114357025.0\n",
      "Iteration 8, loss = 114357025.0\n",
      "Iteration 9, loss = 114357025.0\n"
     ]
    }
   ],
   "source": [
    "model = RobustPCA(k=2)\n",
    "model.fit(X)\n",
    "Z_robust = model.compress(X)\n",
    "Xhat_robust = model.expand(Z_robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.72588088, -7.31380373],\n",
       "       [ 3.89450982, 10.60756305],\n",
       "       [ 7.77930283, -1.90683368],\n",
       "       ...,\n",
       "       [-2.54520707,  6.4204025 ],\n",
       "       [-8.07558037, 21.54278193],\n",
       "       [-4.59888286, 21.69639677]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "#Need to rescale to do PCA\n",
    "def pca_axes(Z,scale,scale2, verbose=False):\n",
    "    \n",
    "    pcs = \n",
    "    colors = np.arctan2(pcs[0,:], pcs[1,:])\n",
    "    colormap = cm.inferno\n",
    "    norm = Normalize()\n",
    "    norm.autoscale(colors)\n",
    "    plt.rcParams['image.cmap'] = 'Paired'\n",
    "\n",
    "    # Quiver\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    plt.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),\n",
    "           pcs[0,:], pcs[1,:], color=colormap(norm(colors)),\n",
    "           angles='xy', scale_units='xy', scale=scale)\n",
    "    o = 0\n",
    "    for i in range(0,pcs.shape[1]):\n",
    "        #plt.arrow(0, 0, pcs[0,i], pcs[1,i],color='k') \n",
    "        if i==3:\n",
    "            plt.text(pcs[0,i]* scale2, pcs[1,i]*scale2-0.03, X.columns[i], color = 'k', \n",
    "             ha = 'center', va = 'center', fontsize=10, weight='bold')\n",
    "        elif i==12:\n",
    "            plt.text(pcs[0,i]* scale2, pcs[1,i]*scale2-0.03, X.columns[i], color = 'k', \n",
    "             ha = 'center', va = 'center', fontsize=10, weight='bold')\n",
    "        elif np.power(pcs[0,i]-pcs[1,i], 2)<0.001:\n",
    "            plt.text(0, 0-o, ' ', color = 'k', \n",
    "             ha = 'center', va = 'center', fontsize=12, weight='bold')\n",
    "        else:\n",
    "            plt.text(pcs[0,i]* scale2, pcs[1,i] *scale2, X.columns[i], color = 'k', \n",
    "             ha = 'center', va = 'center', fontsize=10, weight='bold')\n",
    "        o=o+0.04\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.xlim([-1.0,1.0])\n",
    "    plt.ylim([-0.5,1.0])\n",
    "    plt.xlabel('Principal component 0')\n",
    "    plt.ylabel('Principal component 1')\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    " \n",
    "    if verbose==True:\n",
    "        for i in range (0,pcs.shape[1]):\n",
    "            print(str(i), df.columns[i+2])\n",
    "        \n",
    "    return\n",
    "\n",
    "pca_axes(Z_pca, 0.65, 1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-524e3a5e580a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel_logreg_poly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\EPFL\\MA1\\CS-433 Machine Learning\\ML_course\\projects\\project1\\scripts\\classifiers.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, y, X, Xtest, lambda_, *args)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mKtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\EPFL\\MA1\\CS-433 Machine Learning\\ML_course\\projects\\project1\\scripts\\classifiers.py\u001b[0m in \u001b[0;36mkernel_RBF\u001b[1;34m(X1, X2, sigma)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mK\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[1;32m-> 2076\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   2077\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test kernel\n",
    "kernel_logreg_poly = LeastSquaresKernel(Kernel.kernel_RBF, verbose=True, max_evals=1000)\n",
    "p = 4\n",
    "sigma = 1\n",
    "accuracies = []\n",
    "for i in range(5):\n",
    "    print(i,  end=' - ')\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    X_sub = (X[shuffle_indices,:])[::100]\n",
    "    y_sub = (y[shuffle_indices])[::100]\n",
    "    \n",
    "    X_pred = (X[shuffle_indices,:])[::100]\n",
    "    y_pred = (y[shuffle_indices])[::100]\n",
    "    \n",
    "    preds = kernel_logreg_poly.predict(y_sub, X_sub, X_pred, sigma, lambda_=1)\n",
    "    accuracy = compute_accuracy(preds, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8757090909090909"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mse, loss_mse = least_squares(y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_mse = compute_accuracy(predict_labels(w_mse, X), y)\n",
    "print(accuracy_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_classifier(lambda_):\n",
    "    return ridge_regression(y, X, lambda_)\n",
    "\n",
    "lambda_ridge, _, _ = find_max_hyperparam(ridge_classifier, [10**c for c in range(-3,3)])\n",
    "print(\"Optimal lambda: %f\" % lambda_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_train(y_train, X_train):\n",
    "    return ridge_regression(y, X, lambda_ridge)\n",
    "\n",
    "def ridge_test(X_test, w):\n",
    "    return np.sign(X_test@w)\n",
    "\n",
    "accuracy_ridge = cross_validate(y, X, ridge_train, ridge_test, 0.8, 100)\n",
    "print(accuracy_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_train(y_train, X_train):\n",
    "    return logistic_regression(y_train, X_train, 0.01*np.ones(X_train.shape[1]), 1000, verbose=False)\n",
    "\n",
    "def log_reg_test(X_test, w):\n",
    "    return np.sign(X_test@w)\n",
    "\n",
    "accuracy_log_reg = cross_validate(y, X_safe, log_reg_train, log_reg_test, 0.7, 20)\n",
    "print(accuracy_log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_log_reg_classifier(lambda_):\n",
    "    return reg_logistic_regression(y, X_safe, lambda_, np.zeros(X_safe.shape[1]), 1000)\n",
    "\n",
    "def log_reg_sparse_classifier(lambda_):\n",
    "    return logistic_regression_sparse(y, X_safe, lambda_, np.zeros(X_safe.shape[1]), 1000)\n",
    "\n",
    "def mse_sparse_classifier(lambda_):\n",
    "    return least_squares_sparse(y, X_safe, lambda_, np.zeros(X_safe.shape[1]), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_log_reg_l2, _, _ = find_max_hyperparam(reg_log_reg_classifier, [10**c for c in range(-3,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_log_reg_l1, _, _ = find_max_hyperparam(log_reg_sparse_classifier, [10**c for c in range(-3,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_mse_l2, _, _ = find_max_hyperparam(ridge_classifier, [10**c for c in range(-6,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_mse_l1, w_mse_l1, _ = find_max_hyperparam(mse_sparse_classifier, [10**c for c in range(-6,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mse_l1, _ = least_squares_sparse(y, X_safe, 0.01, np.zeros(X_safe.shape[1]), 1000)\n",
    "print(\"Non-zero weights: %i / %i\" % (np.sum(w_mse_l1 != 0), len(w_mse_l1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1\n",
    "ypred_kernel = kernel_predict(kernel_poly, y, X_safe, X_safe, p, lambda_=1)\n",
    "print(compute_accuracy(ypred_kernel, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "y_test, tX_test, ids_test, _ = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions using new model\n",
    "test_split_indices, X_test_split, _ = split_data(tX_test)\n",
    "X_test_split_poly = [ build_X(X, 8, 2) for X in X_test_split ]\n",
    "y_pred = np.ones(tX_test.shape[0])\n",
    "for model, X, indices in zip(models, X_test_split_poly, test_split_indices):\n",
    "    y_pred[indices] = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/predictions.csv'\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/predictions.csv'\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
