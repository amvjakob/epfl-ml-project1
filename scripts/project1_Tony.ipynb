{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot', 'DER_sum_pt', 'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality', 'DER_lep_eta_centrality', 'PRI_tau_pt', 'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi', 'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi', 'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_all_pt']\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids, features = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tXToX(tX):\n",
    "    X = np.c_[np.ones(tX.shape[0]), tX]\n",
    "\n",
    "    X_safe = X\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            if X[i,j] < -990:\n",
    "                X[i,j] = 0\n",
    "    \n",
    "    return X_safe\n",
    "\n",
    "X = np.c_[np.ones(len(y)), tX]\n",
    "n, d = X.shape\n",
    "\n",
    "X_safe = tXToX(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from classifiers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for j in range(tX.shape[1]):\n",
    "    # get examples where the feature j is NaN\n",
    "    positions = tX[:,j] == -999\n",
    "    if np.sum(positions)/len(positions) <= 0.28:\n",
    "        # add column to X\n",
    "        if len(X) == 0:\n",
    "            X = tX[:,j]\n",
    "        else:\n",
    "            X = np.c_[X, tX[:,j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = replace_NaN_by_median(X)\n",
    "X, _, _ = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub = X[::30]\n",
    "y_sub = y[::30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_new: 351332120652.464 - f: 5776.689 - Backtracking...\n",
      "f_new: 49908376999.952 - f: 5776.689 - Backtracking...\n",
      "f_new: 7089719742.161 - f: 5776.689 - Backtracking...\n",
      "f_new: 1007128546.313 - f: 5776.689 - Backtracking...\n",
      "f_new: 143067923.558 - f: 5776.689 - Backtracking...\n",
      "f_new: 20324057.388 - f: 5776.689 - Backtracking...\n",
      "f_new: 2887715.309 - f: 5776.689 - Backtracking...\n",
      "f_new: 410801.817 - f: 5776.689 - Backtracking...\n",
      "f_new: 58949.841 - f: 5776.689 - Backtracking...\n",
      "f_new: 9667.717 - f: 5776.689 - Backtracking...\n",
      "11 - loss: 5549.281\n",
      "12 - loss: 5405.800\n",
      "13 - loss: 5352.943\n",
      "14 - loss: 5260.879\n",
      "15 - loss: 5057.649\n",
      "f_new: 5188.842 - f: 5057.649 - Backtracking...\n",
      "17 - loss: 5042.088\n",
      "18 - loss: 5030.287\n",
      "19 - loss: 5022.815\n",
      "20 - loss: 4723.083\n",
      "f_new: 5530.003 - f: 4723.083 - Backtracking...\n",
      "22 - loss: 4714.058\n",
      "23 - loss: 4702.917\n",
      "24 - loss: 4700.799\n",
      "25 - loss: 4698.556\n",
      "26 - loss: 4630.235\n",
      "27 - loss: 4608.311\n",
      "f_new: 4631.497 - f: 4608.311 - Backtracking...\n",
      "29 - loss: 4598.083\n",
      "30 - loss: 4596.268\n",
      "31 - loss: 4594.125\n",
      "32 - loss: 4582.093\n",
      "33 - loss: 4575.996\n",
      "34 - loss: 4550.599\n",
      "f_new: 4573.412 - f: 4550.599 - Backtracking...\n",
      "36 - loss: 4549.247\n",
      "37 - loss: 4549.045\n",
      "38 - loss: 4548.844\n",
      "39 - loss: 4529.424\n",
      "f_new: 4534.597 - f: 4529.424 - Backtracking...\n",
      "41 - loss: 4526.565\n",
      "f_new: 4531.257 - f: 4526.565 - Backtracking...\n",
      "43 - loss: 4525.883\n",
      "44 - loss: 4525.601\n",
      "45 - loss: 4525.368\n",
      "46 - loss: 4523.787\n",
      "47 - loss: 4522.931\n",
      "48 - loss: 4522.724\n",
      "49 - loss: 4522.637\n",
      "50 - loss: 4522.573\n",
      "51 - loss: 4521.830\n",
      "52 - loss: 4503.879\n",
      "f_new: 4512.120 - f: 4503.879 - Backtracking...\n",
      "54 - loss: 4503.280\n",
      "f_new: 4504.387 - f: 4503.280 - Backtracking...\n",
      "56 - loss: 4503.135\n",
      "57 - loss: 4503.078\n",
      "58 - loss: 4503.030\n",
      "59 - loss: 4502.749\n",
      "60 - loss: 4502.654\n",
      "61 - loss: 4502.642\n",
      "62 - loss: 4502.635\n",
      "63 - loss: 4502.627\n",
      "64 - loss: 4499.973\n",
      "65 - loss: 4499.873\n",
      "f_new: 4501.201 - f: 4499.873 - Backtracking...\n",
      "67 - loss: 4499.465\n",
      "68 - loss: 4499.452\n",
      "69 - loss: 4499.420\n",
      "70 - loss: 4499.313\n",
      "71 - loss: 4499.241\n",
      "72 - loss: 4499.151\n",
      "73 - loss: 4499.114\n",
      "74 - loss: 4499.100\n",
      "75 - loss: 4499.098\n",
      "76 - loss: 4498.991\n",
      "77 - loss: 4498.931\n",
      "78 - loss: 4498.902\n",
      "f_new: 4498.948 - f: 4498.902 - Backtracking...\n",
      "80 - loss: 4498.897\n",
      "81 - loss: 4498.896\n",
      "82 - loss: 4498.895\n",
      "83 - loss: 4498.882\n",
      "84 - loss: 4498.868\n",
      "85 - loss: 4498.805\n",
      "86 - loss: 4498.798\n",
      "87 - loss: 4498.741\n",
      "88 - loss: 4498.741\n",
      "89 - loss: 4498.740\n",
      "90 - loss: 4498.728\n",
      "91 - loss: 4498.716\n",
      "92 - loss: 4498.636\n",
      "93 - loss: 4498.474\n",
      "94 - loss: 4498.428\n",
      "95 - loss: 4498.427\n",
      "96 - loss: 4498.427\n",
      "97 - loss: 4498.417\n",
      "98 - loss: 4498.407\n",
      "99 - loss: 4497.942\n",
      "f_new: 4498.956 - f: 4497.942 - Backtracking...\n",
      "101 - loss: 4497.912\n",
      "102 - loss: 4497.882\n",
      "103 - loss: 4497.881\n",
      "104 - loss: 4497.880\n",
      "105 - loss: 4497.852\n",
      "106 - loss: 4497.820\n",
      "107 - loss: 4497.806\n",
      "108 - loss: 4497.796\n",
      "109 - loss: 4497.795\n",
      "110 - loss: 4497.794\n",
      "111 - loss: 4497.793\n",
      "112 - loss: 4497.354\n",
      "113 - loss: 4496.800\n",
      "114 - loss: 4496.751\n",
      "f_new: 4496.845 - f: 4496.751 - Backtracking...\n",
      "116 - loss: 4496.748\n",
      "117 - loss: 4496.747\n",
      "118 - loss: 4496.746\n",
      "119 - loss: 4496.737\n",
      "120 - loss: 4496.728\n",
      "121 - loss: 4496.620\n",
      "122 - loss: 4496.497\n",
      "123 - loss: 4496.491\n",
      "124 - loss: 4496.490\n",
      "125 - loss: 4496.489\n",
      "126 - loss: 4496.475\n",
      "127 - loss: 4496.460\n",
      "128 - loss: 4496.432\n",
      "129 - loss: 4496.381\n",
      "130 - loss: 4496.363\n",
      "131 - loss: 4496.360\n",
      "132 - loss: 4496.360\n",
      "133 - loss: 4496.359\n",
      "134 - loss: 4496.353\n",
      "135 - loss: 4496.343\n",
      "136 - loss: 4496.325\n",
      "137 - loss: 4496.039\n",
      "138 - loss: 4496.038\n",
      "139 - loss: 4496.037\n",
      "140 - loss: 4496.036\n",
      "141 - loss: 4496.026\n",
      "142 - loss: 4496.015\n",
      "143 - loss: 4495.933\n",
      "144 - loss: 4493.641\n",
      "f_new: 4493.794 - f: 4493.641 - Backtracking...\n",
      "146 - loss: 4493.636\n",
      "147 - loss: 4493.634\n",
      "148 - loss: 4493.633\n",
      "149 - loss: 4493.621\n",
      "150 - loss: 4493.613\n",
      "151 - loss: 4493.612\n",
      "152 - loss: 4493.611\n",
      "153 - loss: 4493.611\n",
      "154 - loss: 4468.050\n",
      "f_new: 17878.460 - f: 4468.050 - Backtracking...\n",
      "f_new: 4468.051 - f: 4468.050 - Backtracking...\n",
      "157 - loss: 4468.048\n",
      "158 - loss: 4468.046\n",
      "159 - loss: 4468.046\n",
      "160 - loss: 4468.045\n",
      "161 - loss: 4468.032\n",
      "162 - loss: 4468.018\n",
      "163 - loss: 4468.005\n",
      "164 - loss: 4468.003\n",
      "165 - loss: 4468.002\n",
      "166 - loss: 4468.002\n",
      "167 - loss: 4468.000\n",
      "168 - loss: 4467.988\n",
      "169 - loss: 4467.972\n",
      "170 - loss: 4467.958\n",
      "171 - loss: 4467.956\n",
      "172 - loss: 4467.955\n",
      "173 - loss: 4467.954\n",
      "174 - loss: 4467.954\n",
      "175 - loss: 4467.945\n",
      "176 - loss: 4467.935\n",
      "177 - loss: 4467.921\n",
      "178 - loss: 4467.908\n",
      "179 - loss: 4467.908\n",
      "180 - loss: 4467.907\n",
      "181 - loss: 4467.907\n",
      "182 - loss: 4467.899\n",
      "183 - loss: 4467.892\n",
      "184 - loss: 4467.873\n",
      "185 - loss: 4467.829\n",
      "186 - loss: 4467.828\n",
      "187 - loss: 4467.828\n",
      "188 - loss: 4467.828\n",
      "189 - loss: 4467.822\n",
      "190 - loss: 4467.817\n",
      "191 - loss: 4467.605\n",
      "192 - loss: 4467.590\n",
      "193 - loss: 4467.517\n",
      "194 - loss: 4467.516\n",
      "195 - loss: 4467.516\n",
      "196 - loss: 4467.484\n",
      "197 - loss: 4467.446\n",
      "198 - loss: 4467.440\n",
      "199 - loss: 4467.434\n",
      "200 - loss: 4467.433\n",
      "201 - loss: 4467.433\n",
      "202 - loss: 4467.433\n",
      "203 - loss: 4467.224\n",
      "204 - loss: 4461.692\n",
      "f_new: 4465.338 - f: 4461.692 - Backtracking...\n",
      "206 - loss: 4461.682\n",
      "207 - loss: 4461.676\n",
      "208 - loss: 4461.674\n",
      "209 - loss: 4461.673\n",
      "210 - loss: 4461.665\n",
      "211 - loss: 4461.660\n",
      "212 - loss: 4461.659\n",
      "213 - loss: 4461.659\n",
      "214 - loss: 4461.659\n",
      "215 - loss: 4461.646\n",
      "216 - loss: 4461.562\n",
      "217 - loss: 4461.462\n",
      "218 - loss: 4461.461\n",
      "219 - loss: 4461.461\n",
      "220 - loss: 4461.460\n",
      "221 - loss: 4461.445\n",
      "222 - loss: 4461.427\n",
      "223 - loss: 4461.421\n",
      "224 - loss: 4461.416\n",
      "225 - loss: 4461.412\n",
      "226 - loss: 4461.412\n",
      "227 - loss: 4461.411\n",
      "228 - loss: 4461.411\n",
      "229 - loss: 4460.963\n",
      "230 - loss: 4460.522\n",
      "f_new: 4460.592 - f: 4460.522 - Backtracking...\n",
      "232 - loss: 4460.519\n",
      "233 - loss: 4460.517\n",
      "234 - loss: 4460.516\n",
      "235 - loss: 4460.513\n",
      "236 - loss: 4460.506\n",
      "237 - loss: 4460.502\n",
      "238 - loss: 4460.501\n",
      "239 - loss: 4460.500\n",
      "240 - loss: 4460.500\n",
      "241 - loss: 4460.446\n",
      "242 - loss: 4460.396\n",
      "243 - loss: 4460.393\n",
      "244 - loss: 4460.392\n",
      "245 - loss: 4460.392\n",
      "246 - loss: 4460.389\n",
      "247 - loss: 4460.365\n",
      "248 - loss: 4460.297\n",
      "249 - loss: 4460.287\n",
      "250 - loss: 4460.282\n",
      "251 - loss: 4460.280\n",
      "252 - loss: 4460.280\n",
      "253 - loss: 4460.279\n",
      "254 - loss: 4460.246\n",
      "255 - loss: 4453.774\n",
      "f_new: 4465.565 - f: 4453.774 - Backtracking...\n",
      "257 - loss: 4453.764\n",
      "258 - loss: 4453.761\n",
      "259 - loss: 4453.759\n",
      "260 - loss: 4453.759\n",
      "261 - loss: 4453.758\n",
      "262 - loss: 4453.748\n",
      "263 - loss: 4453.736\n",
      "264 - loss: 4453.730\n",
      "265 - loss: 4453.725\n",
      "266 - loss: 4453.721\n",
      "267 - loss: 4453.720\n",
      "268 - loss: 4453.720\n",
      "269 - loss: 4453.720\n",
      "270 - loss: 4453.154\n",
      "271 - loss: 4452.624\n",
      "f_new: 4452.717 - f: 4452.624 - Backtracking...\n",
      "273 - loss: 4452.598\n",
      "274 - loss: 4452.582\n",
      "275 - loss: 4452.577\n",
      "276 - loss: 4452.574\n",
      "277 - loss: 4452.553\n",
      "278 - loss: 4452.546\n",
      "279 - loss: 4452.541\n",
      "280 - loss: 4452.537\n",
      "281 - loss: 4452.536\n",
      "282 - loss: 4452.536\n",
      "283 - loss: 4452.509\n",
      "284 - loss: 4452.487\n",
      "285 - loss: 4452.478\n",
      "286 - loss: 4452.476\n",
      "287 - loss: 4452.476\n",
      "288 - loss: 4452.476\n",
      "289 - loss: 4452.458\n",
      "290 - loss: 4452.438\n",
      "291 - loss: 4452.434\n",
      "292 - loss: 4452.430\n",
      "293 - loss: 4452.428\n",
      "294 - loss: 4452.428\n",
      "295 - loss: 4452.428\n",
      "296 - loss: 4452.420\n",
      "297 - loss: 4452.183\n",
      "298 - loss: 4451.893\n",
      "f_new: 4451.894 - f: 4451.893 - Backtracking...\n",
      "300 - loss: 4451.891\n",
      "301 - loss: 4451.891\n",
      "302 - loss: 4451.891\n",
      "303 - loss: 4451.886\n",
      "304 - loss: 4451.882\n",
      "305 - loss: 4451.871\n",
      "306 - loss: 4451.870\n",
      "307 - loss: 4451.870\n",
      "308 - loss: 4451.869\n",
      "309 - loss: 4451.818\n",
      "310 - loss: 4451.767\n",
      "311 - loss: 4451.711\n",
      "312 - loss: 4451.709\n",
      "313 - loss: 4451.709\n",
      "314 - loss: 4451.709\n",
      "315 - loss: 4451.705\n",
      "316 - loss: 4451.701\n",
      "317 - loss: 4451.073\n",
      "f_new: 4452.226 - f: 4451.073 - Backtracking...\n",
      "319 - loss: 4451.067\n",
      "320 - loss: 4451.061\n",
      "321 - loss: 4451.060\n",
      "322 - loss: 4451.060\n",
      "323 - loss: 4451.017\n",
      "324 - loss: 4450.972\n",
      "325 - loss: 4450.967\n",
      "326 - loss: 4450.963\n",
      "327 - loss: 4450.963\n",
      "328 - loss: 4450.963\n",
      "329 - loss: 4450.962\n",
      "330 - loss: 4450.785\n",
      "331 - loss: 4450.594\n",
      "332 - loss: 4450.583\n",
      "f_new: 4450.584 - f: 4450.583 - Backtracking...\n",
      "334 - loss: 4450.582\n",
      "335 - loss: 4450.581\n",
      "336 - loss: 4450.581\n",
      "337 - loss: 4450.580\n",
      "338 - loss: 4450.575\n",
      "339 - loss: 4450.569\n",
      "340 - loss: 4450.555\n",
      "341 - loss: 4450.551\n",
      "342 - loss: 4450.551\n",
      "343 - loss: 4450.550\n",
      "344 - loss: 4450.550\n",
      "345 - loss: 4450.521\n",
      "346 - loss: 4450.491\n",
      "347 - loss: 4450.487\n",
      "348 - loss: 4450.483\n",
      "349 - loss: 4450.483\n",
      "350 - loss: 4450.482\n",
      "351 - loss: 4450.482\n",
      "352 - loss: 4447.174\n",
      "f_new: 4494.868 - f: 4447.174 - Backtracking...\n",
      "354 - loss: 4446.870\n",
      "355 - loss: 4446.776\n",
      "356 - loss: 4446.682\n",
      "357 - loss: 4446.678\n",
      "358 - loss: 4446.626\n",
      "359 - loss: 4446.595\n",
      "360 - loss: 4446.566\n",
      "f_new: 4446.600 - f: 4446.566 - Backtracking...\n",
      "362 - loss: 4446.564\n",
      "363 - loss: 4446.562\n",
      "364 - loss: 4446.562\n",
      "365 - loss: 4446.561\n",
      "366 - loss: 4446.545\n",
      "367 - loss: 4446.533\n",
      "368 - loss: 4446.529\n",
      "f_new: 4446.530 - f: 4446.529 - Backtracking...\n",
      "370 - loss: 4446.528\n",
      "371 - loss: 4446.528\n",
      "372 - loss: 4446.527\n",
      "373 - loss: 4446.523\n",
      "374 - loss: 4446.519\n",
      "375 - loss: 4446.508\n",
      "376 - loss: 4446.491\n",
      "377 - loss: 4446.491\n",
      "378 - loss: 4446.491\n",
      "379 - loss: 4446.490\n",
      "380 - loss: 4446.487\n",
      "381 - loss: 4446.483\n",
      "382 - loss: 4446.426\n",
      "383 - loss: 4446.209\n",
      "384 - loss: 4446.208\n",
      "385 - loss: 4446.207\n",
      "386 - loss: 4446.207\n",
      "387 - loss: 4446.203\n",
      "388 - loss: 4446.199\n",
      "389 - loss: 4446.186\n",
      "390 - loss: 4446.165\n",
      "391 - loss: 4446.162\n",
      "392 - loss: 4446.162\n",
      "393 - loss: 4446.162\n",
      "394 - loss: 4446.161\n",
      "395 - loss: 4446.144\n",
      "396 - loss: 4446.124\n",
      "397 - loss: 4446.119\n",
      "398 - loss: 4446.116\n",
      "399 - loss: 4446.114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 - loss: 4446.114\n",
      "401 - loss: 4446.114\n",
      "402 - loss: 4446.112\n",
      "403 - loss: 4439.172\n",
      "f_new: 6378.863 - f: 4439.172 - Backtracking...\n",
      "405 - loss: 4439.165\n",
      "406 - loss: 4439.154\n",
      "407 - loss: 4439.154\n",
      "408 - loss: 4439.154\n",
      "409 - loss: 4439.151\n",
      "410 - loss: 4439.148\n",
      "411 - loss: 4439.077\n",
      "f_new: 4439.109 - f: 4439.077 - Backtracking...\n",
      "413 - loss: 4439.072\n",
      "414 - loss: 4439.067\n",
      "415 - loss: 4439.067\n",
      "416 - loss: 4439.066\n",
      "417 - loss: 4438.991\n",
      "418 - loss: 4438.892\n",
      "419 - loss: 4438.888\n",
      "f_new: 4438.888 - f: 4438.888 - Backtracking...\n",
      "421 - loss: 4438.887\n",
      "422 - loss: 4438.887\n",
      "423 - loss: 4438.887\n",
      "424 - loss: 4438.884\n",
      "425 - loss: 4438.881\n",
      "426 - loss: 4438.876\n",
      "427 - loss: 4438.798\n",
      "428 - loss: 4438.691\n",
      "429 - loss: 4438.690\n",
      "430 - loss: 4438.689\n",
      "431 - loss: 4438.689\n",
      "432 - loss: 4438.688\n",
      "433 - loss: 4438.685\n",
      "434 - loss: 4438.682\n",
      "435 - loss: 4438.675\n",
      "436 - loss: 4438.674\n",
      "437 - loss: 4438.674\n",
      "438 - loss: 4438.674\n",
      "439 - loss: 4438.644\n",
      "440 - loss: 4438.614\n",
      "441 - loss: 4438.574\n",
      "442 - loss: 4438.573\n",
      "443 - loss: 4438.572\n",
      "444 - loss: 4438.572\n",
      "445 - loss: 4438.572\n",
      "446 - loss: 4438.569\n",
      "447 - loss: 4438.566\n",
      "448 - loss: 4438.536\n",
      "449 - loss: 4438.519\n",
      "450 - loss: 4438.518\n",
      "451 - loss: 4438.518\n",
      "452 - loss: 4438.518\n",
      "453 - loss: 4438.508\n",
      "454 - loss: 4438.498\n",
      "455 - loss: 4438.494\n",
      "456 - loss: 4438.491\n",
      "457 - loss: 4438.487\n",
      "458 - loss: 4438.487\n",
      "459 - loss: 4438.487\n",
      "460 - loss: 4438.487\n",
      "461 - loss: 4437.533\n",
      "f_new: 4442.972 - f: 4437.533 - Backtracking...\n",
      "463 - loss: 4436.354\n",
      "464 - loss: 4435.380\n",
      "f_new: 4437.712 - f: 4435.380 - Backtracking...\n",
      "466 - loss: 4435.229\n",
      "467 - loss: 4435.229\n",
      "468 - loss: 4435.228\n",
      "469 - loss: 4435.225\n",
      "470 - loss: 4435.222\n",
      "471 - loss: 4435.211\n",
      "472 - loss: 4435.210\n",
      "473 - loss: 4435.210\n",
      "474 - loss: 4435.209\n",
      "475 - loss: 4435.198\n",
      "476 - loss: 4435.186\n",
      "477 - loss: 4435.183\n",
      "478 - loss: 4435.181\n",
      "479 - loss: 4435.180\n",
      "480 - loss: 4435.180\n",
      "481 - loss: 4435.180\n",
      "482 - loss: 4434.978\n",
      "483 - loss: 4434.638\n",
      "484 - loss: 4434.624\n",
      "f_new: 4434.651 - f: 4434.624 - Backtracking...\n",
      "486 - loss: 4434.622\n",
      "487 - loss: 4434.621\n",
      "488 - loss: 4434.620\n",
      "489 - loss: 4434.614\n",
      "490 - loss: 4434.612\n",
      "491 - loss: 4434.611\n",
      "492 - loss: 4434.611\n",
      "493 - loss: 4434.611\n",
      "494 - loss: 4434.592\n",
      "495 - loss: 4434.573\n",
      "496 - loss: 4434.544\n",
      "497 - loss: 4434.543\n",
      "498 - loss: 4434.543\n",
      "499 - loss: 4434.543\n",
      "500 - loss: 4434.539\n",
      "501 - loss: 4434.534\n",
      "502 - loss: 4434.528\n",
      "503 - loss: 4434.518\n",
      "504 - loss: 4434.513\n",
      "505 - loss: 4434.513\n",
      "506 - loss: 4434.513\n",
      "507 - loss: 4434.513\n",
      "508 - loss: 4434.509\n",
      "509 - loss: 4434.506\n",
      "510 - loss: 4434.490\n",
      "511 - loss: 4434.389\n",
      "512 - loss: 4434.388\n",
      "513 - loss: 4434.388\n",
      "514 - loss: 4434.388\n",
      "515 - loss: 4434.385\n",
      "516 - loss: 4434.383\n",
      "517 - loss: 4432.661\n",
      "f_new: 4435.044 - f: 4432.661 - Backtracking...\n",
      "519 - loss: 4432.661\n",
      "520 - loss: 4432.661\n",
      "521 - loss: 4432.661\n",
      "522 - loss: 4432.635\n",
      "523 - loss: 4432.609\n",
      "524 - loss: 4432.606\n",
      "525 - loss: 4432.604\n",
      "526 - loss: 4432.603\n",
      "527 - loss: 4432.603\n",
      "528 - loss: 4432.603\n",
      "529 - loss: 4432.582\n",
      "530 - loss: 4432.445\n",
      "531 - loss: 4431.995\n",
      "f_new: 4432.004 - f: 4431.995 - Backtracking...\n",
      "533 - loss: 4431.993\n",
      "534 - loss: 4431.993\n",
      "535 - loss: 4431.993\n",
      "536 - loss: 4431.989\n",
      "537 - loss: 4431.986\n",
      "538 - loss: 4431.977\n",
      "539 - loss: 4431.977\n",
      "540 - loss: 4431.976\n",
      "541 - loss: 4431.976\n",
      "542 - loss: 4431.966\n",
      "543 - loss: 4431.955\n",
      "544 - loss: 4431.952\n",
      "545 - loss: 4431.950\n",
      "546 - loss: 4431.950\n",
      "547 - loss: 4431.950\n",
      "548 - loss: 4431.949\n",
      "549 - loss: 4431.870\n",
      "550 - loss: 4431.790\n",
      "551 - loss: 4431.775\n",
      "552 - loss: 4431.773\n",
      "553 - loss: 4431.772\n",
      "554 - loss: 4431.772\n",
      "555 - loss: 4431.772\n",
      "556 - loss: 4431.769\n",
      "557 - loss: 4431.767\n",
      "558 - loss: 4431.746\n",
      "559 - loss: 4431.696\n",
      "560 - loss: 4431.688\n",
      "561 - loss: 4431.687\n",
      "562 - loss: 4431.686\n",
      "563 - loss: 4431.686\n",
      "564 - loss: 4431.684\n",
      "565 - loss: 4431.681\n",
      "566 - loss: 4431.554\n",
      "567 - loss: 4431.105\n",
      "f_new: 4431.108 - f: 4431.105 - Backtracking...\n",
      "569 - loss: 4431.103\n",
      "570 - loss: 4431.103\n",
      "571 - loss: 4431.102\n",
      "572 - loss: 4431.091\n",
      "573 - loss: 4431.082\n",
      "574 - loss: 4431.079\n",
      "575 - loss: 4431.077\n",
      "576 - loss: 4431.077\n",
      "577 - loss: 4431.077\n",
      "578 - loss: 4431.076\n",
      "579 - loss: 4431.065\n",
      "580 - loss: 4431.054\n",
      "581 - loss: 4431.052\n",
      "582 - loss: 4431.049\n",
      "583 - loss: 4431.047\n",
      "584 - loss: 4431.047\n",
      "585 - loss: 4431.046\n",
      "586 - loss: 4431.046\n",
      "587 - loss: 4429.409\n",
      "588 - loss: 4428.748\n",
      "f_new: 4435.013 - f: 4428.748 - Backtracking...\n",
      "590 - loss: 4428.595\n",
      "591 - loss: 4428.578\n",
      "592 - loss: 4428.559\n",
      "593 - loss: 4427.957\n",
      "594 - loss: 4427.932\n",
      "f_new: 4428.104 - f: 4427.932 - Backtracking...\n",
      "596 - loss: 4427.866\n",
      "597 - loss: 4427.826\n",
      "598 - loss: 4427.813\n",
      "599 - loss: 4427.805\n",
      "600 - loss: 4427.750\n",
      "601 - loss: 4427.734\n",
      "602 - loss: 4427.729\n",
      "603 - loss: 4427.726\n",
      "604 - loss: 4427.725\n",
      "605 - loss: 4427.722\n",
      "606 - loss: 4427.592\n",
      "607 - loss: 4427.571\n",
      "608 - loss: 4427.564\n",
      "609 - loss: 4427.560\n",
      "610 - loss: 4427.559\n",
      "611 - loss: 4427.555\n",
      "612 - loss: 4427.550\n",
      "613 - loss: 4427.546\n",
      "614 - loss: 4427.542\n",
      "615 - loss: 4427.541\n",
      "616 - loss: 4427.540\n",
      "617 - loss: 4427.540\n",
      "618 - loss: 4427.540\n",
      "619 - loss: 4427.534\n",
      "620 - loss: 4427.527\n",
      "621 - loss: 4427.525\n",
      "622 - loss: 4427.522\n",
      "623 - loss: 4427.522\n",
      "624 - loss: 4427.522\n",
      "625 - loss: 4427.522\n",
      "626 - loss: 4427.505\n",
      "627 - loss: 4427.488\n",
      "628 - loss: 4427.480\n",
      "629 - loss: 4427.478\n",
      "630 - loss: 4427.476\n",
      "631 - loss: 4427.475\n",
      "632 - loss: 4427.475\n",
      "633 - loss: 4427.475\n",
      "634 - loss: 4425.864\n",
      "f_new: 4425.902 - f: 4425.864 - Backtracking...\n",
      "636 - loss: 4425.048\n",
      "f_new: 4431.087 - f: 4425.048 - Backtracking...\n",
      "638 - loss: 4424.960\n",
      "639 - loss: 4424.934\n",
      "640 - loss: 4424.909\n",
      "641 - loss: 4424.625\n",
      "642 - loss: 4424.465\n",
      "643 - loss: 4424.461\n",
      "644 - loss: 4424.348\n",
      "645 - loss: 4424.340\n",
      "646 - loss: 4424.338\n",
      "647 - loss: 4424.272\n",
      "648 - loss: 4424.254\n",
      "649 - loss: 4424.245\n",
      "f_new: 4424.256 - f: 4424.245 - Backtracking...\n",
      "651 - loss: 4424.240\n",
      "652 - loss: 4424.240\n",
      "653 - loss: 4424.239\n",
      "654 - loss: 4424.235\n",
      "655 - loss: 4424.230\n",
      "656 - loss: 4424.221\n",
      "f_new: 4424.222 - f: 4424.221 - Backtracking...\n",
      "658 - loss: 4424.220\n",
      "659 - loss: 4424.219\n",
      "660 - loss: 4424.219\n",
      "661 - loss: 4424.217\n",
      "662 - loss: 4424.201\n",
      "663 - loss: 4424.171\n",
      "f_new: 4424.175 - f: 4424.171 - Backtracking...\n",
      "665 - loss: 4424.170\n",
      "666 - loss: 4424.169\n",
      "667 - loss: 4424.169\n",
      "668 - loss: 4424.168\n",
      "669 - loss: 4424.166\n",
      "670 - loss: 4424.164\n",
      "671 - loss: 4424.135\n",
      "672 - loss: 4424.078\n",
      "673 - loss: 4424.078\n",
      "674 - loss: 4424.077\n",
      "675 - loss: 4424.077\n",
      "676 - loss: 4424.075\n",
      "677 - loss: 4424.073\n",
      "678 - loss: 4424.062\n",
      "679 - loss: 4424.050\n",
      "680 - loss: 4424.049\n",
      "681 - loss: 4424.049\n",
      "682 - loss: 4424.049\n",
      "683 - loss: 4424.035\n",
      "684 - loss: 4424.018\n",
      "685 - loss: 4424.016\n",
      "686 - loss: 4424.014\n",
      "687 - loss: 4424.012\n",
      "688 - loss: 4424.012\n",
      "689 - loss: 4424.012\n",
      "690 - loss: 4424.011\n",
      "691 - loss: 4423.206\n",
      "692 - loss: 4422.442\n",
      "f_new: 4422.453 - f: 4422.442 - Backtracking...\n",
      "694 - loss: 4422.425\n",
      "695 - loss: 4422.425\n",
      "696 - loss: 4422.424\n",
      "697 - loss: 4422.421\n",
      "698 - loss: 4422.419\n",
      "699 - loss: 4422.409\n",
      "700 - loss: 4422.408\n",
      "701 - loss: 4422.408\n",
      "702 - loss: 4422.408\n",
      "703 - loss: 4422.314\n",
      "704 - loss: 4422.227\n",
      "f_new: 4422.263 - f: 4422.227 - Backtracking...\n",
      "706 - loss: 4422.227\n",
      "707 - loss: 4422.227\n",
      "708 - loss: 4422.227\n",
      "709 - loss: 4422.224\n",
      "710 - loss: 4422.221\n",
      "711 - loss: 4422.216\n",
      "712 - loss: 4422.212\n",
      "713 - loss: 4422.211\n",
      "714 - loss: 4422.211\n",
      "715 - loss: 4422.211\n",
      "716 - loss: 4422.208\n",
      "717 - loss: 4422.205\n",
      "718 - loss: 4422.200\n",
      "719 - loss: 4422.192\n",
      "720 - loss: 4422.191\n",
      "721 - loss: 4422.191\n",
      "722 - loss: 4422.190\n",
      "723 - loss: 4422.190\n",
      "724 - loss: 4422.188\n",
      "725 - loss: 4422.185\n",
      "726 - loss: 4422.177\n",
      "727 - loss: 4422.161\n",
      "728 - loss: 4422.161\n",
      "729 - loss: 4422.161\n",
      "730 - loss: 4422.161\n",
      "731 - loss: 4422.159\n",
      "732 - loss: 4422.157\n",
      "733 - loss: 4422.108\n",
      "734 - loss: 4422.046\n",
      "735 - loss: 4422.043\n",
      "736 - loss: 4422.043\n",
      "737 - loss: 4422.042\n",
      "738 - loss: 4422.040\n",
      "739 - loss: 4422.038\n",
      "740 - loss: 4422.021\n",
      "741 - loss: 4421.875\n",
      "742 - loss: 4421.875\n",
      "743 - loss: 4421.873\n",
      "744 - loss: 4421.873\n",
      "745 - loss: 4421.873\n",
      "746 - loss: 4421.861\n",
      "747 - loss: 4421.847\n",
      "748 - loss: 4421.845\n",
      "749 - loss: 4421.843\n",
      "750 - loss: 4421.842\n",
      "751 - loss: 4421.842\n",
      "752 - loss: 4421.842\n",
      "753 - loss: 4421.841\n",
      "754 - loss: 4420.840\n",
      "755 - loss: 4420.055\n",
      "f_new: 4420.469 - f: 4420.055 - Backtracking...\n",
      "757 - loss: 4419.993\n",
      "758 - loss: 4419.989\n",
      "759 - loss: 4419.983\n",
      "760 - loss: 4419.956\n",
      "761 - loss: 4419.937\n",
      "762 - loss: 4419.890\n",
      "763 - loss: 4419.880\n",
      "764 - loss: 4419.874\n",
      "765 - loss: 4419.873\n",
      "766 - loss: 4419.797\n",
      "f_new: 4419.812 - f: 4419.797 - Backtracking...\n",
      "768 - loss: 4419.791\n",
      "769 - loss: 4419.787\n",
      "f_new: 4419.788 - f: 4419.787 - Backtracking...\n",
      "771 - loss: 4419.785\n",
      "772 - loss: 4419.784\n",
      "773 - loss: 4419.784\n",
      "774 - loss: 4419.756\n",
      "775 - loss: 4419.728\n",
      "776 - loss: 4419.725\n",
      "777 - loss: 4419.724\n",
      "778 - loss: 4419.724\n",
      "779 - loss: 4419.723\n",
      "780 - loss: 4419.723\n",
      "781 - loss: 4419.713\n",
      "782 - loss: 4419.701\n",
      "783 - loss: 4419.699\n",
      "784 - loss: 4419.697\n",
      "785 - loss: 4419.697\n",
      "786 - loss: 4419.697\n",
      "787 - loss: 4419.697\n",
      "788 - loss: 4418.609\n",
      "789 - loss: 4418.283\n",
      "f_new: 4419.368 - f: 4418.283 - Backtracking...\n",
      "791 - loss: 4417.996\n",
      "792 - loss: 4417.806\n",
      "793 - loss: 4417.744\n",
      "794 - loss: 4417.683\n",
      "795 - loss: 4417.422\n",
      "f_new: 4417.425 - f: 4417.422 - Backtracking...\n",
      "797 - loss: 4417.421\n",
      "798 - loss: 4417.420\n",
      "799 - loss: 4417.420\n",
      "800 - loss: 4417.419\n",
      "801 - loss: 4417.413\n",
      "802 - loss: 4417.404\n",
      "803 - loss: 4417.398\n",
      "804 - loss: 4417.397\n",
      "805 - loss: 4417.396\n",
      "806 - loss: 4417.396\n",
      "807 - loss: 4417.393\n",
      "808 - loss: 4417.389\n",
      "809 - loss: 4417.378\n",
      "810 - loss: 4417.346\n",
      "f_new: 4417.350 - f: 4417.346 - Backtracking...\n",
      "812 - loss: 4417.345\n",
      "813 - loss: 4417.345\n",
      "814 - loss: 4417.344\n",
      "815 - loss: 4417.342\n",
      "816 - loss: 4417.340\n",
      "817 - loss: 4417.337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "818 - loss: 4417.337\n",
      "819 - loss: 4417.337\n",
      "820 - loss: 4417.337\n",
      "821 - loss: 4417.236\n",
      "822 - loss: 4417.145\n",
      "f_new: 4417.199 - f: 4417.145 - Backtracking...\n",
      "824 - loss: 4417.145\n",
      "825 - loss: 4417.145\n",
      "826 - loss: 4417.145\n",
      "827 - loss: 4417.140\n",
      "828 - loss: 4417.134\n",
      "829 - loss: 4417.132\n",
      "830 - loss: 4417.130\n",
      "831 - loss: 4417.129\n",
      "832 - loss: 4417.128\n",
      "833 - loss: 4417.128\n",
      "834 - loss: 4417.128\n",
      "835 - loss: 4417.107\n",
      "836 - loss: 4417.087\n",
      "837 - loss: 4417.085\n",
      "838 - loss: 4417.085\n",
      "839 - loss: 4417.085\n",
      "840 - loss: 4417.084\n",
      "841 - loss: 4417.083\n",
      "842 - loss: 4417.081\n",
      "843 - loss: 4417.059\n",
      "844 - loss: 4416.973\n",
      "845 - loss: 4416.972\n",
      "846 - loss: 4416.972\n",
      "847 - loss: 4416.972\n",
      "848 - loss: 4416.971\n",
      "849 - loss: 4416.969\n",
      "850 - loss: 4416.967\n",
      "851 - loss: 4416.958\n",
      "852 - loss: 4416.943\n",
      "853 - loss: 4416.941\n",
      "854 - loss: 4416.941\n",
      "855 - loss: 4416.941\n",
      "856 - loss: 4416.941\n",
      "857 - loss: 4416.908\n",
      "858 - loss: 4416.869\n",
      "859 - loss: 4416.867\n",
      "860 - loss: 4416.866\n",
      "861 - loss: 4416.865\n",
      "862 - loss: 4416.865\n",
      "863 - loss: 4416.865\n",
      "864 - loss: 4416.724\n",
      "865 - loss: 4416.566\n",
      "866 - loss: 4416.555\n",
      "f_new: 4416.557 - f: 4416.555 - Backtracking...\n",
      "868 - loss: 4416.554\n",
      "869 - loss: 4416.554\n",
      "870 - loss: 4416.554\n",
      "871 - loss: 4416.552\n",
      "872 - loss: 4416.550\n",
      "873 - loss: 4416.540\n",
      "874 - loss: 4416.536\n",
      "875 - loss: 4416.536\n",
      "876 - loss: 4416.536\n",
      "877 - loss: 4416.536\n",
      "878 - loss: 4415.496\n",
      "879 - loss: 4415.033\n",
      "f_new: 4420.744 - f: 4415.033 - Backtracking...\n",
      "881 - loss: 4414.972\n",
      "882 - loss: 4414.933\n",
      "883 - loss: 4414.909\n",
      "884 - loss: 4414.745\n",
      "885 - loss: 4414.611\n",
      "f_new: 4414.659 - f: 4414.611 - Backtracking...\n",
      "887 - loss: 4414.592\n",
      "888 - loss: 4414.574\n",
      "889 - loss: 4414.569\n",
      "890 - loss: 4414.561\n",
      "891 - loss: 4414.383\n",
      "f_new: 4414.387 - f: 4414.383 - Backtracking...\n",
      "893 - loss: 4414.379\n",
      "f_new: 4414.385 - f: 4414.379 - Backtracking...\n",
      "895 - loss: 4414.379\n",
      "896 - loss: 4414.378\n",
      "897 - loss: 4414.378\n",
      "898 - loss: 4414.377\n",
      "899 - loss: 4414.375\n",
      "900 - loss: 4414.373\n",
      "901 - loss: 4414.373\n",
      "902 - loss: 4414.373\n",
      "903 - loss: 4414.373\n",
      "904 - loss: 4414.359\n",
      "905 - loss: 4414.345\n",
      "906 - loss: 4414.328\n",
      "907 - loss: 4414.327\n",
      "908 - loss: 4414.327\n",
      "909 - loss: 4414.327\n",
      "910 - loss: 4414.326\n",
      "911 - loss: 4414.324\n",
      "912 - loss: 4414.301\n",
      "913 - loss: 4414.275\n",
      "914 - loss: 4414.275\n",
      "915 - loss: 4414.275\n",
      "916 - loss: 4414.275\n",
      "917 - loss: 4414.268\n",
      "918 - loss: 4414.262\n",
      "919 - loss: 4414.260\n",
      "920 - loss: 4414.258\n",
      "921 - loss: 4414.256\n",
      "922 - loss: 4414.256\n",
      "923 - loss: 4414.256\n",
      "924 - loss: 4414.256\n",
      "925 - loss: 4414.037\n",
      "926 - loss: 4413.702\n",
      "f_new: 4413.762 - f: 4413.702 - Backtracking...\n",
      "928 - loss: 4413.701\n",
      "929 - loss: 4413.701\n",
      "930 - loss: 4413.701\n",
      "931 - loss: 4413.699\n",
      "932 - loss: 4413.696\n",
      "933 - loss: 4413.692\n",
      "934 - loss: 4413.692\n",
      "935 - loss: 4413.692\n",
      "936 - loss: 4413.692\n",
      "937 - loss: 4413.686\n",
      "938 - loss: 4413.679\n",
      "939 - loss: 4413.677\n",
      "940 - loss: 4413.676\n",
      "941 - loss: 4413.676\n",
      "942 - loss: 4413.675\n",
      "943 - loss: 4413.675\n",
      "944 - loss: 4413.657\n",
      "945 - loss: 4413.638\n",
      "946 - loss: 4413.633\n",
      "947 - loss: 4413.631\n",
      "948 - loss: 4413.631\n",
      "949 - loss: 4413.631\n",
      "950 - loss: 4413.631\n",
      "951 - loss: 4413.627\n",
      "952 - loss: 4413.622\n",
      "953 - loss: 4413.620\n",
      "954 - loss: 4413.618\n",
      "955 - loss: 4413.616\n",
      "956 - loss: 4413.615\n",
      "957 - loss: 4413.615\n",
      "958 - loss: 4413.615\n",
      "959 - loss: 4413.558\n",
      "960 - loss: 4413.493\n",
      "961 - loss: 4413.491\n",
      "962 - loss: 4413.491\n",
      "963 - loss: 4413.490\n",
      "964 - loss: 4413.490\n",
      "965 - loss: 4413.489\n",
      "966 - loss: 4413.487\n",
      "967 - loss: 4413.483\n",
      "968 - loss: 4413.475\n",
      "969 - loss: 4413.472\n",
      "970 - loss: 4413.472\n",
      "971 - loss: 4413.472\n",
      "972 - loss: 4413.472\n",
      "973 - loss: 4413.470\n",
      "974 - loss: 4413.468\n",
      "975 - loss: 4413.460\n",
      "976 - loss: 4413.416\n",
      "977 - loss: 4413.414\n",
      "978 - loss: 4413.414\n",
      "979 - loss: 4413.414\n",
      "980 - loss: 4413.414\n",
      "981 - loss: 4413.412\n",
      "982 - loss: 4413.410\n",
      "983 - loss: 4413.399\n",
      "984 - loss: 4413.258\n",
      "985 - loss: 4413.252\n",
      "f_new: 4413.252 - f: 4413.252 - Backtracking...\n",
      "987 - loss: 4413.252\n",
      "988 - loss: 4413.252\n",
      "989 - loss: 4413.251\n",
      "990 - loss: 4413.250\n",
      "991 - loss: 4413.249\n",
      "992 - loss: 4413.247\n",
      "993 - loss: 4413.247\n",
      "994 - loss: 4413.247\n",
      "995 - loss: 4413.247\n",
      "996 - loss: 4389.214\n",
      "f_new: 22935.718 - f: 4389.214 - Backtracking...\n",
      "f_new: 4389.214 - f: 4389.214 - Backtracking...\n",
      "999 - loss: 4389.213\n",
      "1000 - loss: 4389.213\n",
      "Reached maximum number of function evaluations 1000\n",
      "0.7283417326613871\n"
     ]
    }
   ],
   "source": [
    "logReg = LogisticRegression(verbose=True, max_evaluations=1000)\n",
    "logReg.fit(y_sub, np.c_[np.ones(X_sub.shape[0]), X_sub])\n",
    "pred = logReg.predict(np.c_[np.ones(X_sub.shape[0]), X_sub])\n",
    "print(compute_accuracy(pred, y_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_new: 388952141074152048493827070704946719003374518272.000 - f: 0.693 - Backtracking...\n",
      "f_new: 42948940344232256197308449618584789201601953792.000 - f: 0.693 - Backtracking...\n",
      "f_new: 4742515291465520316589241094131375336488697856.000 - f: 0.693 - Backtracking...\n",
      "f_new: 523678840723825601365942021904012656230006784.000 - f: 0.693 - Backtracking...\n",
      "f_new: 57825755188467729788181246775209813142929408.000 - f: 0.693 - Backtracking...\n",
      "f_new: 6385245503703762415557669590722242188148736.000 - f: 0.693 - Backtracking...\n",
      "f_new: 705072679287725974358492902368409655705600.000 - f: 0.693 - Backtracking...\n",
      "f_new: 77855656887368529251737837091182763573248.000 - f: 0.693 - Backtracking...\n",
      "f_new: 8596990760565376003751822352078876966912.000 - f: 0.693 - Backtracking...\n",
      "f_new: 949298395159228346089345013828158488576.000 - f: 0.693 - Backtracking...\n",
      "f_new: 104823590969245276918563331445266317312.000 - f: 0.693 - Backtracking...\n",
      "f_new: 11574848624751544723633017999918628864.000 - f: 0.693 - Backtracking...\n",
      "f_new: 1278119929369916136911791130408910848.000 - f: 0.693 - Backtracking...\n",
      "f_new: 141132779080955290663632729679069184.000 - f: 0.693 - Backtracking...\n",
      "f_new: 15584188051064248468229727084085248.000 - f: 0.693 - Backtracking...\n",
      "f_new: 1720839898374159823323440951066624.000 - f: 0.693 - Backtracking...\n",
      "f_new: 190018879785922593601210233651200.000 - f: 0.693 - Backtracking...\n",
      "f_new: 20982297486948518276197993414656.000 - f: 0.693 - Backtracking...\n",
      "f_new: 2316910868682124542541257768960.000 - f: 0.693 - Backtracking...\n",
      "f_new: 255838331181627255559634812928.000 - f: 0.693 - Backtracking...\n",
      "f_new: 28250224290686777644342575104.000 - f: 0.693 - Backtracking...\n",
      "f_new: 3119451134582065730238283776.000 - f: 0.693 - Backtracking...\n",
      "f_new: 344456570713080468250558464.000 - f: 0.693 - Backtracking...\n",
      "f_new: 38035642806538756811653120.000 - f: 0.693 - Backtracking...\n",
      "f_new: 4199978304120254887886848.000 - f: 0.693 - Backtracking...\n",
      "f_new: 463770728019571584794624.000 - f: 0.693 - Backtracking...\n",
      "f_new: 51210571244333019693056.000 - f: 0.693 - Backtracking...\n",
      "f_new: 5654782522324772978688.000 - f: 0.693 - Backtracking...\n",
      "f_new: 624413370087690076160.000 - f: 0.693 - Backtracking...\n",
      "f_new: 68949080748020621312.000 - f: 0.693 - Backtracking...\n",
      "f_new: 7613507275363817472.000 - f: 0.693 - Backtracking...\n",
      "f_new: 840700012286702080.000 - f: 0.693 - Backtracking...\n",
      "f_new: 92831921622493920.000 - f: 0.693 - Backtracking...\n",
      "f_new: 10250702445792234.000 - f: 0.693 - Backtracking...\n",
      "f_new: 1131904831825757.250 - f: 0.693 - Backtracking...\n",
      "f_new: 124987390384784.203 - f: 0.693 - Backtracking...\n",
      "f_new: 13801379158352.516 - f: 0.693 - Backtracking...\n",
      "f_new: 1523978267617.324 - f: 0.693 - Backtracking...\n",
      "f_new: 168280990872.210 - f: 0.693 - Backtracking...\n",
      "f_new: 18581952571.600 - f: 0.693 - Backtracking...\n",
      "f_new: 2051859568.882 - f: 0.693 - Backtracking...\n",
      "f_new: 226570790.942 - f: 0.693 - Backtracking...\n",
      "f_new: 25018438.979 - f: 0.693 - Backtracking...\n",
      "f_new: 2762590.440 - f: 0.693 - Backtracking...\n",
      "f_new: 305051.297 - f: 0.693 - Backtracking...\n",
      "f_new: 33684.487 - f: 0.693 - Backtracking...\n",
      "f_new: 3719.574 - f: 0.693 - Backtracking...\n",
      "f_new: 410.801 - f: 0.693 - Backtracking...\n",
      "f_new: 45.573 - f: 0.693 - Backtracking...\n",
      "f_new: 5.430 - f: 0.693 - Backtracking...\n",
      "f_new: 1.140 - f: 0.693 - Backtracking...\n",
      "f_new: 0.727 - f: 0.693 - Backtracking...\n",
      "f_new: 0.695 - f: 0.693 - Backtracking...\n",
      "54 - loss: 0.693\n",
      "55 - loss: 0.693\n",
      "56 - loss: 0.693\n",
      "57 - loss: 0.693\n",
      "58 - loss: 0.693\n",
      "59 - loss: 0.692\n",
      "f_new: 0.692 - f: 0.692 - Backtracking...\n",
      "61 - loss: 0.692\n",
      "62 - loss: 0.692\n",
      "63 - loss: 0.692\n",
      "64 - loss: 0.691\n",
      "65 - loss: 0.691\n",
      "f_new: 0.693 - f: 0.691 - Backtracking...\n",
      "67 - loss: 0.691\n",
      "68 - loss: 0.690\n",
      "69 - loss: 0.690\n",
      "70 - loss: 0.690\n",
      "f_new: 0.691 - f: 0.690 - Backtracking...\n",
      "72 - loss: 0.689\n",
      "73 - loss: 0.689\n",
      "74 - loss: 0.689\n",
      "75 - loss: 0.689\n",
      "76 - loss: 0.689\n",
      "77 - loss: 0.686\n",
      "f_new: 0.706 - f: 0.686 - Backtracking...\n",
      "79 - loss: 0.686\n",
      "80 - loss: 0.686\n",
      "81 - loss: 0.686\n",
      "82 - loss: 0.686\n",
      "83 - loss: 0.685\n",
      "f_new: 0.698 - f: 0.685 - Backtracking...\n",
      "85 - loss: 0.685\n",
      "86 - loss: 0.685\n",
      "87 - loss: 0.685\n",
      "88 - loss: 0.685\n",
      "89 - loss: 0.684\n",
      "f_new: 0.687 - f: 0.684 - Backtracking...\n",
      "91 - loss: 0.684\n",
      "92 - loss: 0.684\n",
      "93 - loss: 0.684\n",
      "94 - loss: 0.684\n",
      "95 - loss: 0.684\n",
      "96 - loss: 0.683\n",
      "f_new: 0.686 - f: 0.683 - Backtracking...\n",
      "98 - loss: 0.683\n",
      "99 - loss: 0.683\n",
      "100 - loss: 0.683\n",
      "101 - loss: 0.683\n",
      "102 - loss: 0.683\n",
      "103 - loss: 0.683\n",
      "104 - loss: 0.681\n",
      "f_new: 0.683 - f: 0.681 - Backtracking...\n",
      "106 - loss: 0.681\n",
      "107 - loss: 0.681\n",
      "108 - loss: 0.681\n",
      "109 - loss: 0.681\n",
      "110 - loss: 0.679\n",
      "f_new: 0.748 - f: 0.679 - Backtracking...\n",
      "112 - loss: 0.679\n",
      "113 - loss: 0.679\n",
      "114 - loss: 0.679\n",
      "115 - loss: 0.679\n",
      "116 - loss: 0.679\n",
      "f_new: 0.679 - f: 0.679 - Backtracking...\n",
      "118 - loss: 0.679\n",
      "119 - loss: 0.679\n",
      "120 - loss: 0.679\n",
      "121 - loss: 0.679\n",
      "122 - loss: 0.679\n",
      "123 - loss: 0.678\n",
      "f_new: 0.684 - f: 0.678 - Backtracking...\n",
      "125 - loss: 0.678\n",
      "126 - loss: 0.678\n",
      "127 - loss: 0.678\n",
      "128 - loss: 0.678\n",
      "129 - loss: 0.678\n",
      "130 - loss: 0.678\n",
      "f_new: 0.678 - f: 0.678 - Backtracking...\n",
      "132 - loss: 0.678\n",
      "133 - loss: 0.678\n",
      "134 - loss: 0.678\n",
      "135 - loss: 0.678\n",
      "136 - loss: 0.678\n",
      "137 - loss: 0.677\n",
      "f_new: 0.678 - f: 0.677 - Backtracking...\n",
      "139 - loss: 0.677\n",
      "140 - loss: 0.677\n",
      "141 - loss: 0.677\n",
      "142 - loss: 0.677\n",
      "143 - loss: 0.677\n",
      "f_new: 0.677 - f: 0.677 - Backtracking...\n",
      "145 - loss: 0.677\n",
      "146 - loss: 0.677\n",
      "147 - loss: 0.677\n",
      "148 - loss: 0.677\n",
      "149 - loss: 0.677\n",
      "150 - loss: 0.677\n",
      "f_new: 0.677 - f: 0.677 - Backtracking...\n",
      "152 - loss: 0.677\n",
      "153 - loss: 0.676\n",
      "154 - loss: 0.676\n",
      "155 - loss: 0.676\n",
      "156 - loss: 0.676\n",
      "157 - loss: 0.676\n",
      "158 - loss: 0.676\n",
      "159 - loss: 0.676\n",
      "160 - loss: 0.676\n",
      "161 - loss: 0.676\n",
      "162 - loss: 0.676\n",
      "163 - loss: 0.675\n",
      "f_new: 0.683 - f: 0.675 - Backtracking...\n",
      "165 - loss: 0.675\n",
      "166 - loss: 0.675\n",
      "167 - loss: 0.675\n",
      "168 - loss: 0.675\n",
      "169 - loss: 0.675\n",
      "170 - loss: 0.666\n",
      "f_new: 0.886 - f: 0.666 - Backtracking...\n",
      "f_new: 0.666 - f: 0.666 - Backtracking...\n",
      "173 - loss: 0.666\n",
      "174 - loss: 0.666\n",
      "175 - loss: 0.666\n",
      "176 - loss: 0.666\n",
      "177 - loss: 0.666\n",
      "f_new: 0.667 - f: 0.666 - Backtracking...\n",
      "179 - loss: 0.666\n",
      "180 - loss: 0.666\n",
      "181 - loss: 0.666\n",
      "182 - loss: 0.666\n",
      "f_new: 0.666 - f: 0.666 - Backtracking...\n",
      "184 - loss: 0.666\n",
      "185 - loss: 0.666\n",
      "186 - loss: 0.666\n",
      "187 - loss: 0.666\n",
      "188 - loss: 0.666\n",
      "189 - loss: 0.666\n",
      "190 - loss: 0.666\n",
      "191 - loss: 0.666\n",
      "192 - loss: 0.666\n",
      "193 - loss: 0.666\n",
      "194 - loss: 0.665\n",
      "195 - loss: 0.665\n",
      "196 - loss: 0.665\n",
      "197 - loss: 0.665\n",
      "198 - loss: 0.665\n",
      "199 - loss: 0.665\n",
      "200 - loss: 0.664\n",
      "f_new: 0.721 - f: 0.664 - Backtracking...\n",
      "202 - loss: 0.664\n",
      "203 - loss: 0.664\n",
      "204 - loss: 0.664\n",
      "205 - loss: 0.664\n",
      "206 - loss: 0.664\n",
      "207 - loss: 0.664\n",
      "208 - loss: 0.664\n",
      "209 - loss: 0.664\n",
      "210 - loss: 0.664\n",
      "211 - loss: 0.664\n",
      "212 - loss: 0.664\n",
      "213 - loss: 0.664\n",
      "214 - loss: 0.664\n",
      "215 - loss: 0.664\n",
      "216 - loss: 0.664\n",
      "217 - loss: 0.664\n",
      "218 - loss: 0.663\n",
      "f_new: 0.682 - f: 0.663 - Backtracking...\n",
      "220 - loss: 0.663\n",
      "221 - loss: 0.663\n",
      "222 - loss: 0.663\n",
      "223 - loss: 0.663\n",
      "224 - loss: 0.663\n",
      "f_new: 0.663 - f: 0.663 - Backtracking...\n",
      "226 - loss: 0.663\n",
      "227 - loss: 0.663\n",
      "228 - loss: 0.663\n",
      "229 - loss: 0.663\n",
      "230 - loss: 0.662\n",
      "f_new: 0.663 - f: 0.662 - Backtracking...\n",
      "232 - loss: 0.662\n",
      "233 - loss: 0.662\n",
      "234 - loss: 0.662\n",
      "235 - loss: 0.662\n",
      "236 - loss: 0.662\n",
      "237 - loss: 0.662\n",
      "f_new: 0.663 - f: 0.662 - Backtracking...\n",
      "239 - loss: 0.662\n",
      "240 - loss: 0.662\n",
      "241 - loss: 0.662\n",
      "242 - loss: 0.662\n",
      "243 - loss: 0.662\n",
      "244 - loss: 0.662\n",
      "245 - loss: 0.662\n",
      "246 - loss: 0.662\n",
      "247 - loss: 0.662\n",
      "248 - loss: 0.662\n",
      "249 - loss: 0.662\n",
      "250 - loss: 0.662\n",
      "251 - loss: 0.662\n",
      "252 - loss: 0.662\n",
      "253 - loss: 0.662\n",
      "254 - loss: 0.662\n",
      "255 - loss: 0.662\n",
      "256 - loss: 0.662\n",
      "f_new: 0.662 - f: 0.662 - Backtracking...\n",
      "258 - loss: 0.662\n",
      "259 - loss: 0.662\n",
      "260 - loss: 0.662\n",
      "261 - loss: 0.662\n",
      "262 - loss: 0.662\n",
      "263 - loss: 0.662\n",
      "264 - loss: 0.662\n",
      "265 - loss: 0.662\n",
      "266 - loss: 0.662\n",
      "267 - loss: 0.662\n",
      "268 - loss: 0.661\n",
      "f_new: 0.674 - f: 0.661 - Backtracking...\n",
      "270 - loss: 0.661\n",
      "271 - loss: 0.661\n",
      "272 - loss: 0.661\n",
      "273 - loss: 0.661\n",
      "274 - loss: 0.661\n",
      "275 - loss: 0.661\n",
      "276 - loss: 0.661\n",
      "277 - loss: 0.661\n",
      "278 - loss: 0.661\n",
      "279 - loss: 0.661\n",
      "280 - loss: 0.661\n",
      "281 - loss: 0.661\n",
      "282 - loss: 0.661\n",
      "f_new: 0.661 - f: 0.661 - Backtracking...\n",
      "284 - loss: 0.661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 - loss: 0.661\n",
      "286 - loss: 0.661\n",
      "287 - loss: 0.661\n",
      "288 - loss: 0.661\n",
      "f_new: 0.661 - f: 0.661 - Backtracking...\n",
      "290 - loss: 0.661\n",
      "291 - loss: 0.661\n",
      "292 - loss: 0.661\n",
      "293 - loss: 0.661\n",
      "294 - loss: 0.661\n",
      "f_new: 0.661 - f: 0.661 - Backtracking...\n",
      "296 - loss: 0.661\n",
      "297 - loss: 0.661\n",
      "298 - loss: 0.661\n",
      "299 - loss: 0.661\n",
      "300 - loss: 0.660\n",
      "301 - loss: 0.660\n",
      "302 - loss: 0.660\n",
      "303 - loss: 0.660\n",
      "304 - loss: 0.660\n",
      "305 - loss: 0.660\n",
      "306 - loss: 0.660\n",
      "307 - loss: 0.660\n",
      "308 - loss: 0.660\n",
      "309 - loss: 0.660\n",
      "310 - loss: 0.660\n",
      "311 - loss: 0.660\n",
      "312 - loss: 0.660\n",
      "313 - loss: 0.660\n",
      "314 - loss: 0.660\n",
      "315 - loss: 0.660\n",
      "316 - loss: 0.660\n",
      "317 - loss: 0.660\n",
      "318 - loss: 0.660\n",
      "319 - loss: 0.660\n",
      "320 - loss: 0.660\n",
      "321 - loss: 0.660\n",
      "322 - loss: 0.660\n",
      "323 - loss: 0.660\n",
      "324 - loss: 0.660\n",
      "f_new: 0.660 - f: 0.660 - Backtracking...\n",
      "326 - loss: 0.660\n",
      "327 - loss: 0.660\n",
      "328 - loss: 0.660\n",
      "329 - loss: 0.660\n",
      "330 - loss: 0.660\n",
      "331 - loss: 0.660\n",
      "332 - loss: 0.660\n",
      "333 - loss: 0.660\n",
      "334 - loss: 0.660\n",
      "335 - loss: 0.660\n",
      "336 - loss: 0.660\n",
      "f_new: 0.661 - f: 0.660 - Backtracking...\n",
      "338 - loss: 0.660\n",
      "339 - loss: 0.660\n",
      "340 - loss: 0.660\n",
      "341 - loss: 0.660\n",
      "342 - loss: 0.660\n",
      "343 - loss: 0.660\n",
      "344 - loss: 0.660\n",
      "f_new: 0.660 - f: 0.660 - Backtracking...\n",
      "346 - loss: 0.660\n",
      "347 - loss: 0.660\n",
      "348 - loss: 0.660\n",
      "349 - loss: 0.660\n",
      "350 - loss: 0.660\n",
      "351 - loss: 0.660\n",
      "352 - loss: 0.660\n",
      "353 - loss: 0.660\n",
      "354 - loss: 0.660\n",
      "355 - loss: 0.660\n",
      "356 - loss: 0.658\n",
      "f_new: 0.806 - f: 0.658 - Backtracking...\n",
      "f_new: 0.658 - f: 0.658 - Backtracking...\n",
      "359 - loss: 0.658\n",
      "360 - loss: 0.658\n",
      "361 - loss: 0.658\n",
      "362 - loss: 0.658\n",
      "363 - loss: 0.658\n",
      "f_new: 0.658 - f: 0.658 - Backtracking...\n",
      "365 - loss: 0.658\n",
      "366 - loss: 0.658\n",
      "367 - loss: 0.658\n",
      "368 - loss: 0.658\n",
      "369 - loss: 0.658\n",
      "370 - loss: 0.658\n",
      "371 - loss: 0.658\n",
      "372 - loss: 0.658\n",
      "373 - loss: 0.658\n",
      "374 - loss: 0.658\n",
      "375 - loss: 0.658\n",
      "376 - loss: 0.658\n",
      "377 - loss: 0.658\n",
      "378 - loss: 0.658\n",
      "379 - loss: 0.658\n",
      "380 - loss: 0.658\n",
      "381 - loss: 0.658\n",
      "382 - loss: 0.656\n",
      "f_new: 1.061 - f: 0.656 - Backtracking...\n",
      "f_new: 0.656 - f: 0.656 - Backtracking...\n",
      "385 - loss: 0.656\n",
      "386 - loss: 0.656\n",
      "387 - loss: 0.656\n",
      "388 - loss: 0.656\n",
      "389 - loss: 0.656\n",
      "390 - loss: 0.656\n",
      "391 - loss: 0.656\n",
      "392 - loss: 0.656\n",
      "393 - loss: 0.656\n",
      "394 - loss: 0.656\n",
      "395 - loss: 0.656\n",
      "396 - loss: 0.656\n",
      "397 - loss: 0.656\n",
      "398 - loss: 0.656\n",
      "399 - loss: 0.656\n",
      "400 - loss: 0.656\n",
      "401 - loss: 0.656\n",
      "402 - loss: 0.656\n",
      "403 - loss: 0.656\n",
      "404 - loss: 0.656\n",
      "405 - loss: 0.656\n",
      "406 - loss: 0.656\n",
      "f_new: 0.662 - f: 0.656 - Backtracking...\n",
      "408 - loss: 0.656\n",
      "409 - loss: 0.656\n",
      "410 - loss: 0.656\n",
      "411 - loss: 0.656\n",
      "412 - loss: 0.656\n",
      "413 - loss: 0.656\n",
      "f_new: 0.656 - f: 0.656 - Backtracking...\n",
      "415 - loss: 0.656\n",
      "416 - loss: 0.656\n",
      "417 - loss: 0.656\n",
      "418 - loss: 0.656\n",
      "419 - loss: 0.656\n",
      "420 - loss: 0.656\n",
      "421 - loss: 0.656\n",
      "422 - loss: 0.656\n",
      "423 - loss: 0.656\n",
      "424 - loss: 0.656\n",
      "425 - loss: 0.656\n",
      "426 - loss: 0.656\n",
      "427 - loss: 0.656\n",
      "428 - loss: 0.656\n",
      "429 - loss: 0.656\n",
      "430 - loss: 0.656\n",
      "431 - loss: 0.654\n",
      "f_new: 1.052 - f: 0.654 - Backtracking...\n",
      "f_new: 0.654 - f: 0.654 - Backtracking...\n",
      "434 - loss: 0.654\n",
      "435 - loss: 0.654\n",
      "436 - loss: 0.654\n",
      "437 - loss: 0.654\n",
      "438 - loss: 0.654\n",
      "439 - loss: 0.654\n",
      "440 - loss: 0.654\n",
      "441 - loss: 0.654\n",
      "442 - loss: 0.654\n",
      "443 - loss: 0.654\n",
      "444 - loss: 0.654\n",
      "445 - loss: 0.654\n",
      "446 - loss: 0.654\n",
      "447 - loss: 0.654\n",
      "448 - loss: 0.654\n",
      "449 - loss: 0.654\n",
      "450 - loss: 0.654\n",
      "451 - loss: 0.654\n",
      "452 - loss: 0.654\n",
      "453 - loss: 0.654\n",
      "454 - loss: 0.654\n",
      "455 - loss: 0.654\n",
      "456 - loss: 0.654\n",
      "f_new: 0.654 - f: 0.654 - Backtracking...\n",
      "458 - loss: 0.654\n",
      "459 - loss: 0.654\n",
      "460 - loss: 0.654\n",
      "461 - loss: 0.654\n",
      "462 - loss: 0.654\n",
      "463 - loss: 0.654\n",
      "f_new: 0.654 - f: 0.654 - Backtracking...\n",
      "465 - loss: 0.654\n",
      "466 - loss: 0.654\n",
      "467 - loss: 0.654\n",
      "468 - loss: 0.654\n",
      "469 - loss: 0.653\n",
      "470 - loss: 0.653\n",
      "471 - loss: 0.653\n",
      "472 - loss: 0.653\n",
      "473 - loss: 0.653\n",
      "474 - loss: 0.653\n",
      "475 - loss: 0.653\n",
      "476 - loss: 0.653\n",
      "477 - loss: 0.653\n",
      "478 - loss: 0.653\n",
      "479 - loss: 0.653\n",
      "480 - loss: 0.653\n",
      "481 - loss: 0.653\n",
      "482 - loss: 0.653\n",
      "483 - loss: 0.653\n",
      "484 - loss: 0.653\n",
      "485 - loss: 0.653\n",
      "486 - loss: 0.653\n",
      "487 - loss: 0.653\n",
      "488 - loss: 0.653\n",
      "489 - loss: 0.653\n",
      "490 - loss: 0.653\n",
      "491 - loss: 0.653\n",
      "492 - loss: 0.653\n",
      "493 - loss: 0.653\n",
      "494 - loss: 0.653\n",
      "495 - loss: 0.653\n",
      "496 - loss: 0.653\n",
      "497 - loss: 0.653\n",
      "498 - loss: 0.653\n",
      "499 - loss: 0.653\n",
      "500 - loss: 0.653\n",
      "501 - loss: 0.653\n",
      "502 - loss: 0.653\n",
      "503 - loss: 0.653\n",
      "504 - loss: 0.653\n",
      "505 - loss: 0.653\n",
      "506 - loss: 0.653\n",
      "507 - loss: 0.653\n",
      "508 - loss: 0.653\n",
      "509 - loss: 0.653\n",
      "510 - loss: 0.653\n",
      "511 - loss: 0.653\n",
      "512 - loss: 0.653\n",
      "513 - loss: 0.653\n",
      "514 - loss: 0.653\n",
      "515 - loss: 0.653\n",
      "516 - loss: 0.653\n",
      "517 - loss: 0.653\n",
      "518 - loss: 0.653\n",
      "519 - loss: 0.653\n",
      "520 - loss: 0.653\n",
      "521 - loss: 0.653\n",
      "522 - loss: 0.653\n",
      "523 - loss: 0.653\n",
      "f_new: 0.653 - f: 0.653 - Backtracking...\n",
      "525 - loss: 0.653\n",
      "526 - loss: 0.653\n",
      "527 - loss: 0.653\n",
      "528 - loss: 0.653\n",
      "529 - loss: 0.653\n",
      "530 - loss: 0.653\n",
      "531 - loss: 0.653\n",
      "532 - loss: 0.653\n",
      "533 - loss: 0.653\n",
      "534 - loss: 0.653\n",
      "535 - loss: 0.653\n",
      "536 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "538 - loss: 0.652\n",
      "539 - loss: 0.652\n",
      "540 - loss: 0.652\n",
      "541 - loss: 0.652\n",
      "542 - loss: 0.652\n",
      "543 - loss: 0.652\n",
      "544 - loss: 0.652\n",
      "545 - loss: 0.652\n",
      "546 - loss: 0.652\n",
      "547 - loss: 0.652\n",
      "548 - loss: 0.652\n",
      "549 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "551 - loss: 0.652\n",
      "552 - loss: 0.652\n",
      "553 - loss: 0.652\n",
      "554 - loss: 0.652\n",
      "555 - loss: 0.652\n",
      "556 - loss: 0.652\n",
      "557 - loss: 0.652\n",
      "558 - loss: 0.652\n",
      "559 - loss: 0.652\n",
      "560 - loss: 0.652\n",
      "561 - loss: 0.652\n",
      "562 - loss: 0.652\n",
      "563 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "565 - loss: 0.652\n",
      "566 - loss: 0.652\n",
      "567 - loss: 0.652\n",
      "568 - loss: 0.652\n",
      "569 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "571 - loss: 0.652\n",
      "572 - loss: 0.652\n",
      "573 - loss: 0.652\n",
      "574 - loss: 0.652\n",
      "575 - loss: 0.652\n",
      "576 - loss: 0.652\n",
      "577 - loss: 0.652\n",
      "578 - loss: 0.652\n",
      "579 - loss: 0.652\n",
      "580 - loss: 0.652\n",
      "581 - loss: 0.652\n",
      "582 - loss: 0.652\n",
      "f_new: 0.652 - f: 0.652 - Backtracking...\n",
      "584 - loss: 0.652\n",
      "585 - loss: 0.652\n",
      "586 - loss: 0.652\n",
      "587 - loss: 0.652\n",
      "588 - loss: 0.652\n",
      "589 - loss: 0.652\n",
      "590 - loss: 0.652\n",
      "591 - loss: 0.652\n",
      "592 - loss: 0.652\n",
      "593 - loss: 0.652\n",
      "594 - loss: 0.651\n",
      "f_new: 0.672 - f: 0.651 - Backtracking...\n",
      "596 - loss: 0.651\n",
      "597 - loss: 0.651\n",
      "598 - loss: 0.651\n",
      "599 - loss: 0.651\n",
      "600 - loss: 0.651\n",
      "601 - loss: 0.651\n",
      "f_new: 0.651 - f: 0.651 - Backtracking...\n",
      "603 - loss: 0.651\n",
      "604 - loss: 0.651\n",
      "605 - loss: 0.651\n",
      "606 - loss: 0.651\n",
      "607 - loss: 0.651\n",
      "608 - loss: 0.651\n",
      "f_new: 0.651 - f: 0.651 - Backtracking...\n",
      "610 - loss: 0.651\n",
      "611 - loss: 0.651\n",
      "612 - loss: 0.651\n",
      "613 - loss: 0.651\n",
      "614 - loss: 0.651\n",
      "f_new: 0.651 - f: 0.651 - Backtracking...\n",
      "616 - loss: 0.651\n",
      "617 - loss: 0.651\n",
      "618 - loss: 0.651\n",
      "619 - loss: 0.651\n",
      "620 - loss: 0.651\n",
      "621 - loss: 0.651\n",
      "622 - loss: 0.651\n",
      "623 - loss: 0.651\n",
      "624 - loss: 0.651\n",
      "625 - loss: 0.651\n",
      "626 - loss: 0.651\n",
      "627 - loss: 0.651\n",
      "628 - loss: 0.651\n",
      "629 - loss: 0.651\n",
      "630 - loss: 0.651\n",
      "631 - loss: 0.651\n",
      "632 - loss: 0.651\n",
      "633 - loss: 0.651\n",
      "634 - loss: 0.651\n",
      "635 - loss: 0.651\n",
      "636 - loss: 0.651\n",
      "637 - loss: 0.650\n",
      "f_new: 0.738 - f: 0.650 - Backtracking...\n",
      "f_new: 0.650 - f: 0.650 - Backtracking...\n",
      "640 - loss: 0.650\n",
      "641 - loss: 0.650\n",
      "642 - loss: 0.650\n",
      "643 - loss: 0.650\n",
      "644 - loss: 0.650\n",
      "645 - loss: 0.650\n",
      "646 - loss: 0.650\n",
      "647 - loss: 0.650\n",
      "648 - loss: 0.650\n",
      "649 - loss: 0.650\n",
      "650 - loss: 0.650\n",
      "f_new: 0.650 - f: 0.650 - Backtracking...\n",
      "652 - loss: 0.650\n",
      "653 - loss: 0.650\n",
      "654 - loss: 0.650\n",
      "655 - loss: 0.650\n",
      "656 - loss: 0.650\n",
      "657 - loss: 0.650\n",
      "658 - loss: 0.650\n",
      "f_new: 0.650 - f: 0.650 - Backtracking...\n",
      "660 - loss: 0.650\n",
      "661 - loss: 0.650\n",
      "662 - loss: 0.650\n",
      "663 - loss: 0.650\n",
      "664 - loss: 0.650\n",
      "665 - loss: 0.650\n",
      "666 - loss: 0.650\n",
      "667 - loss: 0.650\n",
      "668 - loss: 0.650\n",
      "669 - loss: 0.650\n",
      "670 - loss: 0.649\n",
      "f_new: 0.651 - f: 0.649 - Backtracking...\n",
      "672 - loss: 0.649\n",
      "673 - loss: 0.649\n",
      "674 - loss: 0.649\n",
      "675 - loss: 0.649\n",
      "676 - loss: 0.649\n",
      "677 - loss: 0.649\n",
      "f_new: 0.649 - f: 0.649 - Backtracking...\n",
      "679 - loss: 0.649\n",
      "680 - loss: 0.649\n",
      "681 - loss: 0.649\n",
      "682 - loss: 0.649\n",
      "683 - loss: 0.649\n",
      "684 - loss: 0.649\n",
      "685 - loss: 0.649\n",
      "686 - loss: 0.649\n",
      "687 - loss: 0.649\n",
      "688 - loss: 0.649\n",
      "689 - loss: 0.649\n",
      "690 - loss: 0.649\n",
      "691 - loss: 0.649\n",
      "692 - loss: 0.649\n",
      "693 - loss: 0.649\n",
      "694 - loss: 0.649\n",
      "695 - loss: 0.649\n",
      "f_new: 0.656 - f: 0.649 - Backtracking...\n",
      "697 - loss: 0.649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698 - loss: 0.649\n",
      "699 - loss: 0.649\n",
      "700 - loss: 0.649\n",
      "701 - loss: 0.649\n",
      "f_new: 0.649 - f: 0.649 - Backtracking...\n",
      "703 - loss: 0.649\n",
      "704 - loss: 0.649\n",
      "705 - loss: 0.649\n",
      "706 - loss: 0.649\n",
      "707 - loss: 0.649\n",
      "708 - loss: 0.649\n",
      "709 - loss: 0.649\n",
      "710 - loss: 0.649\n",
      "711 - loss: 0.649\n",
      "712 - loss: 0.649\n",
      "713 - loss: 0.649\n",
      "714 - loss: 0.649\n",
      "715 - loss: 0.649\n",
      "716 - loss: 0.649\n",
      "717 - loss: 0.649\n",
      "718 - loss: 0.649\n",
      "719 - loss: 0.649\n",
      "f_new: 0.649 - f: 0.649 - Backtracking...\n",
      "721 - loss: 0.649\n",
      "722 - loss: 0.649\n",
      "723 - loss: 0.649\n",
      "724 - loss: 0.649\n",
      "725 - loss: 0.649\n",
      "726 - loss: 0.649\n",
      "727 - loss: 0.649\n",
      "728 - loss: 0.649\n",
      "729 - loss: 0.649\n",
      "730 - loss: 0.649\n",
      "731 - loss: 0.649\n",
      "732 - loss: 0.647\n",
      "f_new: 0.663 - f: 0.647 - Backtracking...\n",
      "734 - loss: 0.647\n",
      "735 - loss: 0.647\n",
      "736 - loss: 0.647\n",
      "737 - loss: 0.647\n",
      "738 - loss: 0.647\n",
      "739 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "741 - loss: 0.647\n",
      "742 - loss: 0.647\n",
      "743 - loss: 0.647\n",
      "744 - loss: 0.647\n",
      "745 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "747 - loss: 0.647\n",
      "748 - loss: 0.647\n",
      "749 - loss: 0.647\n",
      "750 - loss: 0.647\n",
      "751 - loss: 0.647\n",
      "752 - loss: 0.647\n",
      "753 - loss: 0.647\n",
      "754 - loss: 0.647\n",
      "755 - loss: 0.647\n",
      "756 - loss: 0.647\n",
      "757 - loss: 0.647\n",
      "758 - loss: 0.647\n",
      "759 - loss: 0.647\n",
      "760 - loss: 0.647\n",
      "761 - loss: 0.647\n",
      "762 - loss: 0.647\n",
      "763 - loss: 0.647\n",
      "764 - loss: 0.647\n",
      "765 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "767 - loss: 0.647\n",
      "768 - loss: 0.647\n",
      "769 - loss: 0.647\n",
      "770 - loss: 0.647\n",
      "771 - loss: 0.647\n",
      "772 - loss: 0.647\n",
      "773 - loss: 0.647\n",
      "774 - loss: 0.647\n",
      "775 - loss: 0.647\n",
      "776 - loss: 0.647\n",
      "777 - loss: 0.647\n",
      "778 - loss: 0.647\n",
      "779 - loss: 0.647\n",
      "780 - loss: 0.647\n",
      "781 - loss: 0.647\n",
      "782 - loss: 0.647\n",
      "783 - loss: 0.647\n",
      "784 - loss: 0.647\n",
      "785 - loss: 0.647\n",
      "786 - loss: 0.647\n",
      "787 - loss: 0.647\n",
      "788 - loss: 0.647\n",
      "789 - loss: 0.647\n",
      "790 - loss: 0.647\n",
      "791 - loss: 0.647\n",
      "792 - loss: 0.647\n",
      "793 - loss: 0.647\n",
      "794 - loss: 0.647\n",
      "795 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "797 - loss: 0.647\n",
      "798 - loss: 0.647\n",
      "799 - loss: 0.647\n",
      "800 - loss: 0.647\n",
      "801 - loss: 0.647\n",
      "802 - loss: 0.647\n",
      "803 - loss: 0.647\n",
      "804 - loss: 0.647\n",
      "805 - loss: 0.647\n",
      "806 - loss: 0.647\n",
      "807 - loss: 0.647\n",
      "808 - loss: 0.647\n",
      "809 - loss: 0.647\n",
      "810 - loss: 0.647\n",
      "811 - loss: 0.647\n",
      "812 - loss: 0.647\n",
      "813 - loss: 0.647\n",
      "814 - loss: 0.647\n",
      "f_new: 0.647 - f: 0.647 - Backtracking...\n",
      "816 - loss: 0.647\n",
      "817 - loss: 0.647\n",
      "818 - loss: 0.647\n",
      "819 - loss: 0.647\n",
      "820 - loss: 0.647\n",
      "821 - loss: 0.647\n",
      "822 - loss: 0.647\n",
      "823 - loss: 0.647\n",
      "824 - loss: 0.647\n",
      "825 - loss: 0.647\n",
      "826 - loss: 0.647\n",
      "827 - loss: 0.647\n",
      "828 - loss: 0.647\n",
      "829 - loss: 0.647\n",
      "830 - loss: 0.647\n",
      "831 - loss: 0.647\n",
      "832 - loss: 0.646\n",
      "f_new: 0.647 - f: 0.646 - Backtracking...\n",
      "834 - loss: 0.646\n",
      "835 - loss: 0.646\n",
      "836 - loss: 0.646\n",
      "837 - loss: 0.646\n",
      "838 - loss: 0.646\n",
      "839 - loss: 0.646\n",
      "840 - loss: 0.646\n",
      "841 - loss: 0.646\n",
      "842 - loss: 0.646\n",
      "843 - loss: 0.646\n",
      "844 - loss: 0.646\n",
      "f_new: 0.646 - f: 0.646 - Backtracking...\n",
      "846 - loss: 0.646\n",
      "847 - loss: 0.646\n",
      "848 - loss: 0.646\n",
      "849 - loss: 0.646\n",
      "850 - loss: 0.646\n",
      "851 - loss: 0.646\n",
      "852 - loss: 0.646\n",
      "853 - loss: 0.646\n",
      "854 - loss: 0.646\n",
      "855 - loss: 0.646\n",
      "856 - loss: 0.646\n",
      "857 - loss: 0.646\n",
      "858 - loss: 0.646\n",
      "859 - loss: 0.646\n",
      "860 - loss: 0.646\n",
      "861 - loss: 0.646\n",
      "862 - loss: 0.646\n",
      "f_new: 0.646 - f: 0.646 - Backtracking...\n",
      "864 - loss: 0.646\n",
      "865 - loss: 0.646\n",
      "866 - loss: 0.646\n",
      "867 - loss: 0.646\n",
      "868 - loss: 0.646\n",
      "869 - loss: 0.646\n",
      "870 - loss: 0.646\n",
      "871 - loss: 0.646\n",
      "872 - loss: 0.646\n",
      "873 - loss: 0.646\n",
      "874 - loss: 0.646\n",
      "875 - loss: 0.646\n",
      "876 - loss: 0.646\n",
      "877 - loss: 0.646\n",
      "878 - loss: 0.646\n",
      "879 - loss: 0.646\n",
      "880 - loss: 0.646\n",
      "881 - loss: 0.646\n",
      "882 - loss: 0.646\n",
      "883 - loss: 0.646\n",
      "884 - loss: 0.646\n",
      "885 - loss: 0.646\n",
      "886 - loss: 0.646\n",
      "887 - loss: 0.646\n",
      "888 - loss: 0.646\n",
      "889 - loss: 0.646\n",
      "890 - loss: 0.646\n",
      "891 - loss: 0.646\n",
      "892 - loss: 0.646\n",
      "893 - loss: 0.646\n",
      "894 - loss: 0.646\n",
      "895 - loss: 0.646\n",
      "896 - loss: 0.646\n",
      "897 - loss: 0.646\n",
      "898 - loss: 0.646\n",
      "899 - loss: 0.646\n",
      "900 - loss: 0.646\n",
      "901 - loss: 0.646\n",
      "902 - loss: 0.646\n",
      "903 - loss: 0.646\n",
      "904 - loss: 0.646\n",
      "905 - loss: 0.646\n",
      "906 - loss: 0.646\n",
      "907 - loss: 0.646\n",
      "908 - loss: 0.646\n",
      "909 - loss: 0.646\n",
      "910 - loss: 0.646\n",
      "911 - loss: 0.646\n",
      "912 - loss: 0.646\n",
      "913 - loss: 0.646\n",
      "914 - loss: 0.646\n",
      "915 - loss: 0.646\n",
      "916 - loss: 0.646\n",
      "917 - loss: 0.646\n",
      "918 - loss: 0.646\n",
      "919 - loss: 0.646\n",
      "920 - loss: 0.646\n",
      "921 - loss: 0.646\n",
      "922 - loss: 0.646\n",
      "923 - loss: 0.646\n",
      "924 - loss: 0.646\n",
      "925 - loss: 0.646\n",
      "926 - loss: 0.646\n",
      "927 - loss: 0.646\n",
      "928 - loss: 0.646\n",
      "f_new: 0.674 - f: 0.646 - Backtracking...\n",
      "f_new: 0.646 - f: 0.646 - Backtracking...\n",
      "931 - loss: 0.646\n",
      "932 - loss: 0.646\n",
      "933 - loss: 0.646\n",
      "934 - loss: 0.646\n",
      "935 - loss: 0.646\n",
      "936 - loss: 0.646\n",
      "937 - loss: 0.646\n",
      "938 - loss: 0.646\n",
      "939 - loss: 0.646\n",
      "940 - loss: 0.646\n",
      "941 - loss: 0.646\n",
      "942 - loss: 0.646\n",
      "943 - loss: 0.646\n",
      "944 - loss: 0.646\n",
      "945 - loss: 0.646\n",
      "946 - loss: 0.646\n",
      "947 - loss: 0.646\n",
      "948 - loss: 0.646\n",
      "949 - loss: 0.646\n",
      "950 - loss: 0.646\n",
      "951 - loss: 0.646\n",
      "952 - loss: 0.646\n",
      "953 - loss: 0.643\n",
      "f_new: 0.748 - f: 0.643 - Backtracking...\n",
      "f_new: 0.643 - f: 0.643 - Backtracking...\n",
      "956 - loss: 0.643\n",
      "957 - loss: 0.643\n",
      "958 - loss: 0.643\n",
      "959 - loss: 0.643\n",
      "960 - loss: 0.642\n",
      "f_new: 0.649 - f: 0.642 - Backtracking...\n",
      "962 - loss: 0.642\n",
      "963 - loss: 0.642\n",
      "964 - loss: 0.642\n",
      "965 - loss: 0.642\n",
      "966 - loss: 0.641\n",
      "f_new: 0.645 - f: 0.641 - Backtracking...\n",
      "968 - loss: 0.641\n",
      "969 - loss: 0.641\n",
      "970 - loss: 0.641\n",
      "971 - loss: 0.641\n",
      "f_new: 0.642 - f: 0.641 - Backtracking...\n",
      "973 - loss: 0.641\n",
      "974 - loss: 0.641\n",
      "975 - loss: 0.641\n",
      "976 - loss: 0.641\n",
      "977 - loss: 0.641\n",
      "f_new: 0.641 - f: 0.641 - Backtracking...\n",
      "979 - loss: 0.641\n",
      "980 - loss: 0.641\n",
      "981 - loss: 0.641\n",
      "982 - loss: 0.641\n",
      "983 - loss: 0.641\n",
      "984 - loss: 0.641\n",
      "985 - loss: 0.641\n",
      "986 - loss: 0.641\n",
      "987 - loss: 0.641\n",
      "988 - loss: 0.641\n",
      "989 - loss: 0.641\n",
      "f_new: 0.641 - f: 0.641 - Backtracking...\n",
      "991 - loss: 0.641\n",
      "992 - loss: 0.641\n",
      "993 - loss: 0.641\n",
      "994 - loss: 0.641\n",
      "995 - loss: 0.641\n",
      "996 - loss: 0.641\n",
      "997 - loss: 0.641\n",
      "998 - loss: 0.641\n",
      "999 - loss: 0.641\n",
      "1000 - loss: 0.641\n",
      "Reached maximum number of function evaluations 1000\n",
      "0.6813054955603551\n"
     ]
    }
   ],
   "source": [
    "#kernel_logreg_poly = LeastSquaresGDKernel(Kernel.kernel_poly, verbose=True, max_evals=1000)\n",
    "#kernel_logreg_poly = LogisticRegressionKernel(Kernel.kernel_poly, verbose=True, max_evals=1000)\n",
    "kernel_logreg_poly = LogisticRegressionKernel(Kernel.kernel_poly, verbose=True, max_evals=1000)\n",
    "p = 4\n",
    "preds = kernel_logreg_poly.predict(y_sub, X_sub, X_sub, p)\n",
    "print(compute_accuracy(preds, y_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.152456"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(tX[:,0] == -999) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9256178831925277"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y[tX[:,0] == -999] == -1) / len(y[tX[:,0] == -999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_clean = remove_NaN_features(tX, 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegression()\n",
    "logReg.fit(y, tX_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 1289883598.7\n",
      "Iteration 1, loss = 929278938.1\n",
      "Iteration 2, loss = 843606874.7\n",
      "Iteration 3, loss = 832528308.0\n",
      "Iteration 4, loss = 831415267.7\n",
      "Iteration 5, loss = 831306045.8\n",
      "Iteration 6, loss = 831295253.7\n",
      "Iteration 7, loss = 831294167.6\n",
      "Iteration 8, loss = 831294054.6\n",
      "Iteration 9, loss = 831294042.1\n"
     ]
    }
   ],
   "source": [
    "model = AlternativePCA(k=2)\n",
    "model.fit(X)\n",
    "Z_pca = model.compress(X)\n",
    "Xhat_pca = model.expand(Z_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss = 1521186763.4\n",
      "Iteration 1, loss = 947138369.7\n",
      "Iteration 2, loss = 854324275.1\n",
      "Iteration 3, loss = 835473177.5\n",
      "Iteration 4, loss = 832070513.0\n",
      "Iteration 5, loss = 831441985.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antho\\Google Drive\\EPFL\\MA1\\CS-433 Machine Learning\\ML_course\\projects\\project1\\scripts\\solver.py:65: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  y = g_new - g\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 831322662.2\n",
      "Iteration 7, loss = 831299623.4\n",
      "Iteration 8, loss = 831295133.9\n",
      "Iteration 9, loss = 831294255.0\n",
      "Iteration 0, loss = 114368921.2\n",
      "Iteration 1, loss = 114357028.3\n",
      "Iteration 2, loss = 114357025.0\n",
      "Iteration 3, loss = 114357025.0\n",
      "Iteration 4, loss = 114357025.0\n",
      "Iteration 5, loss = 114357025.0\n",
      "Iteration 6, loss = 114357025.0\n",
      "Iteration 7, loss = 114357025.0\n",
      "Iteration 8, loss = 114357025.0\n",
      "Iteration 9, loss = 114357025.0\n"
     ]
    }
   ],
   "source": [
    "model = RobustPCA(k=2)\n",
    "model.fit(X)\n",
    "Z_robust = model.compress(X)\n",
    "Xhat_robust = model.expand(Z_robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.72588088, -7.31380373],\n",
       "       [ 3.89450982, 10.60756305],\n",
       "       [ 7.77930283, -1.90683368],\n",
       "       ...,\n",
       "       [-2.54520707,  6.4204025 ],\n",
       "       [-8.07558037, 21.54278193],\n",
       "       [-4.59888286, 21.69639677]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "#Need to rescale to do PCA\n",
    "def pca_axes(Z,scale,scale2, verbose=False):\n",
    "    \n",
    "    pcs = \n",
    "    colors = np.arctan2(pcs[0,:], pcs[1,:])\n",
    "    colormap = cm.inferno\n",
    "    norm = Normalize()\n",
    "    norm.autoscale(colors)\n",
    "    plt.rcParams['image.cmap'] = 'Paired'\n",
    "\n",
    "    # Quiver\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    plt.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),\n",
    "           pcs[0,:], pcs[1,:], color=colormap(norm(colors)),\n",
    "           angles='xy', scale_units='xy', scale=scale)\n",
    "    o = 0\n",
    "    for i in range(0,pcs.shape[1]):\n",
    "        #plt.arrow(0, 0, pcs[0,i], pcs[1,i],color='k') \n",
    "        if i==3:\n",
    "            plt.text(pcs[0,i]* scale2, pcs[1,i]*scale2-0.03, X.columns[i], color = 'k', \n",
    "             ha = 'center', va = 'center', fontsize=10, weight='bold')\n",
    "        elif i==12:\n",
    "            plt.text(pcs[0,i]* scale2, pcs[1,i]*scale2-0.03, X.columns[i], color = 'k', \n",
    "             ha = 'center', va = 'center', fontsize=10, weight='bold')\n",
    "        elif np.power(pcs[0,i]-pcs[1,i], 2)<0.001:\n",
    "            plt.text(0, 0-o, ' ', color = 'k', \n",
    "             ha = 'center', va = 'center', fontsize=12, weight='bold')\n",
    "        else:\n",
    "            plt.text(pcs[0,i]* scale2, pcs[1,i] *scale2, X.columns[i], color = 'k', \n",
    "             ha = 'center', va = 'center', fontsize=10, weight='bold')\n",
    "        o=o+0.04\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.xlim([-1.0,1.0])\n",
    "    plt.ylim([-0.5,1.0])\n",
    "    plt.xlabel('Principal component 0')\n",
    "    plt.ylabel('Principal component 1')\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    " \n",
    "    if verbose==True:\n",
    "        for i in range (0,pcs.shape[1]):\n",
    "            print(str(i), df.columns[i+2])\n",
    "        \n",
    "    return\n",
    "\n",
    "pca_axes(Z_pca, 0.65, 1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-524e3a5e580a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel_logreg_poly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\EPFL\\MA1\\CS-433 Machine Learning\\ML_course\\projects\\project1\\scripts\\classifiers.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, y, X, Xtest, lambda_, *args)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mKtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Google Drive\\EPFL\\MA1\\CS-433 Machine Learning\\ML_course\\projects\\project1\\scripts\\classifiers.py\u001b[0m in \u001b[0;36mkernel_RBF\u001b[1;34m(X1, X2, sigma)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mK\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[1;32m-> 2076\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   2077\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kernel_logreg_poly = LeastSquaresKernel(Kernel.kernel_RBF, verbose=True, max_evals=1000)\n",
    "p = 4\n",
    "sigma = 1\n",
    "accuracies = []\n",
    "for i in range(5):\n",
    "    print(i,  end=' - ')\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    X_sub = (X[shuffle_indices,:])[::100]\n",
    "    y_sub = (y[shuffle_indices])[::100]\n",
    "    \n",
    "    X_pred = (X[shuffle_indices,:])[::100]\n",
    "    y_pred = (y[shuffle_indices])[::100]\n",
    "    \n",
    "    preds = kernel_logreg_poly.predict(y_sub, X_sub, X_pred, sigma, lambda_=1)\n",
    "    accuracy = compute_accuracy(preds, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8757090909090909"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mse, loss_mse = least_squares(y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_mse = compute_accuracy(predict_labels(w_mse, X), y)\n",
    "print(accuracy_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_classifier(lambda_):\n",
    "    return ridge_regression(y, X, lambda_)\n",
    "\n",
    "lambda_ridge, _, _ = find_max_hyperparam(ridge_classifier, [10**c for c in range(-3,3)])\n",
    "print(\"Optimal lambda: %f\" % lambda_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_train(y_train, X_train):\n",
    "    return ridge_regression(y, X, lambda_ridge)\n",
    "\n",
    "def ridge_test(X_test, w):\n",
    "    return np.sign(X_test@w)\n",
    "\n",
    "accuracy_ridge = cross_validate(y, X, ridge_train, ridge_test, 0.8, 100)\n",
    "print(accuracy_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_train(y_train, X_train):\n",
    "    return logistic_regression(y_train, X_train, 0.01*np.ones(X_train.shape[1]), 1000, verbose=False)\n",
    "\n",
    "def log_reg_test(X_test, w):\n",
    "    return np.sign(X_test@w)\n",
    "\n",
    "accuracy_log_reg = cross_validate(y, X_safe, log_reg_train, log_reg_test, 0.7, 20)\n",
    "print(accuracy_log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_log_reg_classifier(lambda_):\n",
    "    return reg_logistic_regression(y, X_safe, lambda_, np.zeros(X_safe.shape[1]), 1000)\n",
    "\n",
    "def log_reg_sparse_classifier(lambda_):\n",
    "    return logistic_regression_sparse(y, X_safe, lambda_, np.zeros(X_safe.shape[1]), 1000)\n",
    "\n",
    "def mse_sparse_classifier(lambda_):\n",
    "    return least_squares_sparse(y, X_safe, lambda_, np.zeros(X_safe.shape[1]), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_log_reg_l2, _, _ = find_max_hyperparam(reg_log_reg_classifier, [10**c for c in range(-3,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_log_reg_l1, _, _ = find_max_hyperparam(log_reg_sparse_classifier, [10**c for c in range(-3,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_mse_l2, _, _ = find_max_hyperparam(ridge_classifier, [10**c for c in range(-6,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_mse_l1, w_mse_l1, _ = find_max_hyperparam(mse_sparse_classifier, [10**c for c in range(-6,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mse_l1, _ = least_squares_sparse(y, X_safe, 0.01, np.zeros(X_safe.shape[1]), 1000)\n",
    "print(\"Non-zero weights: %i / %i\" % (np.sum(w_mse_l1 != 0), len(w_mse_l1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1\n",
    "ypred_kernel = kernel_predict(kernel_poly, y, X_safe, X_safe, p, lambda_=1)\n",
    "print(compute_accuracy(ypred_kernel, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "y_test, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_safe = tXToX(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(np.sign(X_test_safe @ w_mse_l1), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../results/predictions.csv'\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
